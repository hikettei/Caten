{"config":{"indexing":"full","lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome to Caten docs","title":"Home"},{"location":"#home","text":"Welcome to Caten docs","title":"Home"},{"location":"development/","text":"Development","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"quickstart/","text":"QuickStart Install","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"QuickStart"},{"location":"quickstart/#install","text":"","title":"Install"},{"location":"packages/caten.aasm/","text":"caten/aasm UnaryOps :CAST The node :CAST casts the first read tensor into :dtype , writing the result into the first write. When optimizing cast in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :NOT The node :NOT computes the logical-not of the given tensor if the input is a boolean, otherwise (integer) computes a bitwise-not. out = not(x) (if boolean) out = lognot(x) (if integer) When optimizing not in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :SQRT The node :SQRT computes square-root of the first read tensor, writing the result to the first write. out = sqrt(x); When optimizing sqrt in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :LOG2 The node :LOG2 computes log2 of the first read tensor, writing the result to the first write. out = log2(x); When optimizing log2 in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :EXP2 The node :EXP2 computes exp2 of the first read tensor, writing the result to the first write. out = exp2(x); When optimizing exp2 in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :SIN The node :SIN computes sine of the first read tensor, writing the result to the first write. out = sin(x); When optimizing sin in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :RECIP The node :RECIP computes the reciprocal of the first read tensor, writing the result to the first write. out = (1/x); When optimizing recip in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , :NEG The node :NEG flips the sign of the first read tensor, writing the result to the first write. out = (-x); When optimizing neg in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE , BinaryOps :GCD Computes the greatest common divisor of two integer tensors in read, writing the result to the first write. (Note: This computation should only applied to scalar tensors, and used for only computing the dynamic shaped tensor indexing.) out <- gcd(x, y) When optimizing gcd in-place, the 0th read is consumed. superclasses = BINARYOPS , :MAX Computes the maximum value of two tensors in read, writing the result to the first write. out <- max(x, y) When optimizing max in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :MOVE Moves the second read into the first read, setting the result to first write. out <- move(x, y) where move(x, y) is x = y When optimizing move in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :XOR The node :XOR computes the bit-wise xor of two tensors in read if they are integer, otherwise (boolean) computes the logical-xor. out <- x ^ y (if boolean) out <- x ^ y (if integer) When optimizing xor in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :OR The node :OR computes the bit-wise or of two tensors in read if they are integer, otherwise (boolean) computes the logical-or. out <- x || y (if boolean) out <- x | y (if integer) When optimizing or in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :AND The node :AND computes the bit-wise and of two tensors in read if they are integer, otherwise (boolean) computes the logical-and. out <- x && y (if boolean) out <- x & y (if integer) When optimizing and in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :IDIV The node :IDIV divides the first tensor in read by the second tensor in read , writing the result to the first write . Unlike other BinaryOps, :IDIV assumes two tensors to be an integer typed tensor. out <- x / y When optimizing idiv in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :MOD The node :MOD finds the reminder of the first tensor in read divided by the second tensor in read . out <- x % y When optimizing mod in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :MUL The node :MUL multiplies the two tensors in read and writes the result to the first write . out <- x + y When optimizing mul in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , :ADD The node :ADD adds the two tensors in read and writes the result to the first write . out <- x + y When optimizing add in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE , TernaryOps :WMMA The node :WMMA is generated during optimization (simplifiers.lisp) by AJIT and represents a fused computation of :ADD and :MUL. WMMA is not generated during VM execution. WMMA is used to optimize the gemm computation: WMMA(c, a, b) is the equivalent to: c += a * b (if reduction = t) out = c + a * b (if reduction = nil) When optimizing wmma in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE , :WHERE If the result of the first read (boolean) is true, the second read is selected, and if false, the third read is selected and written to the first write. When optimizing in-place, note that the value of the second read is used as a placeholder since choosing the first read would result in a data type mismatch with write. in_dtype = dtype_of(y); out[in_dtype] = x[boolean] ? y[in_dtype] : z[in_dtype]; When optimizing where in-place, the 1th read is consumed. superclasses = TERNARYOPS , JITABLE , :< Compares the second and third tensors in read with < , writing the result to the first write. The first read tensor is an placeholder for the first write tensor and is always boolean. x = y < z; out = x; When optimizing < in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE , :!= Compares the second and third tensors in read with not-equal , writing the result to the first write. The first read tensor is an placeholder for the first write tensor and is always boolean. x = y != z; out = x; When optimizing != in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE , Buffer :VIEW Creates a view object of the tensor in a first read. View object can modify the multi-dimensional offset of tensors, strides, shapes, and strides without copying. out = view(x, *shape-new, *upfrom, *below, *by, *stride-new) upfrom and below describes the multi-dimensional offset of the tensor. Caten applies an operation to out in the range of [upfrom, below) . by indicates the step of stride. the out tensor is reinitialized with shape-new and stride-new . View has an attribute broadcast[list]`, this indicates the stride of thecorresponding axis is recognised as 0 if set to T. nrank[(unsigned-byte 32)] the rank of viewed tensor. broadcast[list] broadcasting order. permute[list] is an optional parameter and does nothing in VM, but requires to apply Polyhedral Compiler. If the view was created in caten/api:!permute , set the argument to this attribute. When optimizing view in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE , :STORE Just like a :MOVE, moves the second tensor in read into the first tensor in read, writing the result to the first write. (Note: :STORE can be removed in the future refactoring) When optimizing store in-place, the 0th read is consumed. superclasses = BUFFEROPS , BINARYOPS , JITABLE , :LOAD Fills the first tensor in read with value , writing the result into the first write. The first read can be either of tensor or scalar. x[...] = value; out = x; value[symbol or number] initial value. When optimizing load in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE , :ALLOCATE Allocates a new matrix of scalar value in the VM. out = allocate(*shape, *stride) :Allocate is defined as described above. The first through nrank -th read scalar tensors represent the size of the Tensor, and from the nrank -th read to the last, they represent the stride of the allocated Tensor. (they can be sliced using subseq ) dtype[dtype-t] dtype to allocate. nrank[(unsigned-byte 32)] a rank of tensor. If set to 0, allocates a scalar. from[symbol or buffer or null] If specified, instead of allocating, an already allocated Buffer is used. If a symbol is specified, a buffer is already defined in the variable table of GraphRuntime. If buffer is specified, use the buffer directly. pool[null or Buffer] A place to store the result of the previous allocation. Allocation will be performed only after this slot is set to nil, or size are different due to dynamic shape. When optimizing allocate in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE , Indexing :INDEX-COMPONENTS The node :INDEX-COMPONENTS Indicates which element-wise computation of the Tensor is being performed. Typically, it should return the argument used when performing Aref on the Tensor with the corresponding strides . out <- index_components(x, *strides) is compiled as: for i=0..N for j=0..M out[i, j] = stride[0] * i + j; When optimizing index-components in-place, the 0th read is consumed. superclasses = INDEXING , JITABLE , JIT Specific Ops :JIT_KERNEL The node :JIT_KERNEL is an instruction that calls a jit-compiled kernel from the VM. When optimizing jit_kernel in-place, the 0th read is consumed. :SPACE Corresponds to: [blockIdx|threadIdx].[rank] When optimizing space in-place, the 0th read is consumed. superclasses = JITABLE , :EXPR The node :EXPR is a data structure used by the JIT compiler, representing a node that fuses multiple composable nodes. - Expr[caten/codegen/expr:EXPR] a tree structure comprised of caten/codegen/expr:EXPR - reduction[boolean] - Meta[caten/codegen/expr:ExprMeta] a place to store ExprMeta. When optimizing expr in-place, the 0th read is consumed. superclasses = JITABLE ,","title":"caten/aasm"},{"location":"packages/caten.aasm/#catenaasm","text":"","title":"caten/aasm"},{"location":"packages/caten.aasm/#unaryops","text":"","title":"UnaryOps"},{"location":"packages/caten.aasm/#cast","text":"The node :CAST casts the first read tensor into :dtype , writing the result into the first write. When optimizing cast in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":CAST"},{"location":"packages/caten.aasm/#not","text":"The node :NOT computes the logical-not of the given tensor if the input is a boolean, otherwise (integer) computes a bitwise-not. out = not(x) (if boolean) out = lognot(x) (if integer) When optimizing not in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":NOT"},{"location":"packages/caten.aasm/#sqrt","text":"The node :SQRT computes square-root of the first read tensor, writing the result to the first write. out = sqrt(x); When optimizing sqrt in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":SQRT"},{"location":"packages/caten.aasm/#log2","text":"The node :LOG2 computes log2 of the first read tensor, writing the result to the first write. out = log2(x); When optimizing log2 in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":LOG2"},{"location":"packages/caten.aasm/#exp2","text":"The node :EXP2 computes exp2 of the first read tensor, writing the result to the first write. out = exp2(x); When optimizing exp2 in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":EXP2"},{"location":"packages/caten.aasm/#sin","text":"The node :SIN computes sine of the first read tensor, writing the result to the first write. out = sin(x); When optimizing sin in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":SIN"},{"location":"packages/caten.aasm/#recip","text":"The node :RECIP computes the reciprocal of the first read tensor, writing the result to the first write. out = (1/x); When optimizing recip in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":RECIP"},{"location":"packages/caten.aasm/#neg","text":"The node :NEG flips the sign of the first read tensor, writing the result to the first write. out = (-x); When optimizing neg in-place, the 0th read is consumed. superclasses = UNARYOPS , JITABLE ,","title":":NEG"},{"location":"packages/caten.aasm/#binaryops","text":"","title":"BinaryOps"},{"location":"packages/caten.aasm/#gcd","text":"Computes the greatest common divisor of two integer tensors in read, writing the result to the first write. (Note: This computation should only applied to scalar tensors, and used for only computing the dynamic shaped tensor indexing.) out <- gcd(x, y) When optimizing gcd in-place, the 0th read is consumed. superclasses = BINARYOPS ,","title":":GCD"},{"location":"packages/caten.aasm/#max","text":"Computes the maximum value of two tensors in read, writing the result to the first write. out <- max(x, y) When optimizing max in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":MAX"},{"location":"packages/caten.aasm/#move","text":"Moves the second read into the first read, setting the result to first write. out <- move(x, y) where move(x, y) is x = y When optimizing move in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":MOVE"},{"location":"packages/caten.aasm/#xor","text":"The node :XOR computes the bit-wise xor of two tensors in read if they are integer, otherwise (boolean) computes the logical-xor. out <- x ^ y (if boolean) out <- x ^ y (if integer) When optimizing xor in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":XOR"},{"location":"packages/caten.aasm/#or","text":"The node :OR computes the bit-wise or of two tensors in read if they are integer, otherwise (boolean) computes the logical-or. out <- x || y (if boolean) out <- x | y (if integer) When optimizing or in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":OR"},{"location":"packages/caten.aasm/#and","text":"The node :AND computes the bit-wise and of two tensors in read if they are integer, otherwise (boolean) computes the logical-and. out <- x && y (if boolean) out <- x & y (if integer) When optimizing and in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":AND"},{"location":"packages/caten.aasm/#idiv","text":"The node :IDIV divides the first tensor in read by the second tensor in read , writing the result to the first write . Unlike other BinaryOps, :IDIV assumes two tensors to be an integer typed tensor. out <- x / y When optimizing idiv in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":IDIV"},{"location":"packages/caten.aasm/#mod","text":"The node :MOD finds the reminder of the first tensor in read divided by the second tensor in read . out <- x % y When optimizing mod in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":MOD"},{"location":"packages/caten.aasm/#mul","text":"The node :MUL multiplies the two tensors in read and writes the result to the first write . out <- x + y When optimizing mul in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":MUL"},{"location":"packages/caten.aasm/#add","text":"The node :ADD adds the two tensors in read and writes the result to the first write . out <- x + y When optimizing add in-place, the 0th read is consumed. superclasses = BINARYOPS , JITABLE ,","title":":ADD"},{"location":"packages/caten.aasm/#ternaryops","text":"","title":"TernaryOps"},{"location":"packages/caten.aasm/#wmma","text":"The node :WMMA is generated during optimization (simplifiers.lisp) by AJIT and represents a fused computation of :ADD and :MUL. WMMA is not generated during VM execution. WMMA is used to optimize the gemm computation: WMMA(c, a, b) is the equivalent to: c += a * b (if reduction = t) out = c + a * b (if reduction = nil) When optimizing wmma in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE ,","title":":WMMA"},{"location":"packages/caten.aasm/#where","text":"If the result of the first read (boolean) is true, the second read is selected, and if false, the third read is selected and written to the first write. When optimizing in-place, note that the value of the second read is used as a placeholder since choosing the first read would result in a data type mismatch with write. in_dtype = dtype_of(y); out[in_dtype] = x[boolean] ? y[in_dtype] : z[in_dtype]; When optimizing where in-place, the 1th read is consumed. superclasses = TERNARYOPS , JITABLE ,","title":":WHERE"},{"location":"packages/caten.aasm/#_1","text":"Compares the second and third tensors in read with < , writing the result to the first write. The first read tensor is an placeholder for the first write tensor and is always boolean. x = y < z; out = x; When optimizing < in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE ,","title":":&lt;"},{"location":"packages/caten.aasm/#_2","text":"Compares the second and third tensors in read with not-equal , writing the result to the first write. The first read tensor is an placeholder for the first write tensor and is always boolean. x = y != z; out = x; When optimizing != in-place, the 0th read is consumed. superclasses = TERNARYOPS , JITABLE ,","title":":!="},{"location":"packages/caten.aasm/#buffer","text":"","title":"Buffer"},{"location":"packages/caten.aasm/#view","text":"Creates a view object of the tensor in a first read. View object can modify the multi-dimensional offset of tensors, strides, shapes, and strides without copying. out = view(x, *shape-new, *upfrom, *below, *by, *stride-new) upfrom and below describes the multi-dimensional offset of the tensor. Caten applies an operation to out in the range of [upfrom, below) . by indicates the step of stride. the out tensor is reinitialized with shape-new and stride-new . View has an attribute broadcast[list]`, this indicates the stride of thecorresponding axis is recognised as 0 if set to T. nrank[(unsigned-byte 32)] the rank of viewed tensor. broadcast[list] broadcasting order. permute[list] is an optional parameter and does nothing in VM, but requires to apply Polyhedral Compiler. If the view was created in caten/api:!permute , set the argument to this attribute. When optimizing view in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE ,","title":":VIEW"},{"location":"packages/caten.aasm/#store","text":"Just like a :MOVE, moves the second tensor in read into the first tensor in read, writing the result to the first write. (Note: :STORE can be removed in the future refactoring) When optimizing store in-place, the 0th read is consumed. superclasses = BUFFEROPS , BINARYOPS , JITABLE ,","title":":STORE"},{"location":"packages/caten.aasm/#load","text":"Fills the first tensor in read with value , writing the result into the first write. The first read can be either of tensor or scalar. x[...] = value; out = x; value[symbol or number] initial value. When optimizing load in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE ,","title":":LOAD"},{"location":"packages/caten.aasm/#allocate","text":"Allocates a new matrix of scalar value in the VM. out = allocate(*shape, *stride) :Allocate is defined as described above. The first through nrank -th read scalar tensors represent the size of the Tensor, and from the nrank -th read to the last, they represent the stride of the allocated Tensor. (they can be sliced using subseq ) dtype[dtype-t] dtype to allocate. nrank[(unsigned-byte 32)] a rank of tensor. If set to 0, allocates a scalar. from[symbol or buffer or null] If specified, instead of allocating, an already allocated Buffer is used. If a symbol is specified, a buffer is already defined in the variable table of GraphRuntime. If buffer is specified, use the buffer directly. pool[null or Buffer] A place to store the result of the previous allocation. Allocation will be performed only after this slot is set to nil, or size are different due to dynamic shape. When optimizing allocate in-place, the 0th read is consumed. superclasses = BUFFEROPS , JITABLE ,","title":":ALLOCATE"},{"location":"packages/caten.aasm/#indexing","text":"","title":"Indexing"},{"location":"packages/caten.aasm/#index-components","text":"The node :INDEX-COMPONENTS Indicates which element-wise computation of the Tensor is being performed. Typically, it should return the argument used when performing Aref on the Tensor with the corresponding strides . out <- index_components(x, *strides) is compiled as: for i=0..N for j=0..M out[i, j] = stride[0] * i + j; When optimizing index-components in-place, the 0th read is consumed. superclasses = INDEXING , JITABLE ,","title":":INDEX-COMPONENTS"},{"location":"packages/caten.aasm/#jit-specific-ops","text":"","title":"JIT Specific Ops"},{"location":"packages/caten.aasm/#jit_kernel","text":"The node :JIT_KERNEL is an instruction that calls a jit-compiled kernel from the VM. When optimizing jit_kernel in-place, the 0th read is consumed.","title":":JIT_KERNEL"},{"location":"packages/caten.aasm/#space","text":"Corresponds to: [blockIdx|threadIdx].[rank] When optimizing space in-place, the 0th read is consumed. superclasses = JITABLE ,","title":":SPACE"},{"location":"packages/caten.aasm/#expr","text":"The node :EXPR is a data structure used by the JIT compiler, representing a node that fuses multiple composable nodes. - Expr[caten/codegen/expr:EXPR] a tree structure comprised of caten/codegen/expr:EXPR - reduction[boolean] - Meta[caten/codegen/expr:ExprMeta] a place to store ExprMeta. When optimizing expr in-place, the 0th read is consumed. superclasses = JITABLE ,","title":":EXPR"},{"location":"packages/caten.air/","text":"aIR(= A bstract IR ) is a Caten's internal IR and is used to represent computations expressed as either Let-Binding Based IR or a DAG. Each nodes are represented using a Node structure, and their list (or tree structure) is maintained by the Graph CLOS class. The compilation is performed by efficiently manipulating and optimizing the graph using a pattern matcher. Note that All nodes must be declared using the defnode macro before compilation. This allows node-related errors to be detected and reported during compilation. Node [struct] Node A node is a computation with input sources (reads) and outputs (writes) . Read1 Read2 Read3 Read4 \\ | | / [Operation] | writes1 writes2 ... Before creating a node using make-node , the node must be defined using defnode . Attributes are defined by defnode. node-class[keyword] indicates the classification of the node (e.g.: :BinaryOps, :UnaryOps ). This information is used for debugging purposes. node-type[keyword] indicates the type of the node (e.g.: :Add, :Sub, :Mul, :Div ). node-id[symbol] a unique symbol that exists only once in the namespaces. (usually generated by gensym) node-writes[list] a list of symbols that represent the output buffers. node-reads[list] a list of symbols or numbers that represent the input buffers. (usually connected with the (node-writes) ) node-attr[Attribute] additional information about the node. (e.g.: reduction, dtype, type inference etc...) out-nth[fixnum] node->id will point to the out-nth element of the writes list. [macro] defnode Defines a new node. ( defnode ( class type ) ( &rest direct-superclasses ) description &key ( placeholder 0 ) ( verify 'identity ) ( slots )) class[keyword] an identifier of the node class. type[keyword] an identifier of the node type. direct-superclasses[list of keyword] a list of superclasses. description[string] a description of the node. placeholder[(unsigned-byte 32) or -1] when mutating the node in-place, the compiler consumes the placeholder-th read buffer. verify[function] a function to verify the arguments. slots[list of (symbol &rest slot-options)] a list of slot definitions. [function] make-node ( make-node class type writes reads &rest attrs ) Create a node with the given class , type , writes , reads , and attrs . class and type should be defined by defnode , otherwise the function will produce an error. class[keyword] indicates the classification of the node (e.g.: :BinaryOps, :UnaryOps ). This information is used for debugging purposes. type[keyword] indicates the type of the node (e.g.: :Add, :Sub, :Mul, :Div ). writes[list] a list of symbols that represent the output buffers. reads[list] a list of symbols or numbers that represent the input buffers. (usually connected with the (node-writes) ) attrs[keyword and value pairs] initializers for the Attribute class. [function] copy-node (copy-node node) Create a deepcopy of the given node. [function] getattrs (getattrs node) Return the list of attributes of the node. [function] getattr (getattr node id &key (allow-undefined nil)) Return the value of the attribute id of the node. If the attribute is not defined, it will produce an error. If allow-undefined is t, it returns nil without error. This function is setfable. [function] remattr (remattr node id &key (allow-undefined nil)) Equivalent to: (setf (getattr node id :allow-undefined allow-undefined) nil) Examples lisp CATEN-USER> ( make-node :BinaryOps :ADD ( list 'x ) ( list 'y 'z ) :reduction t ) Result Result <Node[BINARYOPS] ADD(NID1615) : X* <- (Y, Z) where :reduction=T> Graph [class] Graph A Graph is a data structure used to handle a collection of nodes. Graph has a list of nodes (nodes), node inputs (seen), and node outputs (outputs). Unlike FastGraph , Graph does not assume that the nodes form a DAG. Additionally, it guarantees that the nodes will not be sorted during each process of the Pattern Matcher. Graph structures, such as Let-Binding, are represented using Graph. Note: Using Graph may lead to performance degradation if the graph is a DAG. Please use FastGraph instead. [class] FastGraph FastGraph is a subclass of Graph that implements faster node searches based on the assumption that the nodes form a DAG. (It is approximately 20 times faster than Graph, in the larger scale.) Since FastGraph stores nodes as a hash-table, there are some constraints on node operations. If necessary, converting between (->fast-graph graph) and (->graph graph) can be done frequently with minimal impact on performance. [generic] copy-graph (copy-graph graph) Creates a copy of the given graph. [function] make-graph (make-graph &rest nodes) Creates a new Graph object with the given nodes. [generic] graph-nodes NIL [generic] id->value (id->value graph id) Returns a node whose node-writes includes the given id. [generic] id->users (id->users graph id) Returns a list of nodes whose node-reads includes the given id. [generic] remnode (remnode graph id) Removes all node where node-id = id from the given graph. [generic] insert-nodes (insert-nodes graph nodes) Inserts the given nodes (list) into the graph. [function] ->fast-graph (->fast-graph graph) Creates a FastGraph object from the given graph. [generic] ->graph (->graph graph) Converts the given graph to a fast graph. [generic] verify-graph (verify-graph graph &key no-purge) Verify the consistency of the graphs and simplify them by doing following things: - Checks if all variables are immutable - All variables appeared in read, are also defined in writes. - Purge all isolated graph - Sort nodes by time. - Set no-purge=T to ignore purge isolated graph step. This has no effect on FastGraph. Simplifier [macro] defsimplifier (defsimplifier (name &key (speed 3)) &rest rules) Defines a new simplifier named name . The defined function has a following form: (name graph &key (no-verify nil) (return-changed-p nil)) The graph is a graph to simplify. The no-verify is a flag to skip the verification process. The return-changed-p is a flag to return the result of the simplification process, or a boolean indicating that the graph was changed during simplification process.. The speed is an optimization level. The rules are a list of simplification rules. Each rule has a form: (Node_Name (Read_Args) Attrs) (TODO: Documentation) (See also: ./source/aasm/constant-folding.lisp ) Visualizer [function] ->dot (->dot graph &key (pathname \"/tmp/graph.dot\") (open t) (title \"node\")) Visualizes the graph using graphviz(requirement). Set open=t to open the resulting image in the default browser. A tmp file is created at the pathname location. The graph is saved as a .png and .html file. The title is used in the html file. [function] pprint-graph (pprint-graph graph &key (screen-width 140) (stream t)) The function pprint-graph prints the graph in a tree-like structure. screen-width controls the width of the output, if the graph is too wide and cannot fit in the screen, it will be split into multiple pages. lisp CATEN-USER> ( pprint-graph ( runtime-graph ( caten ( !gelu ( make-tensor ` ( 3 3 ) :id 'x )))) :stream nil ) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {0} \u2514 :MUL {N1} \u251c :MUL {N2} \u2502 \u2514 Allocate[:float32] (3 3) \u2502 \u251c :MOVE {N3} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N4} \u2502 \u251c load(0.5) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N5} \u251c :MOVE {N6} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N7} \u2502 \u251c load(1.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N8} \u2514 :VIEW {N9} \u2502 \u251c load(-1.0) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N10} \u251c :MOVE {N11} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N12} \u2502 \u251c load(2.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :RECIP {N13} \u2514 :ADD {N14} \u251c :MOVE {N15} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N16} \u2502 \u251c load(1.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :EXP2 {N17} \u2514 :MUL {N18} \u2514 load(1.442695) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N19} \u2514 :VIEW {N20} \u2502 \u251c load(0.6931472) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N21} \u2514 :VIEW {N22} \u2502 \u251c load(-1.442695) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N23} \u251c :MOVE {N24} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N25} \u2502 \u251c load(2.0) \u2514 :MUL {N26} \u251c :MOVE {N27} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N28} \u2502 \u251c load(0.7978846) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N29} \u251c Allocate[:float32] (3 3) \u2514 :MUL {N30} \u2514 :MUL {N31} \u2502 \u2514 Allocate[:float32] (3 3) \u2502 \u251c :MUL {N32} \u2502 \u251c Allocate[:float32] (3 3) \u2502 Allocate[:float32] (3 3) \u251c :MOVE {N33} \u251c Allocate[:float32] (3 3) \u2514 :VIEW {N34} \u251c load(0.044715) \u2514 Allocate[:float32] NIL ============================================================================================================================================","title":"caten/air"},{"location":"packages/caten.air/#node","text":"","title":"Node"},{"location":"packages/caten.air/#struct-node","text":"A node is a computation with input sources (reads) and outputs (writes) . Read1 Read2 Read3 Read4 \\ | | / [Operation] | writes1 writes2 ... Before creating a node using make-node , the node must be defined using defnode . Attributes are defined by defnode. node-class[keyword] indicates the classification of the node (e.g.: :BinaryOps, :UnaryOps ). This information is used for debugging purposes. node-type[keyword] indicates the type of the node (e.g.: :Add, :Sub, :Mul, :Div ). node-id[symbol] a unique symbol that exists only once in the namespaces. (usually generated by gensym) node-writes[list] a list of symbols that represent the output buffers. node-reads[list] a list of symbols or numbers that represent the input buffers. (usually connected with the (node-writes) ) node-attr[Attribute] additional information about the node. (e.g.: reduction, dtype, type inference etc...) out-nth[fixnum] node->id will point to the out-nth element of the writes list.","title":"[struct] Node"},{"location":"packages/caten.air/#macro-defnode","text":"Defines a new node. ( defnode ( class type ) ( &rest direct-superclasses ) description &key ( placeholder 0 ) ( verify 'identity ) ( slots )) class[keyword] an identifier of the node class. type[keyword] an identifier of the node type. direct-superclasses[list of keyword] a list of superclasses. description[string] a description of the node. placeholder[(unsigned-byte 32) or -1] when mutating the node in-place, the compiler consumes the placeholder-th read buffer. verify[function] a function to verify the arguments. slots[list of (symbol &rest slot-options)] a list of slot definitions.","title":"[macro] defnode"},{"location":"packages/caten.air/#function-make-node","text":"( make-node class type writes reads &rest attrs ) Create a node with the given class , type , writes , reads , and attrs . class and type should be defined by defnode , otherwise the function will produce an error. class[keyword] indicates the classification of the node (e.g.: :BinaryOps, :UnaryOps ). This information is used for debugging purposes. type[keyword] indicates the type of the node (e.g.: :Add, :Sub, :Mul, :Div ). writes[list] a list of symbols that represent the output buffers. reads[list] a list of symbols or numbers that represent the input buffers. (usually connected with the (node-writes) ) attrs[keyword and value pairs] initializers for the Attribute class.","title":"[function] make-node"},{"location":"packages/caten.air/#function-copy-node","text":"(copy-node node) Create a deepcopy of the given node.","title":"[function] copy-node"},{"location":"packages/caten.air/#function-getattrs","text":"(getattrs node) Return the list of attributes of the node.","title":"[function] getattrs"},{"location":"packages/caten.air/#function-getattr","text":"(getattr node id &key (allow-undefined nil)) Return the value of the attribute id of the node. If the attribute is not defined, it will produce an error. If allow-undefined is t, it returns nil without error. This function is setfable.","title":"[function] getattr"},{"location":"packages/caten.air/#function-remattr","text":"(remattr node id &key (allow-undefined nil)) Equivalent to: (setf (getattr node id :allow-undefined allow-undefined) nil)","title":"[function] remattr"},{"location":"packages/caten.air/#examples","text":"lisp CATEN-USER> ( make-node :BinaryOps :ADD ( list 'x ) ( list 'y 'z ) :reduction t ) Result Result <Node[BINARYOPS] ADD(NID1615) : X* <- (Y, Z) where :reduction=T>","title":"Examples"},{"location":"packages/caten.air/#graph","text":"","title":"Graph"},{"location":"packages/caten.air/#class-graph","text":"A Graph is a data structure used to handle a collection of nodes. Graph has a list of nodes (nodes), node inputs (seen), and node outputs (outputs). Unlike FastGraph , Graph does not assume that the nodes form a DAG. Additionally, it guarantees that the nodes will not be sorted during each process of the Pattern Matcher. Graph structures, such as Let-Binding, are represented using Graph. Note: Using Graph may lead to performance degradation if the graph is a DAG. Please use FastGraph instead.","title":"[class] Graph"},{"location":"packages/caten.air/#class-fastgraph","text":"FastGraph is a subclass of Graph that implements faster node searches based on the assumption that the nodes form a DAG. (It is approximately 20 times faster than Graph, in the larger scale.) Since FastGraph stores nodes as a hash-table, there are some constraints on node operations. If necessary, converting between (->fast-graph graph) and (->graph graph) can be done frequently with minimal impact on performance.","title":"[class] FastGraph"},{"location":"packages/caten.air/#generic-copy-graph","text":"(copy-graph graph) Creates a copy of the given graph.","title":"[generic] copy-graph"},{"location":"packages/caten.air/#function-make-graph","text":"(make-graph &rest nodes) Creates a new Graph object with the given nodes.","title":"[function] make-graph"},{"location":"packages/caten.air/#generic-graph-nodes","text":"NIL","title":"[generic] graph-nodes"},{"location":"packages/caten.air/#generic-id-value","text":"(id->value graph id) Returns a node whose node-writes includes the given id.","title":"[generic] id-&gt;value"},{"location":"packages/caten.air/#generic-id-users","text":"(id->users graph id) Returns a list of nodes whose node-reads includes the given id.","title":"[generic] id-&gt;users"},{"location":"packages/caten.air/#generic-remnode","text":"(remnode graph id) Removes all node where node-id = id from the given graph.","title":"[generic] remnode"},{"location":"packages/caten.air/#generic-insert-nodes","text":"(insert-nodes graph nodes) Inserts the given nodes (list) into the graph.","title":"[generic] insert-nodes"},{"location":"packages/caten.air/#function-fast-graph","text":"(->fast-graph graph) Creates a FastGraph object from the given graph.","title":"[function] -&gt;fast-graph"},{"location":"packages/caten.air/#generic-graph","text":"(->graph graph) Converts the given graph to a fast graph.","title":"[generic] -&gt;graph"},{"location":"packages/caten.air/#generic-verify-graph","text":"(verify-graph graph &key no-purge) Verify the consistency of the graphs and simplify them by doing following things: - Checks if all variables are immutable - All variables appeared in read, are also defined in writes. - Purge all isolated graph - Sort nodes by time. - Set no-purge=T to ignore purge isolated graph step. This has no effect on FastGraph.","title":"[generic] verify-graph"},{"location":"packages/caten.air/#simplifier","text":"","title":"Simplifier"},{"location":"packages/caten.air/#macro-defsimplifier","text":"(defsimplifier (name &key (speed 3)) &rest rules) Defines a new simplifier named name . The defined function has a following form: (name graph &key (no-verify nil) (return-changed-p nil)) The graph is a graph to simplify. The no-verify is a flag to skip the verification process. The return-changed-p is a flag to return the result of the simplification process, or a boolean indicating that the graph was changed during simplification process.. The speed is an optimization level. The rules are a list of simplification rules. Each rule has a form: (Node_Name (Read_Args) Attrs) (TODO: Documentation) (See also: ./source/aasm/constant-folding.lisp )","title":"[macro] defsimplifier"},{"location":"packages/caten.air/#visualizer","text":"","title":"Visualizer"},{"location":"packages/caten.air/#function-dot","text":"(->dot graph &key (pathname \"/tmp/graph.dot\") (open t) (title \"node\")) Visualizes the graph using graphviz(requirement). Set open=t to open the resulting image in the default browser. A tmp file is created at the pathname location. The graph is saved as a .png and .html file. The title is used in the html file.","title":"[function] -&gt;dot"},{"location":"packages/caten.air/#function-pprint-graph","text":"(pprint-graph graph &key (screen-width 140) (stream t)) The function pprint-graph prints the graph in a tree-like structure. screen-width controls the width of the output, if the graph is too wide and cannot fit in the screen, it will be split into multiple pages. lisp CATEN-USER> ( pprint-graph ( runtime-graph ( caten ( !gelu ( make-tensor ` ( 3 3 ) :id 'x )))) :stream nil ) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {0} \u2514 :MUL {N1} \u251c :MUL {N2} \u2502 \u2514 Allocate[:float32] (3 3) \u2502 \u251c :MOVE {N3} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N4} \u2502 \u251c load(0.5) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N5} \u251c :MOVE {N6} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N7} \u2502 \u251c load(1.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N8} \u2514 :VIEW {N9} \u2502 \u251c load(-1.0) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N10} \u251c :MOVE {N11} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N12} \u2502 \u251c load(2.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :RECIP {N13} \u2514 :ADD {N14} \u251c :MOVE {N15} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N16} \u2502 \u251c load(1.0) \u2502 \u2514 Allocate[:float32] NIL \u2514 :EXP2 {N17} \u2514 :MUL {N18} \u2514 load(1.442695) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N19} \u2514 :VIEW {N20} \u2502 \u251c load(0.6931472) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N21} \u2514 :VIEW {N22} \u2502 \u251c load(-1.442695) \u2502 \u2514 Allocate[:float32] NIL \u251c :MUL {N23} \u251c :MOVE {N24} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N25} \u2502 \u251c load(2.0) \u2514 :MUL {N26} \u251c :MOVE {N27} \u2502 \u251c Allocate[:float32] (3 3) \u2502 \u2514 :VIEW {N28} \u2502 \u251c load(0.7978846) \u2502 \u2514 Allocate[:float32] NIL \u2514 :ADD {N29} \u251c Allocate[:float32] (3 3) \u2514 :MUL {N30} \u2514 :MUL {N31} \u2502 \u2514 Allocate[:float32] (3 3) \u2502 \u251c :MUL {N32} \u2502 \u251c Allocate[:float32] (3 3) \u2502 Allocate[:float32] (3 3) \u251c :MOVE {N33} \u251c Allocate[:float32] (3 3) \u2514 :VIEW {N34} \u251c load(0.044715) \u2514 Allocate[:float32] NIL ============================================================================================================================================","title":"[function] pprint-graph"},{"location":"packages/caten.api.differentiable_ops/","text":"Func Func Class [class] Func A CLOS class that represents a computation. [Func] -> <Lower> -> [FastGraph] -> <Simplifier> -> [Completed] Func (as the base class) is syntactic sugar for generating lowered instructions defined in the caten/aasm package. To properly lower the respective Func , you need to implement the following three methods: lower: Lower the Func into a list of caten/air:node . This should return caten/air:graph . forward: Create the type for the Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation. ShapeTracker might help you. (use the st macro) backward: Create the graph for the backward computation of op given prev-grad. Return: (values input_1.grad input_2.grad ...) . [generic] lower (lower op &rest nodes) Lowers the Func into a list of caten/air:node . This should return caten/air:graph. - op[Func] Func to lower. - nodes[list] list of previous nodes (each position corresponds to the position of the variables in the Func). [generic] forward (forward op &rest tensors) Create the type for the Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation. Use the st macro to create a new tensor. op[Func] Func to forward. tensors[list] list of input tensors. [generic] backward (backward op &optional prev-grad) Create the graph for the backward computation of op given prev-grad. Return: (values input_1.grad input_2.grad ...) . save-for-backward is determined automatically, so you do not have to consider about in-place operation. op[Func] Func to backward. prev-grad[Tensor] previous gradient tensor. Differentiable Ops (built_in) [function] !identity (!identity tensor) Equivalent to #'identity, but it is used to create a lazy computation node. lisp CATEN-USER> ( proceed ( !identity ( make-tensor ` ( 3 3 ) :initial-element 1.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103377 ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :op #<PROCEEDNODE {1003C72ED3}> :requires-grad NIL :variables (STC103317) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !view (!view base &rest subscripts) Create a view node from the base tensor and subscripts. We refer to VIEW as a node creating tensor whose buffers are shared with the base tensor, but shapes, strides, dtypes, offsets, or dtypes are different. Subscripts has the following notation: t keep using the base view. fixnum refers to the specified element. e.g.: A[3] (a b) slices in the range of [a, b) (a b c) slices in the range of [a, b) with step c . c can be negative. In that case, b must be larger than a. For example: (10 0 -1) to reverse the elements in the axis. (:~ n) to broadcast the axis, with the size of n It is supported to compose multiple views; the viewed tensors can be created from the viewed tensors. lisp CATEN-USER> ( proceed ( !contiguous ( !view ( ax+b ` ( 10 10 ) 1 0 ) ` ( 3 6 ) ` ( 3 6 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC104263 ((33.0 34.0 35.0) (43.0 44.0 45.0) (53.0 54.0 55.0)) :op #<PROCEEDNODE {1003E28B53}> :requires-grad NIL :variables (STC103581) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !permute (!permute tensor &rest order) Returns a tensor that is a permutation of the original tensor. The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified. order can be passed as a list or separated arguments. That is, both of (!permute x 0 1) or (!permute x (list 0 1)) are valid. lisp CATEN-USER> ( proceed ( !contiguous ( !permute ( ax+b ` ( 10 10 ) 1 0 ) ` ( 1 0 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104454 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1003E91903}> :requires-grad NIL :variables (TID104271) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>} [function] !t (!t tensor) Transposes the last two axes of the tensor lisp CATEN-USER> ( proceed ( !contiguous ( !t ( ax+b ` ( 10 10 ) 1 0 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104645 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1003EFA683}> :requires-grad NIL :variables (TID104462) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>} [function] !transpose (!transpose tensor &optional (dim0 1) (dim1 0)) Transposes dim0 and dim1 . lisp CATEN-USER> ( proceed ( !contiguous ( !transpose ( ax+b ` ( 10 10 ) 1 0 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104836 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1004049D83}> :requires-grad NIL :variables (TID104653) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>} [function] !contiguous (!contiguous x &key (force nil)) If the tensor is viewed, then creates a copy of tensor with contiguous memory. Otherwise, return the original tensor. If force is set to T, it always creates a copy. lisp CATEN-USER> ( proceed ( !contiguous ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104935 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {1004096083}> :requires-grad NIL :variables (STC104843) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !copy (!copy x) Creates a copy of the tensor. In Caten, the in-place operations are automatically determined, so in general, you do not have to consider using it. lisp CATEN-USER> ( proceed ( !copy ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC105065 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {10040CDF73}> :requires-grad NIL :variables (STC104947) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !reshape (!reshape x &rest shape) Returns a same tensor but shape is changed. shape can be passed as a list or separated arguments. That is, both of (!reshape x '(1 2 3)) or (!reshape x 1 2 3) are valid. Shape is a list of integers, symbols, or tensors. If x is a viewed tensor, it creates a copy of the tensor with contiguous memory (but later JIT will try to eliminate this). lisp CATEN-USER> ( proceed ( !reshape ( ax+b ` ( 10 10 ) 1 0 ) ` ( 5 20 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (5 20) :id STC105262 ((0.0 1.0 2.0 3.0 4.0 ~ 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 ~ 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 ~ 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 ~ 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 ~ 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {100412F693}> :requires-grad NIL :variables (TID105073) :tracker #<TRACKER :order={row(0 1)} :shape=(5 20) :contiguous-p=T>} [function] !flatten (!flatten x &key (axis 1)) Flattens the input tensor into a 2D matrix. If input tensor has shape (d_0, d_1, ... d_n) then the output will have shape (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn). lisp CATEN-USER> ( proceed ( !flatten ( make-tensor ` ( 3 3 3 3 )) :axis 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 27) :id STC105567 ((0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {1004246673}> :requires-grad NIL :variables (TID105303) :tracker #<TRACKER :order={row(0 1)} :shape=(3 27) :contiguous-p=T>} [function] !uprank (!uprank x n) Returns a tensor with one is inserted at the beginning of the shape of x for n times. lisp CATEN-USER> ( proceed ( !uprank ( ax+b ` ( 10 10 ) 1 0 ) 2 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1 10 10) :id STC105830 ((((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)))) :op #<PROCEEDNODE {10042E17F3}> :requires-grad NIL :variables (TID105575) :tracker #<TRACKER :order={row(0 1 2 3)} :shape=(1 1 10 10) :contiguous-p=T>} [function] !repeat (!repeat x &rest repeats) Returns a tensor with the shape of x broadcasted by repeats . lisp CATEN-USER> ( proceed ( !repeat ( ax+b ` ( 1 10 ) 1 0 ) 10 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC108317 <<Error during rendering: Invalid index 10 for (SIMPLE-ARRAY SINGLE-FLOAT (10)), should be a non...>> :op #<PROCEEDNODE {1004A7D663}> :requires-grad NIL :variables (VID106200) :tracker #<TRACKER :order={row(0 1)} :shape=(10(0 10 1) 10(0 10 1)) :contiguous-p=NIL>} [function] !expand (!expand x &rest shape) Returns a tensor that is expanded to the shape that is specified. Expand can also increase the number of dimensions that a tensor has. lisp CATEN-USER> ( proceed ( !expand ( ax+b ` ( 1 10 ) 1 0 ) ` ( 10 10 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 10) :id STC108955 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0)) :op #<PROCEEDNODE {1004BAC4F3}> :requires-grad NIL :variables (VID108325) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 10) :contiguous-p=T>} [function] !move (!move a b &key (reduce nil)) Moves the element of b into a, returning a. If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !move ( ax+b ` ( 10 10 ) 0 0 ) ( ax+b ` ( 10 10 ) 0 2 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109123 ((2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0)) :op #<PROCEEDNODE {1004D4C1B3}> :requires-grad NIL :variables (STC108970) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !assign (!assign a b) Equivalent to doing (!move a b :reduce t) . Useful when you want to the value of lazy ops to an pre-allocated buffer, like KV-Cache. lisp CATEN-USER> ( proceed ( !assign ( ax+b ` ( 10 10 ) 0 0 ) ( ax+b ` ( 10 10 ) 0 2 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109291 ((2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0)) :op #<PROCEEDNODE {1004DAB4E3}> :requires-grad NIL :variables (STC109138) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !add (!add a b &key (reduce nil)) ;; or (!+ &rest tensors) Adds a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !add ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109460 ((0.0 2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0) (20.0 22.0 24.0 26.0 28.0 30.0 32.0 34.0 36.0 38.0) (40.0 42.0 44.0 46.0 48.0 50.0 52.0 54.0 56.0 58.0) (60.0 62.0 64.0 66.0 68.0 70.0 72.0 74.0 76.0 78.0) (80.0 82.0 84.0 86.0 88.0 90.0 92.0 94.0 96.0 98.0) (100.0 102.0 104.0 106.0 108.0 110.0 112.0 114.0 116.0 118.0) (120.0 122.0 124.0 126.0 128.0 130.0 132.0 134.0 136.0 138.0) (140.0 142.0 144.0 146.0 148.0 150.0 152.0 154.0 156.0 158.0) (160.0 162.0 164.0 166.0 168.0 170.0 172.0 174.0 176.0 178.0) (180.0 182.0 184.0 186.0 188.0 190.0 192.0 194.0 196.0 198.0)) :op #<PROCEEDNODE {1004DF93A3}> :requires-grad NIL :variables (STC109306) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !sub (!sub a b &key (reduce nil)) ;; or (!- &rest tensors) Subtracts b from a . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !sub ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109635 ((0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {1004E3AD13}> :requires-grad NIL :variables (STC109476) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !mul (!mul a b &key (reduce nil)) ;; or (!* &rest tensors) Multiplies a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !mul ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109810 ((0.0 1.0 4.0 9.0 16.0 25.0 36.0 49.0 64.0 81.0) (100.0 121.0 144.0 169.0 196.0 225.0 256.0 289.0 324.0 361.0) (400.0 441.0 484.0 529.0 576.0 625.0 676.0 729.0 784.0 841.0) (900.0 961.0 1024.0 1089.0 1156.0 1225.0 1296.0 1369.0 1444.0 1521.0) (1600.0 1681.0 1764.0 1849.0 1936.0 2025.0 2116.0 2209.0 2304.0 2401.0) (2500.0 2601.0 2704.0 2809.0 2916.0 3025.0 3136.0 3249.0 3364.0 3481.0) (3600.0 3721.0 3844.0 3969.0 4096.0 4225.0 4356.0 4489.0 4624.0 4761.0) (4900.0 5041.0 5184.0 5329.0 5476.0 5625.0 5776.0 5929.0 6084.0 6241.0) (6400.0 6561.0 6724.0 6889.0 7056.0 7225.0 7396.0 7569.0 7744.0 7921.0) (8100.0 8281.0 8464.0 8649.0 8836.0 9025.0 9216.0 9409.0 9604.0 9801.0)) :op #<PROCEEDNODE {1004E92E33}> :requires-grad NIL :variables (STC109650) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !div (!div a b &key (reduce nil)) ;; or (!/ &rest tensors) Divides a by b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !div ( ax+b ` ( 10 10 ) 1 1 ) ( ax+b ` ( 10 10 ) 1 1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC110000 ((1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (0.99999994 1.0 1.0 1.0 1.0 1.0 0.99999994 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 0.99999994 1.0 1.0 1.0 1.0 1.0) (0.99999994 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 0.99999994 0.99999994 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 0.99999994 1.0 1.0 0.99999994 1.0 1.0 1.0)) :op #<PROCEEDNODE {1004F129A3}> :requires-grad NIL :variables (STC109826) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !mod (!mod a b &key (reduce nil)) Computes the remainder of the division of a by b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !mod ( ax+b ` ( 10 10 ) 1 1 :dtype :int32 ) ( ax+b ` ( 10 10 ) 1 1 :dtype :int32 ))) Result Result {Tensor{LISPBUFFER}[int32] :shape (10 10) :id STC110173 ((0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0)) :op #<PROCEEDNODE {1005194943}> :requires-grad NIL :variables (STC110015) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !idiv (!idiv a b &key (reduce nil)) Assuming both of a and b are the integer, divides a by b and returns the integer part. If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !idiv ( ax+b ` ( 10 10 ) 3 1 :dtype :uint32 ) ( ax+b ` ( 10 10 ) 0 2 :dtype :uint32 ))) Result Result {Tensor{LISPBUFFER}[uint32] :shape (10 10) :id STC110337 ((0 2 3 5 6 8 9 11 12 14) (15 17 18 20 21 23 24 26 27 29) (30 32 33 35 36 38 39 41 42 44) (45 47 48 50 51 53 54 56 57 59) (60 62 63 65 66 68 69 71 72 74) (75 77 78 80 81 83 84 86 87 89) (90 92 93 95 96 98 99 101 102 104) (105 107 108 110 111 113 114 116 117 119) (120 122 123 125 126 128 129 131 132 134) (135 137 138 140 141 143 144 146 147 149)) :op #<PROCEEDNODE {100521B093}> :requires-grad NIL :variables (STC110188) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !maximum (!maximum a b &key (reduce nil)) Returns the maximum of a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !maximum ( rand ` ( 3 3 )) ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC134994 ((0.49818084 0.6545696 1.1272388) (0.71888524 0.86670333 0.97324157) (0.71676797 0.63761955 0.62686044)) :op #<PROCEEDNODE {1002DF3423}> :requires-grad NIL :variables (STC134273) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !minimum (!minimum a b &key (reduce nil)) Returns the minimum of a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !minimum ( rand ` ( 3 3 )) ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC135746 ((0.037297387 -2.037279 -0.49524468) (0.083971575 0.3244131 0.39060554) (-0.52718425 -1.4125385 0.8568455)) :op #<PROCEEDNODE {10030FC523}> :requires-grad NIL :variables (STC135010) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !gcd (!gcd a b &key (reduce nil)) Returns the greatest common divisor of a and b . If reduce is T, it will reduce the result. (Broadcast) a , b are expected to be integer scalars. (dedicated to the view computation) lisp CATEN-USER> ( ctx:with-contextvar ( :backend \"lisp\" ) ( proceed ( !gcd ( iconst 8 ) ( iconst 4 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC135767 4 :op #<PROCEEDNODE {1003249853}> :requires-grad NIL :variables (STC135749) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !lcm (!lcm a b) Returns the least common multiple of a and b . a , b are expected to be integer scalars. lisp CATEN-USER> ( ctx:with-contextvar ( :backend \"lisp\" ) ( proceed ( !lcm ( iconst 8 ) ( iconst 4 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC135829 8 :op #<PROCEEDNODE {10032B8443}> :requires-grad NIL :variables (STC135773) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !exp (!exp x) Computes (exp x) . lisp CATEN-USER> ( proceed ( !exp ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC135951 ((1.0 1.0100502 1.0202013 1.0304545 1.0408108 1.0512711 1.0618366 1.0725082 1.0832871 1.0941743) (1.105171 1.116278 1.1274968 1.1388284 1.1502738 1.1618342 1.1735109 1.1853049 1.1972173 1.2092496) (1.2214028 1.2336781 1.2460767 1.2586 1.2712492 1.2840254 1.2969301 1.3099644 1.3231298 1.3364275) (1.3498588 1.3634251 1.3771278 1.3909681 1.4049476 1.4190675 1.4333293 1.4477346 1.4622846 1.4769808) (1.4918246 1.5068177 1.5219616 1.5372574 1.5527072 1.5683122 1.5840739 1.5999942 1.6160744 1.6323161) (1.6487212 1.6652912 1.6820276 1.6989322 1.7160068 1.733253 1.7506725 1.768267 1.7860384 1.8039883) (1.8221188 1.8404315 1.8589281 1.8776106 1.8964808 1.9155407 1.9347923 1.9542372 1.9738777 1.9937155) (2.0137527 2.033991 2.054433 2.0750806 2.0959353 2.1169999 2.138276 2.1597662 2.181472 2.2033963) (2.2255409 2.2479079 2.2704997 2.2933185 2.316367 2.3396468 2.3631606 2.386911 2.4108996 2.4351294) (2.4596028 2.4843223 2.5092902 2.5345092 2.5599813 2.5857096 2.6116965 2.6379445 2.664456 2.691234)) :op #<PROCEEDNODE {1003320683}> :requires-grad NIL :variables (STC135837) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !log (!log x) Computes (log x) . lisp CATEN-USER> ( proceed ( !log ( ax+b ` ( 10 10 ) 0.01 0.001 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136062 ((-6.9077554 -4.50986 -3.8632329 -3.4737678 -3.1941833 -2.97593 -2.7968814 -2.6450753 -2.5133061 -2.396896) (-2.2926347 -2.198225 -2.1119647 -2.032558 -1.9589953 -1.8904755 -1.8263509 -1.7660917 -1.7092583 -1.6554819) (-1.6044505 -1.555897 -1.5095925 -1.4653376 -1.4229584 -1.3823024 -1.343235 -1.3056365 -1.2694006 -1.2344321) (-1.2006451 -1.1679624 -1.1363143 -1.105637 -1.0758728 -1.046969 -1.0188774 -0.99155325 -0.964956 -0.93904775) (-0.9137939 -0.8891622 -0.8651225 -0.84164727 -0.81871045 -0.796288 -0.7743574 -0.7528972 -0.73188806 -0.7113112) (-0.6911492 -0.6713857 -0.6520053 -0.63299334 -0.6143361 -0.59602046 -0.5780344 -0.5603661 -0.5430046 -0.52593935) (-0.5091604 -0.49265832 -0.47642422 -0.46044946 -0.44472587 -0.42924568 -0.41400146 -0.39898625 -0.38419297 -0.36961547) (-0.35524744 -0.3410829 -0.3271162 -0.3133419 -0.29975465 -0.28634965 -0.27312195 -0.26006696 -0.24718018 -0.23445737) (-0.22189441 -0.20948724 -0.19723219 -0.18512551 -0.17316367 -0.1613432 -0.14966084 -0.1381133 -0.12669767 -0.11541088) (-0.10425006 -0.093212426 -0.082295306 -0.07149601 -0.060812157 -0.050241243 -0.039780907 -0.029428856 -0.019182874 -0.009040808)) :op #<PROCEEDNODE {1003378C73}> :requires-grad NIL :variables (STC135959) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !sqrt (!sqrt x) Computes (sqrt x) . lisp CATEN-USER> ( proceed ( !sqrt ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136342 ((0.0 0.1 0.14142135 0.17320508 0.2 0.2236068 0.24494897 0.26457512 0.2828427 0.29999998) (0.31622776 0.33166248 0.34641016 0.3605551 0.37416574 0.38729832 0.4 0.41231057 0.42426407 0.4358899) (0.4472136 0.45825756 0.4690416 0.47958314 0.48989794 0.5 0.50990194 0.51961523 0.52915025 0.53851646) (0.5477225 0.55677646 0.5656854 0.5744563 0.5830952 0.591608 0.59999996 0.60827625 0.61644137 0.6244998) (0.6324555 0.64031243 0.64807403 0.65574384 0.66332495 0.67082036 0.67823297 0.6855655 0.6928203 0.7) (0.70710677 0.71414286 0.7211102 0.72801095 0.7348469 0.7416199 0.7483315 0.7549834 0.7615773 0.76811457) (0.77459663 0.781025 0.7874008 0.7937254 0.8 0.8062258 0.8124038 0.81853527 0.82462114 0.83066237) (0.83666 0.84261495 0.84852815 0.85440034 0.86023253 0.8660254 0.8717798 0.8774964 0.8831761 0.8888194) (0.8944272 0.9 0.9055385 0.91104335 0.9165151 0.92195445 0.92736185 0.9327379 0.9380832 0.9433981) (0.94868326 0.9539392 0.9591663 0.96436507 0.96953595 0.9746794 0.9797959 0.98488575 0.98994946 0.9949874)) :op #<PROCEEDNODE {100341D103}> :requires-grad NIL :variables (STC136070) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !neg (!neg tensor) !neg computes the negative value of the tensor. lisp CATEN-USER> ( proceed ( !neg ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136445 ((-0.0 -0.01 -0.02 -0.03 -0.04 -0.049999997 -0.06 -0.07 -0.08 -0.089999996) (-0.099999994 -0.11 -0.12 -0.13 -0.14 -0.14999999 -0.16 -0.17 -0.17999999 -0.19) (-0.19999999 -0.21 -0.22 -0.22999999 -0.24 -0.25 -0.26 -0.26999998 -0.28 -0.29) (-0.29999998 -0.31 -0.32 -0.32999998 -0.34 -0.35 -0.35999998 -0.37 -0.38 -0.39) (-0.39999998 -0.41 -0.42 -0.42999998 -0.44 -0.45 -0.45999998 -0.47 -0.48 -0.48999998) (-0.5 -0.51 -0.52 -0.53 -0.53999996 -0.55 -0.56 -0.57 -0.58 -0.59) (-0.59999996 -0.61 -0.62 -0.63 -0.64 -0.65 -0.65999997 -0.66999996 -0.68 -0.69) (-0.7 -0.71 -0.71999997 -0.72999996 -0.74 -0.75 -0.76 -0.77 -0.78 -0.78999996) (-0.79999995 -0.81 -0.82 -0.83 -0.84 -0.84999996 -0.85999995 -0.87 -0.88 -0.89) (-0.9 -0.90999997 -0.91999996 -0.93 -0.94 -0.95 -0.96 -0.96999997 -0.97999996 -0.98999995)) :op #<PROCEEDNODE {100345CFF3}> :requires-grad NIL :variables (STC136350) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !recip (!recip tensor) !recip computes the reciprocal of the tensor. lisp CATEN-USER> ( proceed ( !recip ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136556 ((10.0 9.090909 8.333333 7.692308 7.142857 6.6666665 6.25 5.882353 5.5555553 5.263158) (5.0000005 4.7619047 4.5454545 4.3478265 4.1666665 4.0 3.846154 3.7037036 3.5714285 3.448276) (3.3333335 3.2258065 3.125 3.0303032 2.9411764 2.857143 2.777778 2.702703 2.631579 2.5641026) (2.5000002 2.4390244 2.3809524 2.3255816 2.2727273 2.2222223 2.1739132 2.1276596 2.0833335 2.0408163) (2.0000002 1.9607843 1.923077 1.8867925 1.8518518 1.8181818 1.7857143 1.754386 1.724138 1.6949153) (1.6666666 1.6393442 1.6129032 1.5873016 1.5625 1.5384614 1.5151515 1.4925373 1.4705882 1.4492754) (1.4285715 1.4084506 1.3888888 1.369863 1.3513514 1.3333334 1.3157895 1.2987013 1.2820512 1.2658228) (1.25 1.2345679 1.2195122 1.2048193 1.1904762 1.1764705 1.1627907 1.1494253 1.1363636 1.1235955) (1.1111112 1.098901 1.0869565 1.0752689 1.0638298 1.0526316 1.0416667 1.0309278 1.0204082 1.010101) (1.0 0.990099 0.98039216 0.97087383 0.9615385 0.952381 0.9433963 0.9345795 0.92592597 0.91743124)) :op #<PROCEEDNODE {10034AAA63}> :requires-grad NIL :variables (STC136453) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !square (!square x) Computes the x*x lisp CATEN-USER> ( proceed ( !square ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136664 ((0.010000001 0.0121 0.014400001 0.0169 0.0196 0.0225 0.0256 0.028900001 0.0324 0.0361) (0.039999995 0.044100005 0.0484 0.052899994 0.057600003 0.0625 0.0676 0.072900005 0.0784 0.08409999) (0.08999999 0.0961 0.1024 0.10889999 0.115600005 0.122499995 0.12959999 0.13689998 0.1444 0.15209998) (0.15999998 0.1681 0.17639999 0.18489999 0.1936 0.20249999 0.21159998 0.2209 0.2304 0.24009998) (0.24999997 0.26009998 0.2704 0.28089997 0.29160002 0.3025 0.3136 0.3249 0.33639997 0.34809998) (0.36 0.37210003 0.3844 0.3969 0.4096 0.42250004 0.43560004 0.4489 0.46240002 0.4761) (0.48999998 0.5041 0.5184 0.53290004 0.54760003 0.5625 0.5776 0.5929 0.60840005 0.6241) (0.64000005 0.6561 0.6724 0.6889 0.7056001 0.7225 0.7396 0.7569 0.7744 0.79209995) (0.80999994 0.8281 0.8464 0.8649 0.8836 0.9025 0.9216 0.9409 0.96040004 0.98010004) (1.0 1.0201 1.0403999 1.0609 1.0816 1.1024998 1.1235999 1.1448998 1.1663998 1.1880999)) :op #<PROCEEDNODE {10034E7A03}> :requires-grad NIL :variables (STC136564) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !rsqrt (!rsqrt x) Computes the reciprocal of sqrt x. lisp CATEN-USER> ( proceed ( !rsqrt ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136958 ((3.1622777 3.0151134 2.8867514 2.7735012 2.6726124 2.5819888 2.5 2.4253561 2.3570225 2.2941573) (2.236068 2.1821787 2.1320071 2.0851443 2.0412414 2.0 1.9611614 1.924501 1.8898224 1.8569535) (1.825742 1.7960529 1.7677671 1.7407765 1.7149858 1.6903085 1.6666667 1.6439899 1.6222143 1.6012815) (1.5811388 1.5617375 1.5430336 1.5249858 1.5075567 1.490712 1.4744196 1.4586499 1.4433757 1.4285715) (1.4142135 1.40028 1.3867506 1.3736057 1.3608276 1.3483996 1.3363062 1.3245324 1.3130643 1.3018892) (1.2909944 1.2803688 1.2700013 1.2598816 1.25 1.2403474 1.2309148 1.2216945 1.2126781 1.2038586) (1.1952286 1.1867816 1.1785113 1.1704115 1.1624764 1.1547005 1.1470786 1.1396058 1.132277 1.1250879) (1.118034 1.1111112 1.1043153 1.0976427 1.0910894 1.0846523 1.0783278 1.0721126 1.0660036 1.0599979) (1.0540926 1.0482849 1.0425721 1.0369517 1.0314213 1.0259783 1.0206207 1.0153462 1.0101525 1.0050378) (1.0 0.99503714 0.99014753 0.98532933 0.9805807 0.9759001 0.97128594 0.96673656 0.9622505 0.95782626)) :op #<PROCEEDNODE {100359CFC3}> :requires-grad NIL :variables (STC136673) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !signum (!signum x) Returns the sign of the tensor. If the tensor is positive, it returns 1. If the tensor is negative, it returns -1. If the tensor is zero, it returns 0. Note that this function is not differentiable. lisp CATEN-USER> ( proceed ( !signum ( ax+b ` ( 10 10 ) 0.02 -0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC140119 ((-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0)) :op #<PROCEEDNODE {1003C76403}> :requires-grad NIL :variables (STC137558) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !gid (!gid tensor rank &key (dtype *default-float*)) Finds the rank th index components of the tensor. For example (!gid x 1) for (3 3) tensor is a `(0 1 2). (As a hint) by combining !gid with !where, you can implement a pseudo random access of the tensor. For example: lisp CATEN-USER> ( proceed ( !gid ( make-tensor ` ( 3 3 )) -1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC140744 ((0.0 1.0 2.0) (0.0 1.0 2.0) (0.0 1.0 2.0)) :op #<PROCEEDNODE {1003DD9F33}> :requires-grad NIL :variables (VID140129) :tracker #<TRACKER :order={row(0 1)} :shape=(~3 3) :contiguous-p=T>} [function] !normalize-axis (!normalize-axis ndim axis) Creates a tensor graph which normalizes the axis. If the axis is negative, then it will be normalized to the positive axis. lisp CATEN-USER> ( proceed ( !normalize-axis 3 -1 )) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC140800 2 :op #<PROCEEDNODE {1003E46BF3}> :requires-grad NIL :variables (STC140751) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !abs (!abs x) Returns the absolute value of the tensor. lisp CATEN-USER> ( proceed ( !abs ( ax+b ` ( 10 10 ) 0.02 -0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC143970 ((0.1 0.08 0.060000002 0.040000003 0.020000003 7.450581e-9 0.019999996 0.04 0.059999995 0.07999999) (0.09999999 0.12 0.13999999 0.16 0.18 0.19999999 0.22 0.24000001 0.26 0.28) (0.29999998 0.32 0.34 0.35999998 0.38 0.4 0.42 0.43999997 0.46 0.48) (0.49999997 0.52 0.53999996 0.55999994 0.58 0.59999996 0.61999995 0.64 0.65999997 0.67999995) (0.6999999 0.71999997 0.73999995 0.75999993 0.78 0.79999995 0.81999993 0.84 0.85999995 0.87999994) (0.9 0.91999996 0.93999994 0.9599999 0.9799999 1.0 1.02 1.04 1.06 1.0799999) (1.0999999 1.12 1.14 1.16 1.18 1.1999999 1.2199999 1.2399999 1.26 1.28) (1.3 1.3199999 1.3399999 1.3599999 1.38 1.4 1.42 1.4399999 1.4599999 1.4799999) (1.4999999 1.52 1.54 1.56 1.5799999 1.5999999 1.6199999 1.64 1.66 1.68) (1.6999999 1.7199999 1.7399999 1.76 1.78 1.8 1.8199999 1.8399999 1.8599999 1.8799999)) :op #<PROCEEDNODE {10048E2103}> :requires-grad NIL :variables (STC141401) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>} [function] !> (!> x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !> ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC145421 ((1 1 1) (0 0 1) (1 1 1)) :op #<PROCEEDNODE {1004DB48C3}> :requires-grad NIL :variables (STC144183) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !< (!< x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !< ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC146864 ((0 1 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1005246D33}> :requires-grad NIL :variables (STC145634) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !>= (!>= x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !>= ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC148321 ((1 0 1) (1 1 1) (1 0 1)) :op #<PROCEEDNODE {10077CA8F3}> :requires-grad NIL :variables (STC147077) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !<= (!<= x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !<= ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC149770 ((0 0 0) (0 0 0) (1 1 0)) :op #<PROCEEDNODE {1007C34733}> :requires-grad NIL :variables (STC148534) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !eq (!EQ x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !eq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC151215 ((0 0 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1008118723}> :requires-grad NIL :variables (STC149983) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !neq (!NEQ x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !neq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC152658 ((1 1 1) (1 1 1) (1 1 1)) :op #<PROCEEDNODE {10092D8513}> :requires-grad NIL :variables (STC151428) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !and (!AND x y) Computes the logical/bitwise and of the tensor. lisp CATEN-USER> ( proceed ( !and ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152675 1 :op #<PROCEEDNODE {10092DFA33}> :requires-grad NIL :variables (STC152661) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !xor (!XOR x y) Computes the logical/bitwise xor of the tensor. lisp CATEN-USER> ( proceed ( !xor ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152692 6 :op #<PROCEEDNODE {10092F7013}> :requires-grad NIL :variables (STC152678) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !or (!OR x y) Computes the logical/bitwise or of the tensor. lisp CATEN-USER> ( proceed ( !or ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152709 7 :op #<PROCEEDNODE {100930E643}> :requires-grad NIL :variables (STC152695) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [function] !where (!where condition x y) Selects elements from x or y based on the condition . If the condition is true, it selects the element from x , otherwise from y . lisp CATEN-USER> ( proceed ( !where ( !eq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC154154 ((0 0 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1002E52583}> :requires-grad NIL :variables (STC152922) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !const (!const tensor value) Creates a constant tensor with the specified value from the tensor. lisp CATEN-USER> ( proceed ( !const ( make-tensor ` ( 3 3 )) 1.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape NIL :id STC154168 1.0 :op #<PROCEEDNODE {1002E60333}> :requires-grad NIL :variables (SID154159) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} [generic] !index-components (!index-components object) Returns the index components of the tensor. object can be either of tensor or list. lisp CATEN-USER> ( proceed ( !index-components ( make-tensor ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC154250 ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :op #<PROCEEDNODE {1002E79AD3}> :requires-grad NIL :variables (STC154173) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} lisp CATEN-USER> ( proceed ( !index-components ` ( 1 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 3) :id STC154332 ((0.0 1.0 2.0)) :op #<PROCEEDNODE {1002E8B253}> :requires-grad NIL :variables (STC154255) :tracker #<TRACKER :order={row(0 1)} :shape=(1 3) :contiguous-p=T>}","title":"Func"},{"location":"packages/caten.api.differentiable_ops/#func","text":"","title":"Func"},{"location":"packages/caten.api.differentiable_ops/#func-class","text":"","title":"Func Class"},{"location":"packages/caten.api.differentiable_ops/#class-func","text":"A CLOS class that represents a computation. [Func] -> <Lower> -> [FastGraph] -> <Simplifier> -> [Completed] Func (as the base class) is syntactic sugar for generating lowered instructions defined in the caten/aasm package. To properly lower the respective Func , you need to implement the following three methods: lower: Lower the Func into a list of caten/air:node . This should return caten/air:graph . forward: Create the type for the Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation. ShapeTracker might help you. (use the st macro) backward: Create the graph for the backward computation of op given prev-grad. Return: (values input_1.grad input_2.grad ...) .","title":"[class] Func"},{"location":"packages/caten.api.differentiable_ops/#generic-lower","text":"(lower op &rest nodes) Lowers the Func into a list of caten/air:node . This should return caten/air:graph. - op[Func] Func to lower. - nodes[list] list of previous nodes (each position corresponds to the position of the variables in the Func).","title":"[generic] lower"},{"location":"packages/caten.api.differentiable_ops/#generic-forward","text":"(forward op &rest tensors) Create the type for the Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation. Use the st macro to create a new tensor. op[Func] Func to forward. tensors[list] list of input tensors.","title":"[generic] forward"},{"location":"packages/caten.api.differentiable_ops/#generic-backward","text":"(backward op &optional prev-grad) Create the graph for the backward computation of op given prev-grad. Return: (values input_1.grad input_2.grad ...) . save-for-backward is determined automatically, so you do not have to consider about in-place operation. op[Func] Func to backward. prev-grad[Tensor] previous gradient tensor.","title":"[generic] backward"},{"location":"packages/caten.api.differentiable_ops/#differentiable-ops-built_in","text":"","title":"Differentiable Ops (built_in)"},{"location":"packages/caten.api.differentiable_ops/#function-identity","text":"(!identity tensor) Equivalent to #'identity, but it is used to create a lazy computation node. lisp CATEN-USER> ( proceed ( !identity ( make-tensor ` ( 3 3 ) :initial-element 1.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103377 ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :op #<PROCEEDNODE {1003C72ED3}> :requires-grad NIL :variables (STC103317) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !identity"},{"location":"packages/caten.api.differentiable_ops/#function-view","text":"(!view base &rest subscripts) Create a view node from the base tensor and subscripts. We refer to VIEW as a node creating tensor whose buffers are shared with the base tensor, but shapes, strides, dtypes, offsets, or dtypes are different. Subscripts has the following notation: t keep using the base view. fixnum refers to the specified element. e.g.: A[3] (a b) slices in the range of [a, b) (a b c) slices in the range of [a, b) with step c . c can be negative. In that case, b must be larger than a. For example: (10 0 -1) to reverse the elements in the axis. (:~ n) to broadcast the axis, with the size of n It is supported to compose multiple views; the viewed tensors can be created from the viewed tensors. lisp CATEN-USER> ( proceed ( !contiguous ( !view ( ax+b ` ( 10 10 ) 1 0 ) ` ( 3 6 ) ` ( 3 6 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC104263 ((33.0 34.0 35.0) (43.0 44.0 45.0) (53.0 54.0 55.0)) :op #<PROCEEDNODE {1003E28B53}> :requires-grad NIL :variables (STC103581) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !view"},{"location":"packages/caten.api.differentiable_ops/#function-permute","text":"(!permute tensor &rest order) Returns a tensor that is a permutation of the original tensor. The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified. order can be passed as a list or separated arguments. That is, both of (!permute x 0 1) or (!permute x (list 0 1)) are valid. lisp CATEN-USER> ( proceed ( !contiguous ( !permute ( ax+b ` ( 10 10 ) 1 0 ) ` ( 1 0 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104454 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1003E91903}> :requires-grad NIL :variables (TID104271) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !permute"},{"location":"packages/caten.api.differentiable_ops/#function-t","text":"(!t tensor) Transposes the last two axes of the tensor lisp CATEN-USER> ( proceed ( !contiguous ( !t ( ax+b ` ( 10 10 ) 1 0 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104645 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1003EFA683}> :requires-grad NIL :variables (TID104462) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !t"},{"location":"packages/caten.api.differentiable_ops/#function-transpose","text":"(!transpose tensor &optional (dim0 1) (dim1 0)) Transposes dim0 and dim1 . lisp CATEN-USER> ( proceed ( !contiguous ( !transpose ( ax+b ` ( 10 10 ) 1 0 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104836 ((0.0 10.0 20.0 30.0 40.0 50.0 60.0 70.0 80.0 90.0) (1.0 11.0 21.0 31.0 41.0 51.0 61.0 71.0 81.0 91.0) (2.0 12.0 22.0 32.0 42.0 52.0 62.0 72.0 82.0 92.0) (3.0 13.0 23.0 33.0 43.0 53.0 63.0 73.0 83.0 93.0) (4.0 14.0 24.0 34.0 44.0 54.0 64.0 74.0 84.0 94.0) (5.0 15.0 25.0 35.0 45.0 55.0 65.0 75.0 85.0 95.0) (6.0 16.0 26.0 36.0 46.0 56.0 66.0 76.0 86.0 96.0) (7.0 17.0 27.0 37.0 47.0 57.0 67.0 77.0 87.0 97.0) (8.0 18.0 28.0 38.0 48.0 58.0 68.0 78.0 88.0 98.0) (9.0 19.0 29.0 39.0 49.0 59.0 69.0 79.0 89.0 99.0)) :op #<PROCEEDNODE {1004049D83}> :requires-grad NIL :variables (TID104653) :tracker #<TRACKER :order={row(1 0)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !transpose"},{"location":"packages/caten.api.differentiable_ops/#function-contiguous","text":"(!contiguous x &key (force nil)) If the tensor is viewed, then creates a copy of tensor with contiguous memory. Otherwise, return the original tensor. If force is set to T, it always creates a copy. lisp CATEN-USER> ( proceed ( !contiguous ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC104935 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {1004096083}> :requires-grad NIL :variables (STC104843) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !contiguous"},{"location":"packages/caten.api.differentiable_ops/#function-copy","text":"(!copy x) Creates a copy of the tensor. In Caten, the in-place operations are automatically determined, so in general, you do not have to consider using it. lisp CATEN-USER> ( proceed ( !copy ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC105065 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {10040CDF73}> :requires-grad NIL :variables (STC104947) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !copy"},{"location":"packages/caten.api.differentiable_ops/#function-reshape","text":"(!reshape x &rest shape) Returns a same tensor but shape is changed. shape can be passed as a list or separated arguments. That is, both of (!reshape x '(1 2 3)) or (!reshape x 1 2 3) are valid. Shape is a list of integers, symbols, or tensors. If x is a viewed tensor, it creates a copy of the tensor with contiguous memory (but later JIT will try to eliminate this). lisp CATEN-USER> ( proceed ( !reshape ( ax+b ` ( 10 10 ) 1 0 ) ` ( 5 20 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (5 20) :id STC105262 ((0.0 1.0 2.0 3.0 4.0 ~ 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 ~ 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 ~ 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 ~ 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 ~ 95.0 96.0 97.0 98.0 99.0)) :op #<PROCEEDNODE {100412F693}> :requires-grad NIL :variables (TID105073) :tracker #<TRACKER :order={row(0 1)} :shape=(5 20) :contiguous-p=T>}","title":"[function] !reshape"},{"location":"packages/caten.api.differentiable_ops/#function-flatten","text":"(!flatten x &key (axis 1)) Flattens the input tensor into a 2D matrix. If input tensor has shape (d_0, d_1, ... d_n) then the output will have shape (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn). lisp CATEN-USER> ( proceed ( !flatten ( make-tensor ` ( 3 3 3 3 )) :axis 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 27) :id STC105567 ((0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {1004246673}> :requires-grad NIL :variables (TID105303) :tracker #<TRACKER :order={row(0 1)} :shape=(3 27) :contiguous-p=T>}","title":"[function] !flatten"},{"location":"packages/caten.api.differentiable_ops/#function-uprank","text":"(!uprank x n) Returns a tensor with one is inserted at the beginning of the shape of x for n times. lisp CATEN-USER> ( proceed ( !uprank ( ax+b ` ( 10 10 ) 1 0 ) 2 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1 10 10) :id STC105830 ((((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0) (10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0) (20.0 21.0 22.0 23.0 24.0 25.0 26.0 27.0 28.0 29.0) (30.0 31.0 32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0) (40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0) (50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0) (60.0 61.0 62.0 63.0 64.0 65.0 66.0 67.0 68.0 69.0) (70.0 71.0 72.0 73.0 74.0 75.0 76.0 77.0 78.0 79.0) (80.0 81.0 82.0 83.0 84.0 85.0 86.0 87.0 88.0 89.0) (90.0 91.0 92.0 93.0 94.0 95.0 96.0 97.0 98.0 99.0)))) :op #<PROCEEDNODE {10042E17F3}> :requires-grad NIL :variables (TID105575) :tracker #<TRACKER :order={row(0 1 2 3)} :shape=(1 1 10 10) :contiguous-p=T>}","title":"[function] !uprank"},{"location":"packages/caten.api.differentiable_ops/#function-repeat","text":"(!repeat x &rest repeats) Returns a tensor with the shape of x broadcasted by repeats . lisp CATEN-USER> ( proceed ( !repeat ( ax+b ` ( 1 10 ) 1 0 ) 10 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC108317 <<Error during rendering: Invalid index 10 for (SIMPLE-ARRAY SINGLE-FLOAT (10)), should be a non...>> :op #<PROCEEDNODE {1004A7D663}> :requires-grad NIL :variables (VID106200) :tracker #<TRACKER :order={row(0 1)} :shape=(10(0 10 1) 10(0 10 1)) :contiguous-p=NIL>}","title":"[function] !repeat"},{"location":"packages/caten.api.differentiable_ops/#function-expand","text":"(!expand x &rest shape) Returns a tensor that is expanded to the shape that is specified. Expand can also increase the number of dimensions that a tensor has. lisp CATEN-USER> ( proceed ( !expand ( ax+b ` ( 1 10 ) 1 0 ) ` ( 10 10 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 10) :id STC108955 ((0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0)) :op #<PROCEEDNODE {1004BAC4F3}> :requires-grad NIL :variables (VID108325) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 10) :contiguous-p=T>}","title":"[function] !expand"},{"location":"packages/caten.api.differentiable_ops/#function-move","text":"(!move a b &key (reduce nil)) Moves the element of b into a, returning a. If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !move ( ax+b ` ( 10 10 ) 0 0 ) ( ax+b ` ( 10 10 ) 0 2 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109123 ((2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0)) :op #<PROCEEDNODE {1004D4C1B3}> :requires-grad NIL :variables (STC108970) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !move"},{"location":"packages/caten.api.differentiable_ops/#function-assign","text":"(!assign a b) Equivalent to doing (!move a b :reduce t) . Useful when you want to the value of lazy ops to an pre-allocated buffer, like KV-Cache. lisp CATEN-USER> ( proceed ( !assign ( ax+b ` ( 10 10 ) 0 0 ) ( ax+b ` ( 10 10 ) 0 2 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109291 ((2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0) (2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0)) :op #<PROCEEDNODE {1004DAB4E3}> :requires-grad NIL :variables (STC109138) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !assign"},{"location":"packages/caten.api.differentiable_ops/#function-add","text":"(!add a b &key (reduce nil)) ;; or (!+ &rest tensors) Adds a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !add ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109460 ((0.0 2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0) (20.0 22.0 24.0 26.0 28.0 30.0 32.0 34.0 36.0 38.0) (40.0 42.0 44.0 46.0 48.0 50.0 52.0 54.0 56.0 58.0) (60.0 62.0 64.0 66.0 68.0 70.0 72.0 74.0 76.0 78.0) (80.0 82.0 84.0 86.0 88.0 90.0 92.0 94.0 96.0 98.0) (100.0 102.0 104.0 106.0 108.0 110.0 112.0 114.0 116.0 118.0) (120.0 122.0 124.0 126.0 128.0 130.0 132.0 134.0 136.0 138.0) (140.0 142.0 144.0 146.0 148.0 150.0 152.0 154.0 156.0 158.0) (160.0 162.0 164.0 166.0 168.0 170.0 172.0 174.0 176.0 178.0) (180.0 182.0 184.0 186.0 188.0 190.0 192.0 194.0 196.0 198.0)) :op #<PROCEEDNODE {1004DF93A3}> :requires-grad NIL :variables (STC109306) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !add"},{"location":"packages/caten.api.differentiable_ops/#function-sub","text":"(!sub a b &key (reduce nil)) ;; or (!- &rest tensors) Subtracts b from a . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !sub ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109635 ((0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {1004E3AD13}> :requires-grad NIL :variables (STC109476) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !sub"},{"location":"packages/caten.api.differentiable_ops/#function-mul","text":"(!mul a b &key (reduce nil)) ;; or (!* &rest tensors) Multiplies a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !mul ( ax+b ` ( 10 10 ) 1 0 ) ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC109810 ((0.0 1.0 4.0 9.0 16.0 25.0 36.0 49.0 64.0 81.0) (100.0 121.0 144.0 169.0 196.0 225.0 256.0 289.0 324.0 361.0) (400.0 441.0 484.0 529.0 576.0 625.0 676.0 729.0 784.0 841.0) (900.0 961.0 1024.0 1089.0 1156.0 1225.0 1296.0 1369.0 1444.0 1521.0) (1600.0 1681.0 1764.0 1849.0 1936.0 2025.0 2116.0 2209.0 2304.0 2401.0) (2500.0 2601.0 2704.0 2809.0 2916.0 3025.0 3136.0 3249.0 3364.0 3481.0) (3600.0 3721.0 3844.0 3969.0 4096.0 4225.0 4356.0 4489.0 4624.0 4761.0) (4900.0 5041.0 5184.0 5329.0 5476.0 5625.0 5776.0 5929.0 6084.0 6241.0) (6400.0 6561.0 6724.0 6889.0 7056.0 7225.0 7396.0 7569.0 7744.0 7921.0) (8100.0 8281.0 8464.0 8649.0 8836.0 9025.0 9216.0 9409.0 9604.0 9801.0)) :op #<PROCEEDNODE {1004E92E33}> :requires-grad NIL :variables (STC109650) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !mul"},{"location":"packages/caten.api.differentiable_ops/#function-div","text":"(!div a b &key (reduce nil)) ;; or (!/ &rest tensors) Divides a by b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !div ( ax+b ` ( 10 10 ) 1 1 ) ( ax+b ` ( 10 10 ) 1 1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC110000 ((1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (0.99999994 1.0 1.0 1.0 1.0 1.0 0.99999994 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 0.99999994 1.0 1.0 1.0 1.0 1.0) (0.99999994 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 0.99999994 0.99999994 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 0.99999994 1.0 1.0 0.99999994 1.0 1.0 1.0)) :op #<PROCEEDNODE {1004F129A3}> :requires-grad NIL :variables (STC109826) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !div"},{"location":"packages/caten.api.differentiable_ops/#function-mod","text":"(!mod a b &key (reduce nil)) Computes the remainder of the division of a by b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !mod ( ax+b ` ( 10 10 ) 1 1 :dtype :int32 ) ( ax+b ` ( 10 10 ) 1 1 :dtype :int32 ))) Result Result {Tensor{LISPBUFFER}[int32] :shape (10 10) :id STC110173 ((0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0) (0 0 0 0 0 0 0 0 0 0)) :op #<PROCEEDNODE {1005194943}> :requires-grad NIL :variables (STC110015) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !mod"},{"location":"packages/caten.api.differentiable_ops/#function-idiv","text":"(!idiv a b &key (reduce nil)) Assuming both of a and b are the integer, divides a by b and returns the integer part. If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !idiv ( ax+b ` ( 10 10 ) 3 1 :dtype :uint32 ) ( ax+b ` ( 10 10 ) 0 2 :dtype :uint32 ))) Result Result {Tensor{LISPBUFFER}[uint32] :shape (10 10) :id STC110337 ((0 2 3 5 6 8 9 11 12 14) (15 17 18 20 21 23 24 26 27 29) (30 32 33 35 36 38 39 41 42 44) (45 47 48 50 51 53 54 56 57 59) (60 62 63 65 66 68 69 71 72 74) (75 77 78 80 81 83 84 86 87 89) (90 92 93 95 96 98 99 101 102 104) (105 107 108 110 111 113 114 116 117 119) (120 122 123 125 126 128 129 131 132 134) (135 137 138 140 141 143 144 146 147 149)) :op #<PROCEEDNODE {100521B093}> :requires-grad NIL :variables (STC110188) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !idiv"},{"location":"packages/caten.api.differentiable_ops/#function-maximum","text":"(!maximum a b &key (reduce nil)) Returns the maximum of a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !maximum ( rand ` ( 3 3 )) ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC134994 ((0.49818084 0.6545696 1.1272388) (0.71888524 0.86670333 0.97324157) (0.71676797 0.63761955 0.62686044)) :op #<PROCEEDNODE {1002DF3423}> :requires-grad NIL :variables (STC134273) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !maximum"},{"location":"packages/caten.api.differentiable_ops/#function-minimum","text":"(!minimum a b &key (reduce nil)) Returns the minimum of a and b . If reduce is T, it will reduce the result. (Broadcast) lisp CATEN-USER> ( proceed ( !minimum ( rand ` ( 3 3 )) ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC135746 ((0.037297387 -2.037279 -0.49524468) (0.083971575 0.3244131 0.39060554) (-0.52718425 -1.4125385 0.8568455)) :op #<PROCEEDNODE {10030FC523}> :requires-grad NIL :variables (STC135010) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !minimum"},{"location":"packages/caten.api.differentiable_ops/#function-gcd","text":"(!gcd a b &key (reduce nil)) Returns the greatest common divisor of a and b . If reduce is T, it will reduce the result. (Broadcast) a , b are expected to be integer scalars. (dedicated to the view computation) lisp CATEN-USER> ( ctx:with-contextvar ( :backend \"lisp\" ) ( proceed ( !gcd ( iconst 8 ) ( iconst 4 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC135767 4 :op #<PROCEEDNODE {1003249853}> :requires-grad NIL :variables (STC135749) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !gcd"},{"location":"packages/caten.api.differentiable_ops/#function-lcm","text":"(!lcm a b) Returns the least common multiple of a and b . a , b are expected to be integer scalars. lisp CATEN-USER> ( ctx:with-contextvar ( :backend \"lisp\" ) ( proceed ( !lcm ( iconst 8 ) ( iconst 4 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC135829 8 :op #<PROCEEDNODE {10032B8443}> :requires-grad NIL :variables (STC135773) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !lcm"},{"location":"packages/caten.api.differentiable_ops/#function-exp","text":"(!exp x) Computes (exp x) . lisp CATEN-USER> ( proceed ( !exp ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC135951 ((1.0 1.0100502 1.0202013 1.0304545 1.0408108 1.0512711 1.0618366 1.0725082 1.0832871 1.0941743) (1.105171 1.116278 1.1274968 1.1388284 1.1502738 1.1618342 1.1735109 1.1853049 1.1972173 1.2092496) (1.2214028 1.2336781 1.2460767 1.2586 1.2712492 1.2840254 1.2969301 1.3099644 1.3231298 1.3364275) (1.3498588 1.3634251 1.3771278 1.3909681 1.4049476 1.4190675 1.4333293 1.4477346 1.4622846 1.4769808) (1.4918246 1.5068177 1.5219616 1.5372574 1.5527072 1.5683122 1.5840739 1.5999942 1.6160744 1.6323161) (1.6487212 1.6652912 1.6820276 1.6989322 1.7160068 1.733253 1.7506725 1.768267 1.7860384 1.8039883) (1.8221188 1.8404315 1.8589281 1.8776106 1.8964808 1.9155407 1.9347923 1.9542372 1.9738777 1.9937155) (2.0137527 2.033991 2.054433 2.0750806 2.0959353 2.1169999 2.138276 2.1597662 2.181472 2.2033963) (2.2255409 2.2479079 2.2704997 2.2933185 2.316367 2.3396468 2.3631606 2.386911 2.4108996 2.4351294) (2.4596028 2.4843223 2.5092902 2.5345092 2.5599813 2.5857096 2.6116965 2.6379445 2.664456 2.691234)) :op #<PROCEEDNODE {1003320683}> :requires-grad NIL :variables (STC135837) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !exp"},{"location":"packages/caten.api.differentiable_ops/#function-log","text":"(!log x) Computes (log x) . lisp CATEN-USER> ( proceed ( !log ( ax+b ` ( 10 10 ) 0.01 0.001 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136062 ((-6.9077554 -4.50986 -3.8632329 -3.4737678 -3.1941833 -2.97593 -2.7968814 -2.6450753 -2.5133061 -2.396896) (-2.2926347 -2.198225 -2.1119647 -2.032558 -1.9589953 -1.8904755 -1.8263509 -1.7660917 -1.7092583 -1.6554819) (-1.6044505 -1.555897 -1.5095925 -1.4653376 -1.4229584 -1.3823024 -1.343235 -1.3056365 -1.2694006 -1.2344321) (-1.2006451 -1.1679624 -1.1363143 -1.105637 -1.0758728 -1.046969 -1.0188774 -0.99155325 -0.964956 -0.93904775) (-0.9137939 -0.8891622 -0.8651225 -0.84164727 -0.81871045 -0.796288 -0.7743574 -0.7528972 -0.73188806 -0.7113112) (-0.6911492 -0.6713857 -0.6520053 -0.63299334 -0.6143361 -0.59602046 -0.5780344 -0.5603661 -0.5430046 -0.52593935) (-0.5091604 -0.49265832 -0.47642422 -0.46044946 -0.44472587 -0.42924568 -0.41400146 -0.39898625 -0.38419297 -0.36961547) (-0.35524744 -0.3410829 -0.3271162 -0.3133419 -0.29975465 -0.28634965 -0.27312195 -0.26006696 -0.24718018 -0.23445737) (-0.22189441 -0.20948724 -0.19723219 -0.18512551 -0.17316367 -0.1613432 -0.14966084 -0.1381133 -0.12669767 -0.11541088) (-0.10425006 -0.093212426 -0.082295306 -0.07149601 -0.060812157 -0.050241243 -0.039780907 -0.029428856 -0.019182874 -0.009040808)) :op #<PROCEEDNODE {1003378C73}> :requires-grad NIL :variables (STC135959) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !log"},{"location":"packages/caten.api.differentiable_ops/#function-sqrt","text":"(!sqrt x) Computes (sqrt x) . lisp CATEN-USER> ( proceed ( !sqrt ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136342 ((0.0 0.1 0.14142135 0.17320508 0.2 0.2236068 0.24494897 0.26457512 0.2828427 0.29999998) (0.31622776 0.33166248 0.34641016 0.3605551 0.37416574 0.38729832 0.4 0.41231057 0.42426407 0.4358899) (0.4472136 0.45825756 0.4690416 0.47958314 0.48989794 0.5 0.50990194 0.51961523 0.52915025 0.53851646) (0.5477225 0.55677646 0.5656854 0.5744563 0.5830952 0.591608 0.59999996 0.60827625 0.61644137 0.6244998) (0.6324555 0.64031243 0.64807403 0.65574384 0.66332495 0.67082036 0.67823297 0.6855655 0.6928203 0.7) (0.70710677 0.71414286 0.7211102 0.72801095 0.7348469 0.7416199 0.7483315 0.7549834 0.7615773 0.76811457) (0.77459663 0.781025 0.7874008 0.7937254 0.8 0.8062258 0.8124038 0.81853527 0.82462114 0.83066237) (0.83666 0.84261495 0.84852815 0.85440034 0.86023253 0.8660254 0.8717798 0.8774964 0.8831761 0.8888194) (0.8944272 0.9 0.9055385 0.91104335 0.9165151 0.92195445 0.92736185 0.9327379 0.9380832 0.9433981) (0.94868326 0.9539392 0.9591663 0.96436507 0.96953595 0.9746794 0.9797959 0.98488575 0.98994946 0.9949874)) :op #<PROCEEDNODE {100341D103}> :requires-grad NIL :variables (STC136070) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !sqrt"},{"location":"packages/caten.api.differentiable_ops/#function-neg","text":"(!neg tensor) !neg computes the negative value of the tensor. lisp CATEN-USER> ( proceed ( !neg ( ax+b ` ( 10 10 ) 0.01 0.0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136445 ((-0.0 -0.01 -0.02 -0.03 -0.04 -0.049999997 -0.06 -0.07 -0.08 -0.089999996) (-0.099999994 -0.11 -0.12 -0.13 -0.14 -0.14999999 -0.16 -0.17 -0.17999999 -0.19) (-0.19999999 -0.21 -0.22 -0.22999999 -0.24 -0.25 -0.26 -0.26999998 -0.28 -0.29) (-0.29999998 -0.31 -0.32 -0.32999998 -0.34 -0.35 -0.35999998 -0.37 -0.38 -0.39) (-0.39999998 -0.41 -0.42 -0.42999998 -0.44 -0.45 -0.45999998 -0.47 -0.48 -0.48999998) (-0.5 -0.51 -0.52 -0.53 -0.53999996 -0.55 -0.56 -0.57 -0.58 -0.59) (-0.59999996 -0.61 -0.62 -0.63 -0.64 -0.65 -0.65999997 -0.66999996 -0.68 -0.69) (-0.7 -0.71 -0.71999997 -0.72999996 -0.74 -0.75 -0.76 -0.77 -0.78 -0.78999996) (-0.79999995 -0.81 -0.82 -0.83 -0.84 -0.84999996 -0.85999995 -0.87 -0.88 -0.89) (-0.9 -0.90999997 -0.91999996 -0.93 -0.94 -0.95 -0.96 -0.96999997 -0.97999996 -0.98999995)) :op #<PROCEEDNODE {100345CFF3}> :requires-grad NIL :variables (STC136350) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !neg"},{"location":"packages/caten.api.differentiable_ops/#function-recip","text":"(!recip tensor) !recip computes the reciprocal of the tensor. lisp CATEN-USER> ( proceed ( !recip ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136556 ((10.0 9.090909 8.333333 7.692308 7.142857 6.6666665 6.25 5.882353 5.5555553 5.263158) (5.0000005 4.7619047 4.5454545 4.3478265 4.1666665 4.0 3.846154 3.7037036 3.5714285 3.448276) (3.3333335 3.2258065 3.125 3.0303032 2.9411764 2.857143 2.777778 2.702703 2.631579 2.5641026) (2.5000002 2.4390244 2.3809524 2.3255816 2.2727273 2.2222223 2.1739132 2.1276596 2.0833335 2.0408163) (2.0000002 1.9607843 1.923077 1.8867925 1.8518518 1.8181818 1.7857143 1.754386 1.724138 1.6949153) (1.6666666 1.6393442 1.6129032 1.5873016 1.5625 1.5384614 1.5151515 1.4925373 1.4705882 1.4492754) (1.4285715 1.4084506 1.3888888 1.369863 1.3513514 1.3333334 1.3157895 1.2987013 1.2820512 1.2658228) (1.25 1.2345679 1.2195122 1.2048193 1.1904762 1.1764705 1.1627907 1.1494253 1.1363636 1.1235955) (1.1111112 1.098901 1.0869565 1.0752689 1.0638298 1.0526316 1.0416667 1.0309278 1.0204082 1.010101) (1.0 0.990099 0.98039216 0.97087383 0.9615385 0.952381 0.9433963 0.9345795 0.92592597 0.91743124)) :op #<PROCEEDNODE {10034AAA63}> :requires-grad NIL :variables (STC136453) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !recip"},{"location":"packages/caten.api.differentiable_ops/#function-square","text":"(!square x) Computes the x*x lisp CATEN-USER> ( proceed ( !square ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136664 ((0.010000001 0.0121 0.014400001 0.0169 0.0196 0.0225 0.0256 0.028900001 0.0324 0.0361) (0.039999995 0.044100005 0.0484 0.052899994 0.057600003 0.0625 0.0676 0.072900005 0.0784 0.08409999) (0.08999999 0.0961 0.1024 0.10889999 0.115600005 0.122499995 0.12959999 0.13689998 0.1444 0.15209998) (0.15999998 0.1681 0.17639999 0.18489999 0.1936 0.20249999 0.21159998 0.2209 0.2304 0.24009998) (0.24999997 0.26009998 0.2704 0.28089997 0.29160002 0.3025 0.3136 0.3249 0.33639997 0.34809998) (0.36 0.37210003 0.3844 0.3969 0.4096 0.42250004 0.43560004 0.4489 0.46240002 0.4761) (0.48999998 0.5041 0.5184 0.53290004 0.54760003 0.5625 0.5776 0.5929 0.60840005 0.6241) (0.64000005 0.6561 0.6724 0.6889 0.7056001 0.7225 0.7396 0.7569 0.7744 0.79209995) (0.80999994 0.8281 0.8464 0.8649 0.8836 0.9025 0.9216 0.9409 0.96040004 0.98010004) (1.0 1.0201 1.0403999 1.0609 1.0816 1.1024998 1.1235999 1.1448998 1.1663998 1.1880999)) :op #<PROCEEDNODE {10034E7A03}> :requires-grad NIL :variables (STC136564) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !square"},{"location":"packages/caten.api.differentiable_ops/#function-rsqrt","text":"(!rsqrt x) Computes the reciprocal of sqrt x. lisp CATEN-USER> ( proceed ( !rsqrt ( ax+b ` ( 10 10 ) 0.01 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC136958 ((3.1622777 3.0151134 2.8867514 2.7735012 2.6726124 2.5819888 2.5 2.4253561 2.3570225 2.2941573) (2.236068 2.1821787 2.1320071 2.0851443 2.0412414 2.0 1.9611614 1.924501 1.8898224 1.8569535) (1.825742 1.7960529 1.7677671 1.7407765 1.7149858 1.6903085 1.6666667 1.6439899 1.6222143 1.6012815) (1.5811388 1.5617375 1.5430336 1.5249858 1.5075567 1.490712 1.4744196 1.4586499 1.4433757 1.4285715) (1.4142135 1.40028 1.3867506 1.3736057 1.3608276 1.3483996 1.3363062 1.3245324 1.3130643 1.3018892) (1.2909944 1.2803688 1.2700013 1.2598816 1.25 1.2403474 1.2309148 1.2216945 1.2126781 1.2038586) (1.1952286 1.1867816 1.1785113 1.1704115 1.1624764 1.1547005 1.1470786 1.1396058 1.132277 1.1250879) (1.118034 1.1111112 1.1043153 1.0976427 1.0910894 1.0846523 1.0783278 1.0721126 1.0660036 1.0599979) (1.0540926 1.0482849 1.0425721 1.0369517 1.0314213 1.0259783 1.0206207 1.0153462 1.0101525 1.0050378) (1.0 0.99503714 0.99014753 0.98532933 0.9805807 0.9759001 0.97128594 0.96673656 0.9622505 0.95782626)) :op #<PROCEEDNODE {100359CFC3}> :requires-grad NIL :variables (STC136673) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !rsqrt"},{"location":"packages/caten.api.differentiable_ops/#function-signum","text":"(!signum x) Returns the sign of the tensor. If the tensor is positive, it returns 1. If the tensor is negative, it returns -1. If the tensor is zero, it returns 0. Note that this function is not differentiable. lisp CATEN-USER> ( proceed ( !signum ( ax+b ` ( 10 10 ) 0.02 -0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC140119 ((-1.0 -1.0 -1.0 -1.0 -1.0 -1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0) (1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0)) :op #<PROCEEDNODE {1003C76403}> :requires-grad NIL :variables (STC137558) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !signum"},{"location":"packages/caten.api.differentiable_ops/#function-gid","text":"(!gid tensor rank &key (dtype *default-float*)) Finds the rank th index components of the tensor. For example (!gid x 1) for (3 3) tensor is a `(0 1 2). (As a hint) by combining !gid with !where, you can implement a pseudo random access of the tensor. For example: lisp CATEN-USER> ( proceed ( !gid ( make-tensor ` ( 3 3 )) -1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC140744 ((0.0 1.0 2.0) (0.0 1.0 2.0) (0.0 1.0 2.0)) :op #<PROCEEDNODE {1003DD9F33}> :requires-grad NIL :variables (VID140129) :tracker #<TRACKER :order={row(0 1)} :shape=(~3 3) :contiguous-p=T>}","title":"[function] !gid"},{"location":"packages/caten.api.differentiable_ops/#function-normalize-axis","text":"(!normalize-axis ndim axis) Creates a tensor graph which normalizes the axis. If the axis is negative, then it will be normalized to the positive axis. lisp CATEN-USER> ( proceed ( !normalize-axis 3 -1 )) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC140800 2 :op #<PROCEEDNODE {1003E46BF3}> :requires-grad NIL :variables (STC140751) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !normalize-axis"},{"location":"packages/caten.api.differentiable_ops/#function-abs","text":"(!abs x) Returns the absolute value of the tensor. lisp CATEN-USER> ( proceed ( !abs ( ax+b ` ( 10 10 ) 0.02 -0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (10 10) :id STC143970 ((0.1 0.08 0.060000002 0.040000003 0.020000003 7.450581e-9 0.019999996 0.04 0.059999995 0.07999999) (0.09999999 0.12 0.13999999 0.16 0.18 0.19999999 0.22 0.24000001 0.26 0.28) (0.29999998 0.32 0.34 0.35999998 0.38 0.4 0.42 0.43999997 0.46 0.48) (0.49999997 0.52 0.53999996 0.55999994 0.58 0.59999996 0.61999995 0.64 0.65999997 0.67999995) (0.6999999 0.71999997 0.73999995 0.75999993 0.78 0.79999995 0.81999993 0.84 0.85999995 0.87999994) (0.9 0.91999996 0.93999994 0.9599999 0.9799999 1.0 1.02 1.04 1.06 1.0799999) (1.0999999 1.12 1.14 1.16 1.18 1.1999999 1.2199999 1.2399999 1.26 1.28) (1.3 1.3199999 1.3399999 1.3599999 1.38 1.4 1.42 1.4399999 1.4599999 1.4799999) (1.4999999 1.52 1.54 1.56 1.5799999 1.5999999 1.6199999 1.64 1.66 1.68) (1.6999999 1.7199999 1.7399999 1.76 1.78 1.8 1.8199999 1.8399999 1.8599999 1.8799999)) :op #<PROCEEDNODE {10048E2103}> :requires-grad NIL :variables (STC141401) :tracker #<TRACKER :order={row(0 1)} :shape=(10 10) :contiguous-p=T>}","title":"[function] !abs"},{"location":"packages/caten.api.differentiable_ops/#function","text":"(!> x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !> ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC145421 ((1 1 1) (0 0 1) (1 1 1)) :op #<PROCEEDNODE {1004DB48C3}> :requires-grad NIL :variables (STC144183) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !&gt;"},{"location":"packages/caten.api.differentiable_ops/#function_1","text":"(!< x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !< ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC146864 ((0 1 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1005246D33}> :requires-grad NIL :variables (STC145634) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !&lt;"},{"location":"packages/caten.api.differentiable_ops/#function_2","text":"(!>= x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !>= ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC148321 ((1 0 1) (1 1 1) (1 0 1)) :op #<PROCEEDNODE {10077CA8F3}> :requires-grad NIL :variables (STC147077) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !&gt;="},{"location":"packages/caten.api.differentiable_ops/#function_3","text":"(!<= x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !<= ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC149770 ((0 0 0) (0 0 0) (1 1 0)) :op #<PROCEEDNODE {1007C34733}> :requires-grad NIL :variables (STC148534) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !&lt;="},{"location":"packages/caten.api.differentiable_ops/#function-eq","text":"(!EQ x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !eq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC151215 ((0 0 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1008118723}> :requires-grad NIL :variables (STC149983) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !eq"},{"location":"packages/caten.api.differentiable_ops/#function-neq","text":"(!NEQ x y) Compares x and y element-wise and returns the result as a boolean tensor. lisp CATEN-USER> ( proceed ( !where ( !neq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC152658 ((1 1 1) (1 1 1) (1 1 1)) :op #<PROCEEDNODE {10092D8513}> :requires-grad NIL :variables (STC151428) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !neq"},{"location":"packages/caten.api.differentiable_ops/#function-and","text":"(!AND x y) Computes the logical/bitwise and of the tensor. lisp CATEN-USER> ( proceed ( !and ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152675 1 :op #<PROCEEDNODE {10092DFA33}> :requires-grad NIL :variables (STC152661) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !and"},{"location":"packages/caten.api.differentiable_ops/#function-xor","text":"(!XOR x y) Computes the logical/bitwise xor of the tensor. lisp CATEN-USER> ( proceed ( !xor ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152692 6 :op #<PROCEEDNODE {10092F7013}> :requires-grad NIL :variables (STC152678) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !xor"},{"location":"packages/caten.api.differentiable_ops/#function-or","text":"(!OR x y) Computes the logical/bitwise or of the tensor. lisp CATEN-USER> ( proceed ( !or ( iconst 5 ) ( iconst 3 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC152709 7 :op #<PROCEEDNODE {100930E643}> :requires-grad NIL :variables (STC152695) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !or"},{"location":"packages/caten.api.differentiable_ops/#function-where","text":"(!where condition x y) Selects elements from x or y based on the condition . If the condition is true, it selects the element from x , otherwise from y . lisp CATEN-USER> ( proceed ( !where ( !eq ( rand ` ( 3 3 )) ( randn ` ( 3 3 ))) ( iconst 1 ) ( iconst 0 ))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 3) :id STC154154 ((0 0 0) (0 0 0) (0 0 0)) :op #<PROCEEDNODE {1002E52583}> :requires-grad NIL :variables (STC152922) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !where"},{"location":"packages/caten.api.differentiable_ops/#function-const","text":"(!const tensor value) Creates a constant tensor with the specified value from the tensor. lisp CATEN-USER> ( proceed ( !const ( make-tensor ` ( 3 3 )) 1.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape NIL :id STC154168 1.0 :op #<PROCEEDNODE {1002E60333}> :requires-grad NIL :variables (SID154159) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>}","title":"[function] !const"},{"location":"packages/caten.api.differentiable_ops/#generic-index-components","text":"(!index-components object) Returns the index components of the tensor. object can be either of tensor or list. lisp CATEN-USER> ( proceed ( !index-components ( make-tensor ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC154250 ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :op #<PROCEEDNODE {1002E79AD3}> :requires-grad NIL :variables (STC154173) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} lisp CATEN-USER> ( proceed ( !index-components ` ( 1 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 3) :id STC154332 ((0.0 1.0 2.0)) :op #<PROCEEDNODE {1002E8B253}> :requires-grad NIL :variables (STC154255) :tracker #<TRACKER :order={row(0 1)} :shape=(1 3) :contiguous-p=T>}","title":"[generic] !index-components"},{"location":"packages/caten.api.facet/","text":"Facet API We are going to see how to access the Tensor as a lisp object. [generic] change-facet (change-facet obj direction) change-facet converts obj to the data type specified by direction. During the conversion process, it attempts to synchronize the Buffer (i.e., no copying is performed). By default, :direction could be one of :tensor, :simple-array, :array. Users can extend this method if needed. lisp CATEN-USER> ( change-facet 1 :tensor ) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC339781 1 :op #<PROCEEDNODE {10093D75C3}> :requires-grad NIL :variables (SID339772) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} lisp CATEN-USER> ( change-facet ' (( 1.0 2.0 3.0 ) ( 4.0 5.0 6.0 )) :tensor ) Result Result {Tensor{LISPBUFFER}[float32] :shape (2 3) :id TID339782 ((1.0 2.0 3.0) (4.0 5.0 6.0)) :op #<ALLOCATE {10093D7E23}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(2 3) :contiguous-p=T>} lisp CATEN-USER> ( change-facet #( 1 2 3 ) :tensor ) Result Result {Tensor{LISPBUFFER}[int64] :shape (3) :id TID339786 (1 2 3) :op #<ALLOCATE {10093D7FC3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0)} :shape=(3) :contiguous-p=T>} lisp CATEN-USER> ( change-facet ( rand ` ( 2 2 )) :simple-array ) Result Result #(0.60825 0.5240749 0.87959164 0.27363405) lisp CATEN-USER> ( change-facet ( rand ` ( 2 2 )) :array ) Result Result #2A((0.7030305 0.46698764) (0.40553668 0.9920771)) [macro] with-facet (with-facet (bind (object &key (direction :array))) &body body) Binds the result of (change-facet object direction) to the bind . [macro] with-facets (with-facets ((&rest input-forms) &body body)) Expands to a series of with-facet forms. If you want to access an individual element of Tensor, it is wiser to convert it into an Array. The following code snippet initializes the diagonal of a Tensor to 0.0 without creating a copy: lisp CATEN-USER> ( let (( x ( rand ` ( 3 3 )))) ( with-facet ( a ( x :direction :array )) ( setf ( aref a 0 0 ) 0.0 ( aref a 1 1 ) 0.0 ( aref a 2 2 ) 0.0 )) ( print x )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID339800 ((0.0 0.41170076 0.84356534) (0.72477406 0.0 0.9174599) (0.46578503 0.55132806 0.0)) :op #<ALLOCATE {10095BF3E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"Facet API"},{"location":"packages/caten.api.facet/#facet-api","text":"We are going to see how to access the Tensor as a lisp object.","title":"Facet API"},{"location":"packages/caten.api.facet/#generic-change-facet","text":"(change-facet obj direction) change-facet converts obj to the data type specified by direction. During the conversion process, it attempts to synchronize the Buffer (i.e., no copying is performed). By default, :direction could be one of :tensor, :simple-array, :array. Users can extend this method if needed. lisp CATEN-USER> ( change-facet 1 :tensor ) Result Result {Tensor{LISPBUFFER}[int64] :shape NIL :id STC339781 1 :op #<PROCEEDNODE {10093D75C3}> :requires-grad NIL :variables (SID339772) :tracker #<TRACKER :order={rowNIL} :shape=() :contiguous-p=T>} lisp CATEN-USER> ( change-facet ' (( 1.0 2.0 3.0 ) ( 4.0 5.0 6.0 )) :tensor ) Result Result {Tensor{LISPBUFFER}[float32] :shape (2 3) :id TID339782 ((1.0 2.0 3.0) (4.0 5.0 6.0)) :op #<ALLOCATE {10093D7E23}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(2 3) :contiguous-p=T>} lisp CATEN-USER> ( change-facet #( 1 2 3 ) :tensor ) Result Result {Tensor{LISPBUFFER}[int64] :shape (3) :id TID339786 (1 2 3) :op #<ALLOCATE {10093D7FC3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0)} :shape=(3) :contiguous-p=T>} lisp CATEN-USER> ( change-facet ( rand ` ( 2 2 )) :simple-array ) Result Result #(0.60825 0.5240749 0.87959164 0.27363405) lisp CATEN-USER> ( change-facet ( rand ` ( 2 2 )) :array ) Result Result #2A((0.7030305 0.46698764) (0.40553668 0.9920771))","title":"[generic] change-facet"},{"location":"packages/caten.api.facet/#macro-with-facet","text":"(with-facet (bind (object &key (direction :array))) &body body) Binds the result of (change-facet object direction) to the bind .","title":"[macro] with-facet"},{"location":"packages/caten.api.facet/#macro-with-facets","text":"(with-facets ((&rest input-forms) &body body)) Expands to a series of with-facet forms. If you want to access an individual element of Tensor, it is wiser to convert it into an Array. The following code snippet initializes the diagonal of a Tensor to 0.0 without creating a copy: lisp CATEN-USER> ( let (( x ( rand ` ( 3 3 )))) ( with-facet ( a ( x :direction :array )) ( setf ( aref a 0 0 ) 0.0 ( aref a 1 1 ) 0.0 ( aref a 2 2 ) 0.0 )) ( print x )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID339800 ((0.0 0.41170076 0.84356534) (0.72477406 0.0 0.9174599) (0.46578503 0.55132806 0.0)) :op #<ALLOCATE {10095BF3E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[macro] with-facets"},{"location":"packages/caten.api.initializers/","text":"Initializers [function] set-manual-seed (set-manual-seed &key (seed 0)) Sets the seed for random operations. [macro] with-manual-seed (with-manual-seed (seed) &body body) Sets the seed for random operations within the scope of the body. [function] ax+b (ax+b shape a b &key (out nil) (dtype *default-float*) (order *default-order*)) Generates an array sampled from this formula: x_i = a * index_components(i) + b. There is a linspace function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( ax+b ` ( 3 3 ) 2 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC207777 ((1.0 3.0 5.0) (7.0 9.0 11.0) (13.0 15.0 17.0)) :op #<PROCEEDNODE {1003239673}> :requires-grad NIL :variables (STC207688) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( linspace ` ( 3 3 ) 1 0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID207778 ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :op #<ALLOCATE {1003269B63}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !rand (!rand shape &key (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [0, 1) . There is a rand function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !rand ` ( 3 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC215307 ((0.40309912 0.8688215 0.9638163) (0.032803092 0.918832 0.59222883) (0.53384453 0.48372424 0.8027758)) :op #<PROCEEDNODE {10072FC363}> :requires-grad NIL :variables (STC207840) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( rand ` ( 3 3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID215308 ((0.8544815 0.89949685 0.06944181) (0.78085196 7.4467063e-4 0.94722736) (0.047116876 0.9624656 0.6471626)) :op #<ALLOCATE {10073A1E13}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !normal (!normal shape &key (mean 0.0) (std 1.0) (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a normal distribution with the given mean and std . There is a normal function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !normal ` ( 3 3 ) :mean 0.0 :std 2.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC231243 ((1.9926395 -3.0460858 2.0359108) (-1.3665679 2.2740877 -0.34971234) (0.7738801 -0.7173807 1.5476315)) :op #<PROCEEDNODE {100394FD63}> :requires-grad NIL :variables (STC215320) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( normal ` ( 3 3 ) :mean 0.0 :std 1.0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID231244 ((-0.6933528 -0.618868 -0.09813504) (-0.3018336 0.73544437 -1.0356082) (-1.6392002 0.5641113 -0.094371416)) :op #<ALLOCATE {10038585C3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !randn (!randn shape &key (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a normal distribution with a mean of 0 and a standard deviation of 1. There is a randn function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !randn ` ( 3 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC263491 ((0.4054314 -2.6517396 -0.93262064) (-0.44230795 -0.18937276 -0.42351857) (0.09603451 -0.004643383 -0.9779817)) :op #<PROCEEDNODE {1001D978C3}> :requires-grad NIL :variables (STC248794) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( randn ` ( 3 3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID263492 ((-0.23316607 0.39065427 0.5918632) (-0.771492 1.3980544 -0.2156676) (-0.96204996 -0.6995641 1.4401776)) :op #<ALLOCATE {1001DEE6E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !uniform (!uniform shape &key (low 0.0) (high 1.0) (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [low, high) There is a uniform function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !uniform ` ( 3 3 ) :low 1.0 :high 2.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC272262 ((1.5054502 1.7935554 1.0513923) (1.4500921 1.7713478 1.5610737) (1.7057834 1.4408889 1.9995518)) :op #<PROCEEDNODE {10054A87E3}> :requires-grad NIL :variables (STC263504) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( uniform ` ( 3 3 ) :low 1.0 :high 2.0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID272263 ((1.5276278 1.196649 1.2917286) (1.0492711 1.2254326 1.0443223) (1.2354891 1.7603551 1.2700266)) :op #<ALLOCATE {1001A52793}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !randint (!randint shape &key (low 0) (high 1) (dtype *default-int*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [low, high) . There is a randint function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( ctx:with-contextvar ( :backend \"clang\" ) ( proceed ( !randint ` ( 3 3 ) :low 1 :high 10 ))) Result Result {Tensor{CLANGBUFFER}[int64] :shape (3 3) :id STC311011 ((5 5 6) (7 8 9) (1 2 6)) :op #<PROCEEDNODE {1002B03DA3}> :requires-grad NIL :variables (STC279893) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( ctx:with-contextvar ( :backend \"clang\" ) ( randint ` ( 3 3 ) :low 1 :high 10 :dtype :int32 )) Result Result {Tensor{CLANGBUFFER}[int32] :shape (3 3) :id TID311012 ((3 3 4) (7 4 3) (5 1 9)) :op #<ALLOCATE {100916C1F3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !full (!full shape fill-value &key (dtype *default-float*) (order *default-order*)) Initializes a tensor filled with fill-value . lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC339745 ((#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY)) :op #<PROCEEDNODE {10091948C3}> :requires-grad NIL :variables (TID339684) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] xavier-uniform No description provided [function] xavier-gaussian No description provided","title":"Initializers"},{"location":"packages/caten.api.initializers/#initializers","text":"","title":"Initializers"},{"location":"packages/caten.api.initializers/#function-set-manual-seed","text":"(set-manual-seed &key (seed 0)) Sets the seed for random operations.","title":"[function] set-manual-seed"},{"location":"packages/caten.api.initializers/#macro-with-manual-seed","text":"(with-manual-seed (seed) &body body) Sets the seed for random operations within the scope of the body.","title":"[macro] with-manual-seed"},{"location":"packages/caten.api.initializers/#function-axb","text":"(ax+b shape a b &key (out nil) (dtype *default-float*) (order *default-order*)) Generates an array sampled from this formula: x_i = a * index_components(i) + b. There is a linspace function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( ax+b ` ( 3 3 ) 2 1 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC207777 ((1.0 3.0 5.0) (7.0 9.0 11.0) (13.0 15.0 17.0)) :op #<PROCEEDNODE {1003239673}> :requires-grad NIL :variables (STC207688) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( linspace ` ( 3 3 ) 1 0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID207778 ((0.0 1.0 2.0) (3.0 4.0 5.0) (6.0 7.0 8.0)) :op #<ALLOCATE {1003269B63}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] ax+b"},{"location":"packages/caten.api.initializers/#function-rand","text":"(!rand shape &key (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [0, 1) . There is a rand function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !rand ` ( 3 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC215307 ((0.40309912 0.8688215 0.9638163) (0.032803092 0.918832 0.59222883) (0.53384453 0.48372424 0.8027758)) :op #<PROCEEDNODE {10072FC363}> :requires-grad NIL :variables (STC207840) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( rand ` ( 3 3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID215308 ((0.8544815 0.89949685 0.06944181) (0.78085196 7.4467063e-4 0.94722736) (0.047116876 0.9624656 0.6471626)) :op #<ALLOCATE {10073A1E13}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !rand"},{"location":"packages/caten.api.initializers/#function-normal","text":"(!normal shape &key (mean 0.0) (std 1.0) (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a normal distribution with the given mean and std . There is a normal function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !normal ` ( 3 3 ) :mean 0.0 :std 2.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC231243 ((1.9926395 -3.0460858 2.0359108) (-1.3665679 2.2740877 -0.34971234) (0.7738801 -0.7173807 1.5476315)) :op #<PROCEEDNODE {100394FD63}> :requires-grad NIL :variables (STC215320) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( normal ` ( 3 3 ) :mean 0.0 :std 1.0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID231244 ((-0.6933528 -0.618868 -0.09813504) (-0.3018336 0.73544437 -1.0356082) (-1.6392002 0.5641113 -0.094371416)) :op #<ALLOCATE {10038585C3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !normal"},{"location":"packages/caten.api.initializers/#function-randn","text":"(!randn shape &key (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a normal distribution with a mean of 0 and a standard deviation of 1. There is a randn function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !randn ` ( 3 3 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC263491 ((0.4054314 -2.6517396 -0.93262064) (-0.44230795 -0.18937276 -0.42351857) (0.09603451 -0.004643383 -0.9779817)) :op #<PROCEEDNODE {1001D978C3}> :requires-grad NIL :variables (STC248794) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( randn ` ( 3 3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID263492 ((-0.23316607 0.39065427 0.5918632) (-0.771492 1.3980544 -0.2156676) (-0.96204996 -0.6995641 1.4401776)) :op #<ALLOCATE {1001DEE6E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !randn"},{"location":"packages/caten.api.initializers/#function-uniform","text":"(!uniform shape &key (low 0.0) (high 1.0) (dtype *default-float*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [low, high) There is a uniform function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( proceed ( !uniform ` ( 3 3 ) :low 1.0 :high 2.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC272262 ((1.5054502 1.7935554 1.0513923) (1.4500921 1.7713478 1.5610737) (1.7057834 1.4408889 1.9995518)) :op #<PROCEEDNODE {10054A87E3}> :requires-grad NIL :variables (STC263504) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( uniform ` ( 3 3 ) :low 1.0 :high 2.0 ) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id TID272263 ((1.5276278 1.196649 1.2917286) (1.0492711 1.2254326 1.0443223) (1.2354891 1.7603551 1.2700266)) :op #<ALLOCATE {1001A52793}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !uniform"},{"location":"packages/caten.api.initializers/#function-randint","text":"(!randint shape &key (low 0) (high 1) (dtype *default-int*) (order *default-order*) (out nil)) Creates a tensor whose elements are randomly sampled from a uniform distribution over the interval [low, high) . There is a randint function for the same purpose, but it is not lazy. Lazy CATEN-USER> ( ctx:with-contextvar ( :backend \"clang\" ) ( proceed ( !randint ` ( 3 3 ) :low 1 :high 10 ))) Result Result {Tensor{CLANGBUFFER}[int64] :shape (3 3) :id STC311011 ((5 5 6) (7 8 9) (1 2 6)) :op #<PROCEEDNODE {1002B03DA3}> :requires-grad NIL :variables (STC279893) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Static CATEN-USER> ( ctx:with-contextvar ( :backend \"clang\" ) ( randint ` ( 3 3 ) :low 1 :high 10 :dtype :int32 )) Result Result {Tensor{CLANGBUFFER}[int32] :shape (3 3) :id TID311012 ((3 3 4) (7 4 3) (5 1 9)) :op #<ALLOCATE {100916C1F3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !randint"},{"location":"packages/caten.api.initializers/#function-full","text":"(!full shape fill-value &key (dtype *default-float*) (order *default-order*)) Initializes a tensor filled with fill-value . lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC339745 ((#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY)) :op #<PROCEEDNODE {10091948C3}> :requires-grad NIL :variables (TID339684) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !full"},{"location":"packages/caten.api.initializers/#function-xavier-uniform","text":"No description provided","title":"[function] xavier-uniform"},{"location":"packages/caten.api.initializers/#function-xavier-gaussian","text":"No description provided","title":"[function] xavier-gaussian"},{"location":"packages/caten.api/","text":"Overview Welcome to Caten/APIs WIP proceed is a function that evaluates a computational graph. lisp CATEN-USER> ( proceed ( make-tensor ` ( 3 3 ) :initial-element 1.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC28090 ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :op #<PROCEEDNODE {100328CD63}> :requires-grad NIL :variables (TID28027) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} ctx:with-contextvar is a macro that sets the context variable. BACKEND=CLANG to use JIT, JIT_DEBUG=4 to see the generated code. DOT=1 to debug the pattern matcher on your browser. (need graphviz) lisp CATEN-USER> ( ctx:with-contextvar ( :BACKEND \"CLANG\" :jit_debug 4 :dot 0 ) ( caten ( forward ( Embedding 10 10 ) ( make-tensor ` ( b c ))))) Result Result #<CLANGRUNTIME {(c b) -> ((val_35), nil)} val_2 = allocate(shape=(10, 10), stride=(10, 1), from=<Realized Buffer>); val_9 = allocate(shape=(), stride=()); val_10 = load(val_9); // :value = C val_15 = allocate(shape=(), stride=()); val_16 = load(val_15); // :value = C val_17 = allocate(shape=(), stride=()); val_18 = load(val_17); // :value = B val_22 = allocate(shape=(val_18, val_16), stride=(val_10, 1)); val_12 = allocate(shape=(), stride=()); val_13 = load(val_12); // :value = 10 val_14 = mul(val_13, val_10); val_34 = allocate(shape=(val_18, val_16, 1, 10), stride=(val_14, 10, 10, 1)); val_34 = fused_sumnode_embedding73130(val_34, c, b, val_2, val_22); // JIT: <CLANG[FUSED_SUMNODE_EMBEDDING73130] : #<EXPR (3*(100*(b*c)))>FLOP> val_35 = pause/backward(val_34); >","title":"Overview"},{"location":"packages/caten.api/#overview","text":"Welcome to Caten/APIs WIP proceed is a function that evaluates a computational graph. lisp CATEN-USER> ( proceed ( make-tensor ` ( 3 3 ) :initial-element 1.0 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC28090 ((1.0 1.0 1.0) (1.0 1.0 1.0) (1.0 1.0 1.0)) :op #<PROCEEDNODE {100328CD63}> :requires-grad NIL :variables (TID28027) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} ctx:with-contextvar is a macro that sets the context variable. BACKEND=CLANG to use JIT, JIT_DEBUG=4 to see the generated code. DOT=1 to debug the pattern matcher on your browser. (need graphviz) lisp CATEN-USER> ( ctx:with-contextvar ( :BACKEND \"CLANG\" :jit_debug 4 :dot 0 ) ( caten ( forward ( Embedding 10 10 ) ( make-tensor ` ( b c ))))) Result Result #<CLANGRUNTIME {(c b) -> ((val_35), nil)} val_2 = allocate(shape=(10, 10), stride=(10, 1), from=<Realized Buffer>); val_9 = allocate(shape=(), stride=()); val_10 = load(val_9); // :value = C val_15 = allocate(shape=(), stride=()); val_16 = load(val_15); // :value = C val_17 = allocate(shape=(), stride=()); val_18 = load(val_17); // :value = B val_22 = allocate(shape=(val_18, val_16), stride=(val_10, 1)); val_12 = allocate(shape=(), stride=()); val_13 = load(val_12); // :value = 10 val_14 = mul(val_13, val_10); val_34 = allocate(shape=(val_18, val_16, 1, 10), stride=(val_14, 10, 10, 1)); val_34 = fused_sumnode_embedding73130(val_34, c, b, val_2, val_22); // JIT: <CLANG[FUSED_SUMNODE_EMBEDDING73130] : #<EXPR (3*(100*(b*c)))>FLOP> val_35 = pause/backward(val_34); >","title":"Overview"},{"location":"packages/caten.api.models/","text":"Models TODO [macro] defcall (defcall (model-bind model) (&rest inputs) body0 A macro to write defmethod call in a more concise way. Example ( defcall ( model Transformer ) ( Tokens[Batch Seq-Len] Start-Pos[] ) ( with-slots (( wte wte ) ( wpe wpe ) ( h h ) ( ln-f ln-f ) ( lm-head lm-head )) model ( let* (( token-emb ( forward wte tokens )) ( pos-emb ( forward wpe ( !cast ( !add start-pos ( !index-components ` ( 1 , seq-len ))) ( dtype-of tokens )))) ( hi ( !add token-emb pos-emb )) ( mask ( !triu ( !full ` ( 1 1 , seq-len , ( !+ start-pos ( iconst seq-len ))) ( -inf )) :diagonal ( !+ ( iconst 1 ) start-pos ))) ( _ ( dolist ( hn h ) ( setf hi ( forward hn hi mask start-pos )))) ( logits ( forward lm-head ( forward ln-f hi )))) ( declare ( ignore _ )) ;; (!argmax (!view logits t -1 t)) ( !argmax logits )))) [macro] defsequence ( defsequence ( name ( &rest args ) &optional docstring &rest nodes )) Defines a model which the definition is given as a sequence of nodes. Note that models defined by this macro only accept one input. Example ( defsequence MLP ( in-features hidden-dim out-features &key ( activation #' !relu )) ( Linear in-features hidden-dim ) ( asnode activation ) ( Linear hidden-dim hidden-dim ) ( asnode activation ) ( Linear hidden-dim out-features )) [function] asnode (asnode function &rest rest-args) Wraps the function as a callable node by (forward ...) . function is a function which takes one argument and returns one value. rest-args is a place to pass additional arguments like: (asnode #'!leaky-relu :neg-slope 1e-2)","title":"Model"},{"location":"packages/caten.api.models/#models","text":"TODO","title":"Models"},{"location":"packages/caten.api.models/#macro-defcall","text":"(defcall (model-bind model) (&rest inputs) body0 A macro to write defmethod call in a more concise way.","title":"[macro] defcall"},{"location":"packages/caten.api.models/#example","text":"( defcall ( model Transformer ) ( Tokens[Batch Seq-Len] Start-Pos[] ) ( with-slots (( wte wte ) ( wpe wpe ) ( h h ) ( ln-f ln-f ) ( lm-head lm-head )) model ( let* (( token-emb ( forward wte tokens )) ( pos-emb ( forward wpe ( !cast ( !add start-pos ( !index-components ` ( 1 , seq-len ))) ( dtype-of tokens )))) ( hi ( !add token-emb pos-emb )) ( mask ( !triu ( !full ` ( 1 1 , seq-len , ( !+ start-pos ( iconst seq-len ))) ( -inf )) :diagonal ( !+ ( iconst 1 ) start-pos ))) ( _ ( dolist ( hn h ) ( setf hi ( forward hn hi mask start-pos )))) ( logits ( forward lm-head ( forward ln-f hi )))) ( declare ( ignore _ )) ;; (!argmax (!view logits t -1 t)) ( !argmax logits ))))","title":"Example"},{"location":"packages/caten.api.models/#macro-defsequence","text":"( defsequence ( name ( &rest args ) &optional docstring &rest nodes )) Defines a model which the definition is given as a sequence of nodes. Note that models defined by this macro only accept one input.","title":"[macro] defsequence"},{"location":"packages/caten.api.models/#example_1","text":"( defsequence MLP ( in-features hidden-dim out-features &key ( activation #' !relu )) ( Linear in-features hidden-dim ) ( asnode activation ) ( Linear hidden-dim hidden-dim ) ( asnode activation ) ( Linear hidden-dim out-features ))","title":"Example"},{"location":"packages/caten.api.models/#function-asnode","text":"(asnode function &rest rest-args) Wraps the function as a callable node by (forward ...) . function is a function which takes one argument and returns one value. rest-args is a place to pass additional arguments like: (asnode #'!leaky-relu :neg-slope 1e-2)","title":"[function] asnode"},{"location":"packages/caten.api.module/","text":"Module [macro] defmodule (defmodule (name ((&rest constructor-args) &rest attrs) &key (where nil) (direct-superclasses nil)) (&rest slots) &key (documentation \"\") (impl nil) (forward nil) (backward nil)) Define a module named name . In Caten, Module is a CLOS class that represents a set of Funcs and is defined as a subclass of Func itself. It is used to represent computational nodes that can be expressed through compositions of Funcs. Consequently, as a subclass of Func, Module utilizes the following three methods for manipulation: [method] impl (impl (op Module) &rest tensors) tensors[list] a list of the input tensor. In the impl method, please describe the process for constructing the computational graph of the Module using a combination of Func and Module . The computational graph must begin with the inputs. If there are multiple outputs, bind them with cl:values . If you need to record Tensors for the backward process, now is the time to do so. [method] forward (forward (op Module) &rest tensors) tensors[List] a list of the input tensor. In the forward method, describe the operation to create a Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation at this stage. The st macro in ShapeTracker is quite useful for creating the Tensor after the operation. If necessary, you may also include checks for additional attributes or slots here. If you specify ShapeTracker in :where , the defmodule macro will automatically generate the forward. Therefore, you must describe either :where or :forward . [method] backward (optional) (backward (op Module) prev-grad) -> (values input_1.grad input_2.grad ...) prev-grad[Tensor] In the backward method, describe the gradient computation for the Module using a combination of Func and Module . The arguments are fixed as (op prev-grad) , where op = module instance, prev-grad is a tensor. If you need the value of the Tensor at the input stage for the gradient calculation, temporarily store it using the module-sv4bws accessor while calling the impl method. The compiler does not depend on module-sv4bws , so you are free to choose how to store the Tensor. In Caten, since save-for-backward is automatically determined, there is no need to be concerned about in-place operations. Note that backward is optional . If it is not provided, AD will be applied based on the computational graph from impl . [method] lower The lower method is automatically written by the defmodule , so there is no need to consider it when describing the module. However, it is necessary to understand how it is lowered for when defining simplifiers for the Module . lower produces the following node: (make-node :Graph (intern (symbol-name (symb 'graph/ name)) \"KEYWORD\") outputs inputs &rest attrs) Nodes whose class is :Graph are completely eliminated during lower by impl . Syntax forward , backward , impl are described in one of the following format. forward := ((op &rest args) &body body) forward := (lambda (&rest args) &body body) forward := fname Effects it defines a class named name . it defines a function named name . it works as a constructor. Arguments name[symbol] the name of module constructor-args[list] arguments for the constructor attrs[list] define attrs for the lowered graph based on the constructor-args variables using the following format: (:key1 value1 :key1 value2 ...) . slots[list] slots for the defined class. where[nil or string] ShapeTracker documentation[string] documentation Notes The methods are called in the order of forward->impl->backward during compilation impl is performed recursively, so modules must not be co-dependent within the impl method. (e.g.: do not define a module A that depends on B that depends on A ...) Modules (built_in) [function] !sum (!sum x &key (axis t) (keepdims nil)) Compute the sum of the tensor. lisp CATEN-USER> ( proceed ( !sum ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC155728 ((4950.0)) :op #<PROCEEDNODE {10031A1833}> :requires-grad NIL :variables (VID154435) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>} [function] !mean (!mean x &key (axis t) (keepdims nil)) Compute the mean of the tensor. lisp CATEN-USER> ( proceed ( !mean ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC157467 ((49.5)) :op #<PROCEEDNODE {100360BE83}> :requires-grad NIL :variables (VID155831) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>} [function] !max (!max x &key (axis t) (keepdims nil)) Compute the maximum of the tensor. lisp CATEN-USER> ( proceed ( !max ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC159537 ((99.0)) :op #<PROCEEDNODE {1003B2D8C3}> :requires-grad NIL :variables (VID157570) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>} [function] !min (!min x &key (axis t) (keepdims nil)) Compute the minimum of the tensor. lisp CATEN-USER> ( proceed ( !min ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC161625 ((0.0)) :op #<PROCEEDNODE {1004155583}> :requires-grad NIL :variables (VID159640) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>} [function] !matmul (!matmul a b) Performs matrix multiplication between the two tensors a and b . lisp CATEN-USER> ( proceed ( !matmul ( rand ` ( 32 64 )) ( rand ` ( 64 128 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (32 128) :id STC166090 ((16.115143 16.458525 16.092386 17.81228 13.661716 ~ 17.686089 18.038162 16.683573 16.274176 18.700905) (16.326677 15.621148 16.356201 17.916739 14.871678 ~ 18.09569 17.361448 15.73287 16.648724 18.704985) (16.722721 14.823622 14.077707 16.202015 13.752501 ~ 17.039087 16.04018 16.320084 16.388088 16.415977) (16.117361 16.602467 14.900114 16.754038 12.972164 ~ 15.914988 16.225424 14.572695 16.187672 17.73983) (15.025454 14.202459 13.762134 14.921074 12.034962 ~ 14.2497 14.7951 15.23291 14.765818 17.027578) ... (14.671678 15.548787 14.961614 16.846735 14.662898 ~ 15.099126 15.20463 15.049035 16.246407 17.84826) (15.084174 14.32615 15.439503 17.32295 13.282019 ~ 16.467468 15.561042 15.09145 16.568888 16.251673) (15.426824 15.761291 14.47131 15.671989 13.309795 ~ 15.386046 15.088989 14.298187 14.939453 17.597435) (16.099691 15.497888 13.230323 15.339464 12.640831 ~ 16.377731 16.77367 15.217857 16.147154 15.99234) (14.86157 15.624828 13.565197 16.511955 13.524295 ~ 16.017046 14.623064 14.877545 16.962063 16.800804)) :op #<PROCEEDNODE {1002ED0633}> :requires-grad NIL :variables (STC161638) :tracker #<TRACKER :order={row(0 1)} :shape=(32 128) :contiguous-p=T>} [function] !sinh (!sinh x) lisp CATEN-USER> ( proceed ( !sinh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC166854 ((-0.6667148 -1.7986057 -0.16147432) (0.99311143 1.5690467 -0.45024085) (0.2741389 -0.84445757 3.6562643)) :op #<PROCEEDNODE {1003399B73}> :requires-grad NIL :variables (STC166097) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !cosh (!cosh x) lisp CATEN-USER> ( proceed ( !cosh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC167612 ((1.1091444 12.141601 1.8187159) (1.0450624 1.0673907 2.024306) (1.0057459 11.280086 1.0366853)) :op #<PROCEEDNODE {100372F103}> :requires-grad NIL :variables (STC166861) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !tanh (!tanh x) lisp CATEN-USER> ( proceed ( !tanh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC170370 ((0.48222518 0.83497787 0.15090048) (-0.6021365 -0.8142663 0.0836575) (0.90113175 0.5248045 0.26375103)) :op #<PROCEEDNODE {1003FA9C23}> :requires-grad NIL :variables (STC167619) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !cos (!cos x) lisp CATEN-USER> ( proceed ( !cos ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC171055 ((0.8741474 0.6821368 0.96187115) (0.9756673 -0.15136945 0.5390583) (0.7348094 0.56210893 0.9987133)) :op #<PROCEEDNODE {1005563B83}> :requires-grad NIL :variables (STC170377) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !tan (!tan x) lisp CATEN-USER> ( proceed ( !tan ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC171779 ((2.2656112 1.2551787 -52.0603) (0.11679068 0.27259648 0.0093172565) (0.26652285 -0.48955667 -2.6124444)) :op #<PROCEEDNODE {1005BD7AB3}> :requires-grad NIL :variables (STC171062) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !log2 (!log2 x) lisp CATEN-USER> ( proceed ( !log2 ( ax+b ` ( 3 3 ) 1 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC172535 ((-3.321928 0.13750356 1.0703893) (1.6322682 2.0356238 2.3504972) (2.6088092 2.827819 3.017922)) :op #<PROCEEDNODE {100719F343}> :requires-grad NIL :variables (STC171787) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !exp2 (!exp2 x) lisp CATEN-USER> ( proceed ( !exp2 ( ax+b ` ( 3 3 ) 1 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC173274 ((1.0717734 2.143547 4.2870936) (8.574187 17.148375 34.29675) (68.593475 137.18695 274.37408)) :op #<PROCEEDNODE {10073224D3}> :requires-grad NIL :variables (STC172543) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !expt (!expt base power) Computes the power of base with power. lisp CATEN-USER> ( proceed ( !expt ( ax+b ` ( 3 3 ) 1 0.1 ) 2 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC175289 ((0.010000001 1.21 4.4099994) (9.61 16.81 26.009998) (37.21 50.41 65.61001)) :op #<PROCEEDNODE {10081FA423}> :requires-grad NIL :variables (STC173577) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !truncate (!truncate x) lisp CATEN-USER> ( proceed ( !truncate ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC175481 ((0.0 0.0 0.0) (0.0 0.0 1.0) (0.0 0.0 -1.0)) :op #<PROCEEDNODE {10090E3853}> :requires-grad NIL :variables (STC175296) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !ceiling (!ceiling x) lisp CATEN-USER> ( proceed ( !ceiling ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC176399 ((1.0 -3.0 -1.0) (1.0 0.0 0.0) (0.0 0.0 0.0)) :op #<PROCEEDNODE {10094B0D33}> :requires-grad NIL :variables (STC175488) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !floor (!floor x) lisp CATEN-USER> ( proceed ( !floor ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC177319 ((-1.0 0.0 -2.0) (0.0 0.0 -2.0) (-1.0 -1.0 -1.0)) :op #<PROCEEDNODE {1009A380A3}> :requires-grad NIL :variables (STC176406) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !triu (!triu x &key (diagonal 0)) Returns the upper triangular part of the tensor (>= 2D) or batch of matrices input. lisp CATEN-USER> ( proceed ( !triu ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC178677 ((0.016040208 0.9379116 0.9069626) (0.0 0.90060127 0.21504083) (0.0 0.0 0.8353897)) :op #<PROCEEDNODE {10018581F3}> :requires-grad NIL :variables (STC177326) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !tril (!tril x &key (diagonal 0)) Returns the lower triangular part of the tensor (>= 2D) or batch of matrices input. lisp CATEN-USER> ( proceed ( !tril ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC180043 ((0.81040674 0.0 0.0) (0.5340498 0.44669133 0.0) (0.47975928 0.7199338 0.24915472)) :op #<PROCEEDNODE {1001CD1FF3}> :requires-grad NIL :variables (STC178684) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !argmax (!argmax x &key (axis -1) (keepdims nil)) Returns the indices of the maximum values along an axis. lisp CATEN-USER> ( proceed ( !argmax ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 1) :id STC186798 ((1) (0) (1)) :op #<PROCEEDNODE {1003760EE3}> :requires-grad NIL :variables (VID180145) :tracker #<TRACKER :order={row(0 1)} :shape=(3 ~1) :contiguous-p=T>} [function] !argmin (!argmin x &key (axis -1) (keepdims nil)) Returns the indices of the minimum values along an axis. lisp CATEN-USER> ( proceed ( !argmin ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 1) :id STC193742 ((2) (1) (1)) :op #<PROCEEDNODE {10072078D3}> :requires-grad NIL :variables (VID186900) :tracker #<TRACKER :order={row(0 1)} :shape=(3 ~1) :contiguous-p=T>} [function] !clip (!clip x min max) !clip limits the given input within an interval. The interval is specified by the inputs 'min' and 'max'. min/max is either a number, a symbol, or a tensor. The implementation follows the ONNX specification. https://github.com/onnx/onnx/blob/main/docs/Changelog.md#clip-13 lisp CATEN-USER> ( proceed ( !clip ( randn ` ( 3 3 )) -0.3 0.3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC196335 ((-0.3 0.3 0.3) (0.3 0.2180472 0.3) (0.3 -0.3 0.3)) :op #<PROCEEDNODE {10090BBF73}> :requires-grad NIL :variables (STC193939) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] !erf (!erf x) Computes the error function of x. lisp CATEN-USER> ( proceed ( !erf ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC207681 ((-0.5891361 -0.9240581 -0.08728349) (0.9355107 0.5068274 -0.44668216) (-0.76412225 -0.96422094 0.37599617)) :op #<PROCEEDNODE {10031D7013}> :requires-grad NIL :variables (STC196342) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"Module"},{"location":"packages/caten.api.module/#module","text":"","title":"Module"},{"location":"packages/caten.api.module/#macro-defmodule","text":"(defmodule (name ((&rest constructor-args) &rest attrs) &key (where nil) (direct-superclasses nil)) (&rest slots) &key (documentation \"\") (impl nil) (forward nil) (backward nil)) Define a module named name . In Caten, Module is a CLOS class that represents a set of Funcs and is defined as a subclass of Func itself. It is used to represent computational nodes that can be expressed through compositions of Funcs. Consequently, as a subclass of Func, Module utilizes the following three methods for manipulation:","title":"[macro] defmodule"},{"location":"packages/caten.api.module/#method-impl","text":"(impl (op Module) &rest tensors) tensors[list] a list of the input tensor. In the impl method, please describe the process for constructing the computational graph of the Module using a combination of Func and Module . The computational graph must begin with the inputs. If there are multiple outputs, bind them with cl:values . If you need to record Tensors for the backward process, now is the time to do so.","title":"[method] impl"},{"location":"packages/caten.api.module/#method-forward","text":"(forward (op Module) &rest tensors) tensors[List] a list of the input tensor. In the forward method, describe the operation to create a Tensor after computation. Be aware of its lazy evaluation nature; do not perform the actual computation at this stage. The st macro in ShapeTracker is quite useful for creating the Tensor after the operation. If necessary, you may also include checks for additional attributes or slots here. If you specify ShapeTracker in :where , the defmodule macro will automatically generate the forward. Therefore, you must describe either :where or :forward .","title":"[method] forward"},{"location":"packages/caten.api.module/#method-backward-optional","text":"(backward (op Module) prev-grad) -> (values input_1.grad input_2.grad ...) prev-grad[Tensor] In the backward method, describe the gradient computation for the Module using a combination of Func and Module . The arguments are fixed as (op prev-grad) , where op = module instance, prev-grad is a tensor. If you need the value of the Tensor at the input stage for the gradient calculation, temporarily store it using the module-sv4bws accessor while calling the impl method. The compiler does not depend on module-sv4bws , so you are free to choose how to store the Tensor. In Caten, since save-for-backward is automatically determined, there is no need to be concerned about in-place operations. Note that backward is optional . If it is not provided, AD will be applied based on the computational graph from impl .","title":"[method] backward (optional)"},{"location":"packages/caten.api.module/#method-lower","text":"The lower method is automatically written by the defmodule , so there is no need to consider it when describing the module. However, it is necessary to understand how it is lowered for when defining simplifiers for the Module . lower produces the following node: (make-node :Graph (intern (symbol-name (symb 'graph/ name)) \"KEYWORD\") outputs inputs &rest attrs) Nodes whose class is :Graph are completely eliminated during lower by impl .","title":"[method] lower"},{"location":"packages/caten.api.module/#syntax","text":"forward , backward , impl are described in one of the following format. forward := ((op &rest args) &body body) forward := (lambda (&rest args) &body body) forward := fname","title":"Syntax"},{"location":"packages/caten.api.module/#effects","text":"it defines a class named name . it defines a function named name . it works as a constructor.","title":"Effects"},{"location":"packages/caten.api.module/#arguments","text":"name[symbol] the name of module constructor-args[list] arguments for the constructor attrs[list] define attrs for the lowered graph based on the constructor-args variables using the following format: (:key1 value1 :key1 value2 ...) . slots[list] slots for the defined class. where[nil or string] ShapeTracker documentation[string] documentation","title":"Arguments"},{"location":"packages/caten.api.module/#notes","text":"The methods are called in the order of forward->impl->backward during compilation impl is performed recursively, so modules must not be co-dependent within the impl method. (e.g.: do not define a module A that depends on B that depends on A ...)","title":"Notes"},{"location":"packages/caten.api.module/#modules-built_in","text":"","title":"Modules (built_in)"},{"location":"packages/caten.api.module/#function-sum","text":"(!sum x &key (axis t) (keepdims nil)) Compute the sum of the tensor. lisp CATEN-USER> ( proceed ( !sum ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC155728 ((4950.0)) :op #<PROCEEDNODE {10031A1833}> :requires-grad NIL :variables (VID154435) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>}","title":"[function] !sum"},{"location":"packages/caten.api.module/#function-mean","text":"(!mean x &key (axis t) (keepdims nil)) Compute the mean of the tensor. lisp CATEN-USER> ( proceed ( !mean ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC157467 ((49.5)) :op #<PROCEEDNODE {100360BE83}> :requires-grad NIL :variables (VID155831) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>}","title":"[function] !mean"},{"location":"packages/caten.api.module/#function-max","text":"(!max x &key (axis t) (keepdims nil)) Compute the maximum of the tensor. lisp CATEN-USER> ( proceed ( !max ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC159537 ((99.0)) :op #<PROCEEDNODE {1003B2D8C3}> :requires-grad NIL :variables (VID157570) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>}","title":"[function] !max"},{"location":"packages/caten.api.module/#function-min","text":"(!min x &key (axis t) (keepdims nil)) Compute the minimum of the tensor. lisp CATEN-USER> ( proceed ( !min ( ax+b ` ( 10 10 ) 1 0 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (1 1) :id STC161625 ((0.0)) :op #<PROCEEDNODE {1004155583}> :requires-grad NIL :variables (VID159640) :tracker #<TRACKER :order={row(0 1)} :shape=(~1 ~1) :contiguous-p=T>}","title":"[function] !min"},{"location":"packages/caten.api.module/#function-matmul","text":"(!matmul a b) Performs matrix multiplication between the two tensors a and b . lisp CATEN-USER> ( proceed ( !matmul ( rand ` ( 32 64 )) ( rand ` ( 64 128 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (32 128) :id STC166090 ((16.115143 16.458525 16.092386 17.81228 13.661716 ~ 17.686089 18.038162 16.683573 16.274176 18.700905) (16.326677 15.621148 16.356201 17.916739 14.871678 ~ 18.09569 17.361448 15.73287 16.648724 18.704985) (16.722721 14.823622 14.077707 16.202015 13.752501 ~ 17.039087 16.04018 16.320084 16.388088 16.415977) (16.117361 16.602467 14.900114 16.754038 12.972164 ~ 15.914988 16.225424 14.572695 16.187672 17.73983) (15.025454 14.202459 13.762134 14.921074 12.034962 ~ 14.2497 14.7951 15.23291 14.765818 17.027578) ... (14.671678 15.548787 14.961614 16.846735 14.662898 ~ 15.099126 15.20463 15.049035 16.246407 17.84826) (15.084174 14.32615 15.439503 17.32295 13.282019 ~ 16.467468 15.561042 15.09145 16.568888 16.251673) (15.426824 15.761291 14.47131 15.671989 13.309795 ~ 15.386046 15.088989 14.298187 14.939453 17.597435) (16.099691 15.497888 13.230323 15.339464 12.640831 ~ 16.377731 16.77367 15.217857 16.147154 15.99234) (14.86157 15.624828 13.565197 16.511955 13.524295 ~ 16.017046 14.623064 14.877545 16.962063 16.800804)) :op #<PROCEEDNODE {1002ED0633}> :requires-grad NIL :variables (STC161638) :tracker #<TRACKER :order={row(0 1)} :shape=(32 128) :contiguous-p=T>}","title":"[function] !matmul"},{"location":"packages/caten.api.module/#function-sinh","text":"(!sinh x) lisp CATEN-USER> ( proceed ( !sinh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC166854 ((-0.6667148 -1.7986057 -0.16147432) (0.99311143 1.5690467 -0.45024085) (0.2741389 -0.84445757 3.6562643)) :op #<PROCEEDNODE {1003399B73}> :requires-grad NIL :variables (STC166097) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !sinh"},{"location":"packages/caten.api.module/#function-cosh","text":"(!cosh x) lisp CATEN-USER> ( proceed ( !cosh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC167612 ((1.1091444 12.141601 1.8187159) (1.0450624 1.0673907 2.024306) (1.0057459 11.280086 1.0366853)) :op #<PROCEEDNODE {100372F103}> :requires-grad NIL :variables (STC166861) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !cosh"},{"location":"packages/caten.api.module/#function-tanh","text":"(!tanh x) lisp CATEN-USER> ( proceed ( !tanh ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC170370 ((0.48222518 0.83497787 0.15090048) (-0.6021365 -0.8142663 0.0836575) (0.90113175 0.5248045 0.26375103)) :op #<PROCEEDNODE {1003FA9C23}> :requires-grad NIL :variables (STC167619) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !tanh"},{"location":"packages/caten.api.module/#function-cos","text":"(!cos x) lisp CATEN-USER> ( proceed ( !cos ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC171055 ((0.8741474 0.6821368 0.96187115) (0.9756673 -0.15136945 0.5390583) (0.7348094 0.56210893 0.9987133)) :op #<PROCEEDNODE {1005563B83}> :requires-grad NIL :variables (STC170377) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !cos"},{"location":"packages/caten.api.module/#function-tan","text":"(!tan x) lisp CATEN-USER> ( proceed ( !tan ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC171779 ((2.2656112 1.2551787 -52.0603) (0.11679068 0.27259648 0.0093172565) (0.26652285 -0.48955667 -2.6124444)) :op #<PROCEEDNODE {1005BD7AB3}> :requires-grad NIL :variables (STC171062) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !tan"},{"location":"packages/caten.api.module/#function-log2","text":"(!log2 x) lisp CATEN-USER> ( proceed ( !log2 ( ax+b ` ( 3 3 ) 1 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC172535 ((-3.321928 0.13750356 1.0703893) (1.6322682 2.0356238 2.3504972) (2.6088092 2.827819 3.017922)) :op #<PROCEEDNODE {100719F343}> :requires-grad NIL :variables (STC171787) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !log2"},{"location":"packages/caten.api.module/#function-exp2","text":"(!exp2 x) lisp CATEN-USER> ( proceed ( !exp2 ( ax+b ` ( 3 3 ) 1 0.1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC173274 ((1.0717734 2.143547 4.2870936) (8.574187 17.148375 34.29675) (68.593475 137.18695 274.37408)) :op #<PROCEEDNODE {10073224D3}> :requires-grad NIL :variables (STC172543) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !exp2"},{"location":"packages/caten.api.module/#function-expt","text":"(!expt base power) Computes the power of base with power. lisp CATEN-USER> ( proceed ( !expt ( ax+b ` ( 3 3 ) 1 0.1 ) 2 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC175289 ((0.010000001 1.21 4.4099994) (9.61 16.81 26.009998) (37.21 50.41 65.61001)) :op #<PROCEEDNODE {10081FA423}> :requires-grad NIL :variables (STC173577) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !expt"},{"location":"packages/caten.api.module/#function-truncate","text":"(!truncate x) lisp CATEN-USER> ( proceed ( !truncate ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC175481 ((0.0 0.0 0.0) (0.0 0.0 1.0) (0.0 0.0 -1.0)) :op #<PROCEEDNODE {10090E3853}> :requires-grad NIL :variables (STC175296) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !truncate"},{"location":"packages/caten.api.module/#function-ceiling","text":"(!ceiling x) lisp CATEN-USER> ( proceed ( !ceiling ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC176399 ((1.0 -3.0 -1.0) (1.0 0.0 0.0) (0.0 0.0 0.0)) :op #<PROCEEDNODE {10094B0D33}> :requires-grad NIL :variables (STC175488) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !ceiling"},{"location":"packages/caten.api.module/#function-floor","text":"(!floor x) lisp CATEN-USER> ( proceed ( !floor ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC177319 ((-1.0 0.0 -2.0) (0.0 0.0 -2.0) (-1.0 -1.0 -1.0)) :op #<PROCEEDNODE {1009A380A3}> :requires-grad NIL :variables (STC176406) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !floor"},{"location":"packages/caten.api.module/#function-triu","text":"(!triu x &key (diagonal 0)) Returns the upper triangular part of the tensor (>= 2D) or batch of matrices input. lisp CATEN-USER> ( proceed ( !triu ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC178677 ((0.016040208 0.9379116 0.9069626) (0.0 0.90060127 0.21504083) (0.0 0.0 0.8353897)) :op #<PROCEEDNODE {10018581F3}> :requires-grad NIL :variables (STC177326) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !triu"},{"location":"packages/caten.api.module/#function-tril","text":"(!tril x &key (diagonal 0)) Returns the lower triangular part of the tensor (>= 2D) or batch of matrices input. lisp CATEN-USER> ( proceed ( !tril ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC180043 ((0.81040674 0.0 0.0) (0.5340498 0.44669133 0.0) (0.47975928 0.7199338 0.24915472)) :op #<PROCEEDNODE {1001CD1FF3}> :requires-grad NIL :variables (STC178684) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !tril"},{"location":"packages/caten.api.module/#function-argmax","text":"(!argmax x &key (axis -1) (keepdims nil)) Returns the indices of the maximum values along an axis. lisp CATEN-USER> ( proceed ( !argmax ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 1) :id STC186798 ((1) (0) (1)) :op #<PROCEEDNODE {1003760EE3}> :requires-grad NIL :variables (VID180145) :tracker #<TRACKER :order={row(0 1)} :shape=(3 ~1) :contiguous-p=T>}","title":"[function] !argmax"},{"location":"packages/caten.api.module/#function-argmin","text":"(!argmin x &key (axis -1) (keepdims nil)) Returns the indices of the minimum values along an axis. lisp CATEN-USER> ( proceed ( !argmin ( rand ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[int64] :shape (3 1) :id STC193742 ((2) (1) (1)) :op #<PROCEEDNODE {10072078D3}> :requires-grad NIL :variables (VID186900) :tracker #<TRACKER :order={row(0 1)} :shape=(3 ~1) :contiguous-p=T>}","title":"[function] !argmin"},{"location":"packages/caten.api.module/#function-clip","text":"(!clip x min max) !clip limits the given input within an interval. The interval is specified by the inputs 'min' and 'max'. min/max is either a number, a symbol, or a tensor. The implementation follows the ONNX specification. https://github.com/onnx/onnx/blob/main/docs/Changelog.md#clip-13 lisp CATEN-USER> ( proceed ( !clip ( randn ` ( 3 3 )) -0.3 0.3 )) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC196335 ((-0.3 0.3 0.3) (0.3 0.2180472 0.3) (0.3 -0.3 0.3)) :op #<PROCEEDNODE {10090BBF73}> :requires-grad NIL :variables (STC193939) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !clip"},{"location":"packages/caten.api.module/#function-erf","text":"(!erf x) Computes the error function of x. lisp CATEN-USER> ( proceed ( !erf ( randn ` ( 3 3 )))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC207681 ((-0.5891361 -0.9240581 -0.08728349) (0.9355107 0.5068274 -0.44668216) (-0.76412225 -0.96422094 0.37599617)) :op #<PROCEEDNODE {10031D7013}> :requires-grad NIL :variables (STC196342) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] !erf"},{"location":"packages/caten.api.shapetracker/","text":"ShapeTracker TODO","title":"ShapeTracker"},{"location":"packages/caten.api.shapetracker/#shapetracker","text":"TODO","title":"ShapeTracker"},{"location":"packages/caten.api.state-dict/","text":"State-Dict [struct] State-Dict A structure representing the state dictionary of a neural network model. Entry[Hash-Table]: A hash table containing the parameters of the model, where: Key[string]: A string representing the parameter name, following a naming convention that reflects the model's architecture. Value[Tensor]: A tensor containing the parameter values associated with the key. This hash table stores all the parameters of the model, enabling easy saving, loading, and manipulation of model weights. [generic] ->state-dict (->state-dict module parents) Generates a list of cons cells containing the keys and values for creating a State-Dict from any given Module/Class . Return: A list of cons cells in the form (coms (list module_names ...) . tensor) , representing parameters and their corresponding tensors. module_names are the list of symbols representing the path to the parameter slot from the root module. TL;DR. this function traverses all slots of the Module/Class and recognizes the following objects as parameters: Tensor Module/Class List of Tensors List of Modules/Classes By default, the keys are created according to the following rules: Tensor If a slot contains a Tensor, create a cons cell with the key as (append parent (list slot_name)) and the value as the Tensor. Module If a slot contains a Module, recursively apply (->state-dict slot_value parent) to that Module with setting parent = (append parent (list slot_name)) as the new Parent. List of Tensors/Modules If a slot contains a list of Tensors or Modules, apply the above rules to each element. Keys are created by appending the slot name and the index (e.g., slot_name 0 , slot_name 1 , ...) to the Parent. To recognize other objects as parameters, please extend this method by adding methods for the desired classes. [function] get-state-dict (get-state-dict module &key (key-mapper #'pytorch-style-dict-key)) Constructs a State-Dict by recursively exploring all paramters of the given Module/Class. Module[Module/Class] The module from which to extract parameters. Key-Mapper[Function] A function that takes a list of names (the first element of the cons cell) and returns a string to be used as the key in the StateDict. Defaults to pytorch-style-dict-key . which must be #'(lambda (x) ...) Returns: State-Dict [function] load-state-dict (load-state-dict module state-dict &key (silent nil) (key-mapper #'pytorch-style-dict-key)) Loads the parameters from the given state-dict into the module, returning the given module. - silent[boolean] If set to t, suppresses warnings about unused keys, dtype mismatches, shape mismatches, and uninitialized tensors. - key-mapper[function] A function used to map the keys in the state-dict to the keys in the module. Defaults to pytorch-style-dict-key . (see: get-state-dict) Example: Transformer lisp CATEN-USER> ( progn ( ql:quickload :caten/llm ) ( get-state-dict ( caten/llm:Transformer 32 2 2 1e-5 32 ))) Result Result #<STATE-DICT { wte.weight -> (32 32) wpe.weight -> (1024 32) h.0.attn.c_attn.weight -> (96 32) h.0.attn.c_attn.bias -> (96) h.0.attn.c_proj.weight -> (32 32) h.0.attn.c_proj.bias -> (32) h.0.mlp.c_fc.weight -> (128 32) h.0.mlp.c_fc.bias -> (128) h.0.mlp.c_proj.weight -> (32 128) h.0.mlp.c_proj.bias -> (32) h.0.ln_1.affine -> (32) h.0.ln_1.bias -> (32) h.0.ln_2.affine -> (32) h.0.ln_2.bias -> (32) h.1.attn.c_attn.weight -> (96 32) h.1.attn.c_attn.bias -> (96) h.1.attn.c_proj.weight -> (32 32) h.1.attn.c_proj.bias -> (32) h.1.mlp.c_fc.weight -> (128 32) h.1.mlp.c_fc.bias -> (128) h.1.mlp.c_proj.weight -> (32 128) h.1.mlp.c_proj.bias -> (32) h.1.ln_1.affine -> (32) h.1.ln_1.bias -> (32) h.1.ln_2.affine -> (32) h.1.ln_2.bias -> (32) ln_f.affine -> (32) ln_f.bias -> (32) lm_head.weight -> (32 32) } {100B25A0B3}>","title":"StateDict"},{"location":"packages/caten.api.state-dict/#state-dict","text":"","title":"State-Dict"},{"location":"packages/caten.api.state-dict/#struct-state-dict","text":"A structure representing the state dictionary of a neural network model. Entry[Hash-Table]: A hash table containing the parameters of the model, where: Key[string]: A string representing the parameter name, following a naming convention that reflects the model's architecture. Value[Tensor]: A tensor containing the parameter values associated with the key. This hash table stores all the parameters of the model, enabling easy saving, loading, and manipulation of model weights.","title":"[struct] State-Dict"},{"location":"packages/caten.api.state-dict/#generic-state-dict","text":"(->state-dict module parents) Generates a list of cons cells containing the keys and values for creating a State-Dict from any given Module/Class . Return: A list of cons cells in the form (coms (list module_names ...) . tensor) , representing parameters and their corresponding tensors. module_names are the list of symbols representing the path to the parameter slot from the root module. TL;DR. this function traverses all slots of the Module/Class and recognizes the following objects as parameters: Tensor Module/Class List of Tensors List of Modules/Classes By default, the keys are created according to the following rules: Tensor If a slot contains a Tensor, create a cons cell with the key as (append parent (list slot_name)) and the value as the Tensor. Module If a slot contains a Module, recursively apply (->state-dict slot_value parent) to that Module with setting parent = (append parent (list slot_name)) as the new Parent. List of Tensors/Modules If a slot contains a list of Tensors or Modules, apply the above rules to each element. Keys are created by appending the slot name and the index (e.g., slot_name 0 , slot_name 1 , ...) to the Parent. To recognize other objects as parameters, please extend this method by adding methods for the desired classes.","title":"[generic] -&gt;state-dict"},{"location":"packages/caten.api.state-dict/#function-get-state-dict","text":"(get-state-dict module &key (key-mapper #'pytorch-style-dict-key)) Constructs a State-Dict by recursively exploring all paramters of the given Module/Class. Module[Module/Class] The module from which to extract parameters. Key-Mapper[Function] A function that takes a list of names (the first element of the cons cell) and returns a string to be used as the key in the StateDict. Defaults to pytorch-style-dict-key . which must be #'(lambda (x) ...) Returns: State-Dict","title":"[function] get-state-dict"},{"location":"packages/caten.api.state-dict/#function-load-state-dict","text":"(load-state-dict module state-dict &key (silent nil) (key-mapper #'pytorch-style-dict-key)) Loads the parameters from the given state-dict into the module, returning the given module. - silent[boolean] If set to t, suppresses warnings about unused keys, dtype mismatches, shape mismatches, and uninitialized tensors. - key-mapper[function] A function used to map the keys in the state-dict to the keys in the module. Defaults to pytorch-style-dict-key . (see: get-state-dict)","title":"[function] load-state-dict"},{"location":"packages/caten.api.state-dict/#example-transformer","text":"lisp CATEN-USER> ( progn ( ql:quickload :caten/llm ) ( get-state-dict ( caten/llm:Transformer 32 2 2 1e-5 32 ))) Result Result #<STATE-DICT { wte.weight -> (32 32) wpe.weight -> (1024 32) h.0.attn.c_attn.weight -> (96 32) h.0.attn.c_attn.bias -> (96) h.0.attn.c_proj.weight -> (32 32) h.0.attn.c_proj.bias -> (32) h.0.mlp.c_fc.weight -> (128 32) h.0.mlp.c_fc.bias -> (128) h.0.mlp.c_proj.weight -> (32 128) h.0.mlp.c_proj.bias -> (32) h.0.ln_1.affine -> (32) h.0.ln_1.bias -> (32) h.0.ln_2.affine -> (32) h.0.ln_2.bias -> (32) h.1.attn.c_attn.weight -> (96 32) h.1.attn.c_attn.bias -> (96) h.1.attn.c_proj.weight -> (32 32) h.1.attn.c_proj.bias -> (32) h.1.mlp.c_fc.weight -> (128 32) h.1.mlp.c_fc.bias -> (128) h.1.mlp.c_proj.weight -> (32 128) h.1.mlp.c_proj.bias -> (32) h.1.ln_1.affine -> (32) h.1.ln_1.bias -> (32) h.1.ln_2.affine -> (32) h.1.ln_2.bias -> (32) ln_f.affine -> (32) ln_f.bias -> (32) lm_head.weight -> (32 32) } {100B25A0B3}>","title":"Example: Transformer"},{"location":"packages/caten.api.tensor/","text":"Tensor [struct] Tensor A struct tensor is a multi-dimensional, and strided matrix (and nrank=0 to scalar value) containing elements of the single dtype. Also, the tensor has the following slots: shape[list] The shape as a list of elements of: number, symbol, or Tensor . buffer[AbstractBuffer] The buffer of the tensor. Realized arrays are stored here. dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either :row or :column. id[symbol] The unique identifier of the tensor. (usually created via gensym) op[Func] The Func object that represents the operation of the tensor. views[list] A list of ViewRange objects, determining the bound of loops. requires-grad[boolean] A flag to determine whether the tensor requires a gradient. grad[Tensor] A gradient and realized tensor of the tensor. grad-id[symbol] A unique identifier of the gradient tensor. variables[list] A list of Tensor objects that are used in the operation of the tensor. Tensors listed here are involved in the compilation. [function] make-tensor (make-tensor shape &key (dtype *default-float*) (order *default-order*) (id (gensym \"TID\")) (requires-grad nil) (initial-element nil) (views nil) (from nil)) Create a new lazy tensor. shape[list] The shape as a list of elements of: number, symbol, or Tensor . dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either :row or :column. id[symbol] The unique identifier of the tensor. (usually created by gensym) requires-grad[boolean] A flag to determine whether the tensor requires a gradient. initial-element[null|number|symbol|ScalarTensor] An initial value of the tensor. views[list] A list of ViewRange objects, determining the bound of loops. from[null|AbstractBuffer|Symbol] A buffer used to initialize the tensor. If a symbol is given, then the buffer is taken from the variable table. [function] make-scalar (make-scalar value &key (dtype *default-float*) (order *default-order*) (id (gensym \"SID\")) (requires-grad nil)) Create a new scalar tensor. A ScalarTensor in Caten is a tensor with a rank of 0. value[number|symbol] The value of the scalar tensor. dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either of :row or :column. id[symbol] A unique identifier of the tensor (usually created by gensym). requires-grad[boolean] A flag to determine whether the tensor requires a gradient. Hint: You can use fconst , uconst , and iconst to create a scalar tensor with a default dtype (arguments are the same as make-scalar .) [function] grad (grad tensor) Returns a gradient of the tensor [function] shape (shape tensor) Returns a copy of the shape of the tensor [function] ndim (ndim tensor) Returns a rank of the tensor [function] dtype-of (dtype-of tensor) Returns a dtype of the tensor [function] order (order tensor) Returns a memory-layout of the tensor. [function] caten ( caten tensors &key ( backend ( ctx:getenv :BACKEND )) ( rewriters nil ) ( simplifiers *external-simplifiers* )) An entry point for compiling the given tensors, returning GraphRuntime. tensor[Tensor|List] toplevel tensors. backend[keyword] a keyword of the backend to use. (assumed to be defined by define-backend) rewriters[list] a list of graph rewriters called each time the compiler will lower the module. simplifiers[list] a list of external simplifiers used in the graph-level compilation (defined by defsimplifier). Pass the function name. [function] tensor-graph (tensor-graph tensor) Lowers the given tensors into an aasm graph, only constant folding is applied. For some convenience, this function returns the first tensor if the input is not a tensor. [function] tensor-lowered-graph (tensor-lowered-graph &rest tensors) Creates a lowered graph from the given tensors (i.e., an input graph to JIT=1). ### [method] forward (forward runtime &rest params) Compute the forward pass of the compiled computational graph (GraphRuntime). The params are additional parameters that are used to initialize the GraphRuntime variable table, passed in the following format: - `(symbol . number)` Loaded as a `*default-int*` scalar tensor, used to determine the shape of dynamic shaped tensors. - `(symbol . buffer)` or `(symbol . tensor)` Used to assign the initial elements of the tensor specified by the name in `form` in `(make-tensor ... :from x)` or `(forward runtime place1 value1 place2 value2 ...)` is added recently. Here's an example. #### Examples ```lisp title=\"Example of forward with dynamic shaped\" (let ((model (caten (!randn `(:a :b))))) (forward model :a 10 :b 10)) Result Result {Tensor{LISPBUFFER}[float32] :shape (A B) :id STC102882 ((0.4678763 -0.4861329 -0.94086486 1.04754 2.2546127 0.50788605 -0.10552773 0.049141496 -1.5750943 -1.7679455) (1.3089938 -0.23512708 1.1692804 -0.28485575 0.3088914 -0.6845822 2.1176116 0.66664845 -0.45067576 -0.27492777) (-0.12159313 -1.3346667 -0.5996805 -0.47102702 0.5524228 0.35672027 -0.60367817 -0.21228786 -0.14100133 -1.1875058) (1.1116551 1.8247204 -1.0528817 -0.3054723 0.29911244 0.13449419 0.8812368 -0.08793502 -1.4413477 -0.2823714) (-0.8456068 0.20695838 -0.8928872 0.34807467 0.64626217 -1.1382821 1.5031484 -1.5459219 -0.7998622 -1.2453525) (0.33562827 -1.4916059 0.5784393 -1.9095349 1.2488891 0.40152097 -0.33903646 1.8610382 -0.22350395 -0.94860715) (1.2874851 1.0844897 -1.9168866 -0.042802844 -0.93771714 1.3879867 -0.46589965 2.1235034 -1.4425595 0.19452804) (-1.3601065 -1.061901 0.8746508 -0.7386104 1.9441216 0.72194284 -0.17606297 -0.17282888 2.7211494 0.41256538) (-1.3365294 0.41671795 0.7274572 -1.0201341 0.07424722 1.0455105 2.0199647 0.07859164 -0.81086075 0.3092433) (-0.56345034 1.2789423 -0.6200388 0.95400995 -1.3767093 -1.0824282 0.4128665 0.6920807 -0.22273287 -2.4520183)) :op #<PROCEEDNODE {100397A8B3}> :requires-grad NIL :variables (STC80700) :tracker #<TRACKER :order={row(0 1)} :shape=(A B) :contiguous-p=T>} [method] backward (forward runtime &optional prev-dout) Compute the backward pass of the compiled computational graph (GraphRuntime). Note that the prev-dout is ignored. Forward pass must be computed first. Gradients are automatically reset to zero before the forward pass. [function] proceed (proceed &rest tensors) Compiles the given tensors, returning an evaluated tensors. [macro] with-no-grad (with-no-grad &body body) Within the scope of with-no-grad , gradient computation is disabled. This parameter should be set during inference, such that the compiler will generate more efficient code. [variable] inference-mode When *inference-mode* is set to T, it explicitly indicates that the code is being executed in inference mode. Additionally, it has the following effects: Static Random Generation (e.g., rand) with :requires-grad=T does nothing (it is expected to load a parameter from the state dictionary). The behavior of certain NN operations, such as BatchNorm and Dropout, changes. [macro] with-inference-mode (with-inference-mode (() &body body)) Sets *inference-mode*=T and *no-grad*=T within the scope of the body. [function] get-global-runtime (get-global-runtime) Returns a temporary runtime object just used for allocation global buffer. Examples Tensor Creation lisp CATEN-USER> ( make-tensor ` ( 30 30 )) Result Result {Tensor[float32] :shape (30 30) :id TID102883 :buffer nil :op #<ALLOCATE {100397B2E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(30 30) :contiguous-p=T>} Realize lisp CATEN-USER> ( proceed ( make-tensor ` ( 30 30 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (30 30) :id STC102946 ((0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) ... (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {10039C19D3}> :requires-grad NIL :variables (TID102887) :tracker #<TRACKER :order={row(0 1)} :shape=(30 30) :contiguous-p=T>} Creating a computational graph (Lazy) lisp CATEN-USER> ( !add ( ax+b ` ( 3 3 ) 0 1 ) ( ax+b ` ( 3 3 ) 0 1 )) Result Result {Tensor[float32] :shape (3 3) :id STC102961 :buffer nil :op #<ADD {1003A28F53}> :requires-grad NIL :variables (STC102953 STC102960) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Evaluating a computational graph. lisp CATEN-USER> ( proceed ( !add ( ax+b ` ( 3 3 ) 0 1 ) ( ax+b ` ( 3 3 ) 0 1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103126 ((2.0 2.0 2.0) (2.0 2.0 2.0) (2.0 2.0 2.0)) :op #<PROCEEDNODE {1003BAEBB3}> :requires-grad NIL :variables (STC102976) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} Floating Features [function] inf (inf &key (dtype *default-float*)) Returns positive infinity of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( inf ) Result Result #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103188 ((#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY)) :op #<PROCEEDNODE {1003BC6F23}> :requires-grad NIL :variables (TID103127) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] -inf (-inf &key (dtype *default-float*)) Returns negative infinity of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( -inf ) Result Result #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( -inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103250 ((#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY)) :op #<PROCEEDNODE {1003BE7283}> :requires-grad NIL :variables (TID103189) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] nan (nan &key (dtype *default-float*)) Returns NaN of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( nan ) Result Result #<SINGLE-FLOAT quiet NaN> lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( nan ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103312 ((#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>) (#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>) (#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>)) :op #<PROCEEDNODE {1003BFF5F3}> :requires-grad NIL :variables (TID103251) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>} [function] float-type-of (float-type-of x) Returns :INF if the number is negative infinity, :-INF if the number is negative infinity, :nan if the number is NaN, or T otherwise.","title":"Tensor"},{"location":"packages/caten.api.tensor/#tensor","text":"","title":"Tensor"},{"location":"packages/caten.api.tensor/#struct-tensor","text":"A struct tensor is a multi-dimensional, and strided matrix (and nrank=0 to scalar value) containing elements of the single dtype. Also, the tensor has the following slots: shape[list] The shape as a list of elements of: number, symbol, or Tensor . buffer[AbstractBuffer] The buffer of the tensor. Realized arrays are stored here. dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either :row or :column. id[symbol] The unique identifier of the tensor. (usually created via gensym) op[Func] The Func object that represents the operation of the tensor. views[list] A list of ViewRange objects, determining the bound of loops. requires-grad[boolean] A flag to determine whether the tensor requires a gradient. grad[Tensor] A gradient and realized tensor of the tensor. grad-id[symbol] A unique identifier of the gradient tensor. variables[list] A list of Tensor objects that are used in the operation of the tensor. Tensors listed here are involved in the compilation.","title":"[struct] Tensor"},{"location":"packages/caten.api.tensor/#function-make-tensor","text":"(make-tensor shape &key (dtype *default-float*) (order *default-order*) (id (gensym \"TID\")) (requires-grad nil) (initial-element nil) (views nil) (from nil)) Create a new lazy tensor. shape[list] The shape as a list of elements of: number, symbol, or Tensor . dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either :row or :column. id[symbol] The unique identifier of the tensor. (usually created by gensym) requires-grad[boolean] A flag to determine whether the tensor requires a gradient. initial-element[null|number|symbol|ScalarTensor] An initial value of the tensor. views[list] A list of ViewRange objects, determining the bound of loops. from[null|AbstractBuffer|Symbol] A buffer used to initialize the tensor. If a symbol is given, then the buffer is taken from the variable table.","title":"[function] make-tensor"},{"location":"packages/caten.api.tensor/#function-make-scalar","text":"(make-scalar value &key (dtype *default-float*) (order *default-order*) (id (gensym \"SID\")) (requires-grad nil)) Create a new scalar tensor. A ScalarTensor in Caten is a tensor with a rank of 0. value[number|symbol] The value of the scalar tensor. dtype[keyword] The dtype of the tensor. order[order] The memory layout of the tensor, selected from either of :row or :column. id[symbol] A unique identifier of the tensor (usually created by gensym). requires-grad[boolean] A flag to determine whether the tensor requires a gradient. Hint: You can use fconst , uconst , and iconst to create a scalar tensor with a default dtype (arguments are the same as make-scalar .)","title":"[function] make-scalar"},{"location":"packages/caten.api.tensor/#function-grad","text":"(grad tensor) Returns a gradient of the tensor","title":"[function] grad"},{"location":"packages/caten.api.tensor/#function-shape","text":"(shape tensor) Returns a copy of the shape of the tensor","title":"[function] shape"},{"location":"packages/caten.api.tensor/#function-ndim","text":"(ndim tensor) Returns a rank of the tensor","title":"[function] ndim"},{"location":"packages/caten.api.tensor/#function-dtype-of","text":"(dtype-of tensor) Returns a dtype of the tensor","title":"[function] dtype-of"},{"location":"packages/caten.api.tensor/#function-order","text":"(order tensor) Returns a memory-layout of the tensor.","title":"[function] order"},{"location":"packages/caten.api.tensor/#function-caten","text":"( caten tensors &key ( backend ( ctx:getenv :BACKEND )) ( rewriters nil ) ( simplifiers *external-simplifiers* )) An entry point for compiling the given tensors, returning GraphRuntime. tensor[Tensor|List] toplevel tensors. backend[keyword] a keyword of the backend to use. (assumed to be defined by define-backend) rewriters[list] a list of graph rewriters called each time the compiler will lower the module. simplifiers[list] a list of external simplifiers used in the graph-level compilation (defined by defsimplifier). Pass the function name.","title":"[function] caten"},{"location":"packages/caten.api.tensor/#function-tensor-graph","text":"(tensor-graph tensor) Lowers the given tensors into an aasm graph, only constant folding is applied. For some convenience, this function returns the first tensor if the input is not a tensor.","title":"[function] tensor-graph"},{"location":"packages/caten.api.tensor/#function-tensor-lowered-graph","text":"(tensor-lowered-graph &rest tensors) Creates a lowered graph from the given tensors (i.e., an input graph to JIT=1). ### [method] forward (forward runtime &rest params) Compute the forward pass of the compiled computational graph (GraphRuntime). The params are additional parameters that are used to initialize the GraphRuntime variable table, passed in the following format: - `(symbol . number)` Loaded as a `*default-int*` scalar tensor, used to determine the shape of dynamic shaped tensors. - `(symbol . buffer)` or `(symbol . tensor)` Used to assign the initial elements of the tensor specified by the name in `form` in `(make-tensor ... :from x)` or `(forward runtime place1 value1 place2 value2 ...)` is added recently. Here's an example. #### Examples ```lisp title=\"Example of forward with dynamic shaped\" (let ((model (caten (!randn `(:a :b))))) (forward model :a 10 :b 10)) Result Result {Tensor{LISPBUFFER}[float32] :shape (A B) :id STC102882 ((0.4678763 -0.4861329 -0.94086486 1.04754 2.2546127 0.50788605 -0.10552773 0.049141496 -1.5750943 -1.7679455) (1.3089938 -0.23512708 1.1692804 -0.28485575 0.3088914 -0.6845822 2.1176116 0.66664845 -0.45067576 -0.27492777) (-0.12159313 -1.3346667 -0.5996805 -0.47102702 0.5524228 0.35672027 -0.60367817 -0.21228786 -0.14100133 -1.1875058) (1.1116551 1.8247204 -1.0528817 -0.3054723 0.29911244 0.13449419 0.8812368 -0.08793502 -1.4413477 -0.2823714) (-0.8456068 0.20695838 -0.8928872 0.34807467 0.64626217 -1.1382821 1.5031484 -1.5459219 -0.7998622 -1.2453525) (0.33562827 -1.4916059 0.5784393 -1.9095349 1.2488891 0.40152097 -0.33903646 1.8610382 -0.22350395 -0.94860715) (1.2874851 1.0844897 -1.9168866 -0.042802844 -0.93771714 1.3879867 -0.46589965 2.1235034 -1.4425595 0.19452804) (-1.3601065 -1.061901 0.8746508 -0.7386104 1.9441216 0.72194284 -0.17606297 -0.17282888 2.7211494 0.41256538) (-1.3365294 0.41671795 0.7274572 -1.0201341 0.07424722 1.0455105 2.0199647 0.07859164 -0.81086075 0.3092433) (-0.56345034 1.2789423 -0.6200388 0.95400995 -1.3767093 -1.0824282 0.4128665 0.6920807 -0.22273287 -2.4520183)) :op #<PROCEEDNODE {100397A8B3}> :requires-grad NIL :variables (STC80700) :tracker #<TRACKER :order={row(0 1)} :shape=(A B) :contiguous-p=T>}","title":"[function] tensor-lowered-graph"},{"location":"packages/caten.api.tensor/#method-backward","text":"(forward runtime &optional prev-dout) Compute the backward pass of the compiled computational graph (GraphRuntime). Note that the prev-dout is ignored. Forward pass must be computed first. Gradients are automatically reset to zero before the forward pass.","title":"[method] backward"},{"location":"packages/caten.api.tensor/#function-proceed","text":"(proceed &rest tensors) Compiles the given tensors, returning an evaluated tensors.","title":"[function] proceed"},{"location":"packages/caten.api.tensor/#macro-with-no-grad","text":"(with-no-grad &body body) Within the scope of with-no-grad , gradient computation is disabled. This parameter should be set during inference, such that the compiler will generate more efficient code.","title":"[macro] with-no-grad"},{"location":"packages/caten.api.tensor/#variable-inference-mode","text":"When *inference-mode* is set to T, it explicitly indicates that the code is being executed in inference mode. Additionally, it has the following effects: Static Random Generation (e.g., rand) with :requires-grad=T does nothing (it is expected to load a parameter from the state dictionary). The behavior of certain NN operations, such as BatchNorm and Dropout, changes.","title":"[variable] inference-mode"},{"location":"packages/caten.api.tensor/#macro-with-inference-mode","text":"(with-inference-mode (() &body body)) Sets *inference-mode*=T and *no-grad*=T within the scope of the body.","title":"[macro] with-inference-mode"},{"location":"packages/caten.api.tensor/#function-get-global-runtime","text":"(get-global-runtime) Returns a temporary runtime object just used for allocation global buffer.","title":"[function] get-global-runtime"},{"location":"packages/caten.api.tensor/#examples","text":"","title":"Examples"},{"location":"packages/caten.api.tensor/#tensor-creation","text":"lisp CATEN-USER> ( make-tensor ` ( 30 30 )) Result Result {Tensor[float32] :shape (30 30) :id TID102883 :buffer nil :op #<ALLOCATE {100397B2E3}> :requires-grad NIL :variables NIL :tracker #<TRACKER :order={row(0 1)} :shape=(30 30) :contiguous-p=T>}","title":"Tensor Creation"},{"location":"packages/caten.api.tensor/#realize","text":"lisp CATEN-USER> ( proceed ( make-tensor ` ( 30 30 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (30 30) :id STC102946 ((0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) ... (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0) (0.0 0.0 0.0 0.0 0.0 ~ 0.0 0.0 0.0 0.0 0.0)) :op #<PROCEEDNODE {10039C19D3}> :requires-grad NIL :variables (TID102887) :tracker #<TRACKER :order={row(0 1)} :shape=(30 30) :contiguous-p=T>}","title":"Realize"},{"location":"packages/caten.api.tensor/#creating-a-computational-graph-lazy","text":"lisp CATEN-USER> ( !add ( ax+b ` ( 3 3 ) 0 1 ) ( ax+b ` ( 3 3 ) 0 1 )) Result Result {Tensor[float32] :shape (3 3) :id STC102961 :buffer nil :op #<ADD {1003A28F53}> :requires-grad NIL :variables (STC102953 STC102960) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"Creating a computational graph (Lazy)"},{"location":"packages/caten.api.tensor/#evaluating-a-computational-graph","text":"lisp CATEN-USER> ( proceed ( !add ( ax+b ` ( 3 3 ) 0 1 ) ( ax+b ` ( 3 3 ) 0 1 ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103126 ((2.0 2.0 2.0) (2.0 2.0 2.0) (2.0 2.0 2.0)) :op #<PROCEEDNODE {1003BAEBB3}> :requires-grad NIL :variables (STC102976) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"Evaluating a computational graph."},{"location":"packages/caten.api.tensor/#floating-features","text":"","title":"Floating Features"},{"location":"packages/caten.api.tensor/#function-inf","text":"(inf &key (dtype *default-float*)) Returns positive infinity of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( inf ) Result Result #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103188 ((#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-POSITIVE-INFINITY)) :op #<PROCEEDNODE {1003BC6F23}> :requires-grad NIL :variables (TID103127) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] inf"},{"location":"packages/caten.api.tensor/#function-inf_1","text":"(-inf &key (dtype *default-float*)) Returns negative infinity of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( -inf ) Result Result #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( -inf ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103250 ((#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY) (#.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY #.SB-EXT:SINGLE-FLOAT-NEGATIVE-INFINITY)) :op #<PROCEEDNODE {1003BE7283}> :requires-grad NIL :variables (TID103189) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] -inf"},{"location":"packages/caten.api.tensor/#function-nan","text":"(nan &key (dtype *default-float*)) Returns NaN of the dtype for the current Common Lisp implementation. This feature is supported by float-features lisp CATEN-USER> ( nan ) Result Result #<SINGLE-FLOAT quiet NaN> lisp CATEN-USER> ( proceed ( !full ` ( 3 3 ) ( nan ))) Result Result {Tensor{LISPBUFFER}[float32] :shape (3 3) :id STC103312 ((#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>) (#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>) (#<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN> #<SINGLE-FLOAT quiet NaN>)) :op #<PROCEEDNODE {1003BFF5F3}> :requires-grad NIL :variables (TID103251) :tracker #<TRACKER :order={row(0 1)} :shape=(3 3) :contiguous-p=T>}","title":"[function] nan"},{"location":"packages/caten.api.tensor/#function-float-type-of","text":"(float-type-of x) Returns :INF if the number is negative infinity, :-INF if the number is negative infinity, :nan if the number is NaN, or T otherwise.","title":"[function] float-type-of"},{"location":"packages/caten.apps.gpt2/","text":"caten/apps.gpt2 Implements GPT2 text generation from the pre-trained model. [function] make-gpt2 (make-gpt2 model-type &key max-seq-len) Creates a compiled GPT2 model from the pre-trained gguf file. The model-type must be one of :gpt2 , :gpt2-medium , :gpt2-large , or :gpt2-xl . The max-seq-len is the maximum sequence length for the model. Since GPT2 is a heavy model, you should consider JIT=1 as a prerequisite. The pretrained model is downloaded from the following HuggingFace repository: ( defun url ( model-type ) ( format nil \"https://huggingface.co/hikettei/gpt2-gguf/resolve/main/~(~a~)-f32.gguf?download=true\" model-type )) [function] gpt2-inference (gpt2-inference model input) Takes a compiled GPT2 model and a string input, and generates a text output.","title":"caten/apps.gpt2"},{"location":"packages/caten.apps.gpt2/#catenappsgpt2","text":"Implements GPT2 text generation from the pre-trained model.","title":"caten/apps.gpt2"},{"location":"packages/caten.apps.gpt2/#function-make-gpt2","text":"(make-gpt2 model-type &key max-seq-len) Creates a compiled GPT2 model from the pre-trained gguf file. The model-type must be one of :gpt2 , :gpt2-medium , :gpt2-large , or :gpt2-xl . The max-seq-len is the maximum sequence length for the model. Since GPT2 is a heavy model, you should consider JIT=1 as a prerequisite. The pretrained model is downloaded from the following HuggingFace repository: ( defun url ( model-type ) ( format nil \"https://huggingface.co/hikettei/gpt2-gguf/resolve/main/~(~a~)-f32.gguf?download=true\" model-type ))","title":"[function] make-gpt2"},{"location":"packages/caten.apps.gpt2/#function-gpt2-inference","text":"(gpt2-inference model input) Takes a compiled GPT2 model and a string input, and generates a text output.","title":"[function] gpt2-inference"},{"location":"packages/caten.apps/","text":"Overview Do you want to integrate Caten's features into your Common Lisp application? With caten/apps , you can effortlessly experiment with inference using ready-made implementations of various deep learning models. caten/apps is a suite of packages that implement pipelines to smoothly perform the following tasks: Automatically download pretrained weights Automatically optimize and compile models for inference For instance, text generation inference provided by caten/apps.gpt2 can be accomplished in just two lines: ( defparameter *model* ( make-gpt2 :gpt2 )) ( gpt2-generate *model* \"What is the answer to life, and the world?\" ) If you're considering contributing to Caten, we warmly welcome PRs that add new models to caten/apps . If you've implemented a model using Caten, please consider adding it to caten/apps ! (Feel free to create a PR!)","title":"Overview"},{"location":"packages/caten.apps/#overview","text":"Do you want to integrate Caten's features into your Common Lisp application? With caten/apps , you can effortlessly experiment with inference using ready-made implementations of various deep learning models. caten/apps is a suite of packages that implement pipelines to smoothly perform the following tasks: Automatically download pretrained weights Automatically optimize and compile models for inference For instance, text generation inference provided by caten/apps.gpt2 can be accomplished in just two lines: ( defparameter *model* ( make-gpt2 :gpt2 )) ( gpt2-generate *model* \"What is the answer to life, and the world?\" ) If you're considering contributing to Caten, we warmly welcome PRs that add new models to caten/apps . If you've implemented a model using Caten, please consider adding it to caten/apps ! (Feel free to create a PR!)","title":"Overview"},{"location":"packages/caten.codegen/","text":"caten/codegen Hackable Kernel Generator The caten/codegen is a system designed to JIT-compile AASM Graphs into other languages, such as C or Metal, to achieve faster computation. This package has the following two objectives: Device Independence : It imposes no constraints on the execution device, allowing users to extend it with only 200\u2013300 lines of code. Performance : It ensures computations run fairly fast on each target device. Caten's code generation is divided into two stages based on the intended purpose: caten/codegen Generates unoptimized code (e.g., Scheduler, Lowerer, op fusion, etc.). caten/polyhedral (Optional) Performs advanced kernel loop transformations and optimizations on the scheduled code, such as Auto Scheduler. This document focuses on the first stage, caten/codegen , and provides related information and some examples. How to enable JIT To enable JIT, you need to feed the :BACKEND contextvar with the desired backend. (backends must be defined by caten/codegen/backend:define-backend and :JIT must be set to 1) The following code snippet demonstrates how to enable JIT on repl: ;; Option1: Set globally (setf (ctx:getenv :BACKEND) \"CLANG\") ;; Option2: Set locally (ctx:with-contextvar (:BACKEND \"CLANG\") ;; ... ) After setting the contextvar, you can run the function caten to JIT-compile the AASM Graphs, e.g.: (caten (!rand `(3 3))) How to debug JIT You can set the JIT_DEBUG contextvar to debug the JIT compilation process. The debugging level ranges from 0 to 4, as shown below: JIT_DEBUG | Effect ----------|------- 0 | None 1 | Nothing for now 2 | Displays blueprint and scop 3 | Displays schedule-graph 4 | Displays memory-planner and the compiled code How to learn JIT You can start by reading ./caten/docs/getting-started.lisp to understand the basic concepts of caten/codegen through a running example. Additionally, the following sections provide documentation and explanations for each JIT mechanism, along with simple executable examples. Shape Inference The JIT compilation starts by inferring the shape of each points in the graph. caten/codegen/shape-inference is responsible for this task. [function] run-type-infer (run-type-infer runtime) Runs the shape inference to the given GraphRuntime, returning Type-Reporter . [struct] Inferred-Type A structure Inferred-Type contains the shape/stride/view information of the node at each points. Also, it containts the iteration space information that is used to render the kernel. If the shape inference is successfully done and properly deployed to the target graph by the rewriting-rule , the function (read-type-relay node) will return the Inferred-Type structure. relay-reads returns the list of the buffer corresponding to the node-reads. (If node-reads a number, nil is set) relay-writes returns the list of the buffer corresponding to the node-writes. relay-read-iters returns the list of the iteration space corresponding to the node-reads. relay-write-iters returns the list of the iteration space corresponding to the node-writes. [struct] Iteration-Space Iteration-Space is a structure that contains the shape/stride/view information of the each buffer. It is used to render the kernel. iteration-space-shape to get the shape, iteration-space-strides to get the stride, iteration-space-views to get the offsets and increments, and iteration-space-procedure to get how the iteraton-space is collapsed or permuted. Each elements for shape/stride are Expr. iteration-space-expr-aref to render the aref. (iteration-space-expr-aref iteration-space buffer gids) gids corresponds for the loop idx in the kernel. Rewriting Rules TODO: apply-rewriting-rules doc Optimization for gemm TODO: WMMA Rewriting Docs Scheduler The caten/codegen/scheduler is responsible for assigining the execution order of the computation graph in the aasm. Occasionally they merges or rewrites a view in order to schedule multiple nodes into the same schedule if possible. The function graph-schedule is an entry point, and takes a shape-inferred aasm graph as input, performs scheduling, and returns a schedule graph (called Schedule-Graph) whose each node is composed of Schedule-Item. One Schedule-Item corresponds to one kernel in GPU, graph-schedule must ensure that each item is merged only within the bounds that can be lowered into a single kernel.## Schedule-Item :SCHEDULE-ITEM Schedule-Item is an intermidate object to represent a one kernel in GPU. f = cache_name or name write_ids = f(*[storage_id_dst], *[dynamic_shape], *[inputs]) ^ can be modified by the memory-planner It has a unique name , and cache-name . If cache-name was assigned, the compiler will fail to compile this schedule-item and reuse the kernel named cache-name instead. In order to lowering the computation graph as the foreign language, items must be consisted of JITAble operations (except for special irs and :allocate). If it qualifies, jitable is set to T. Otherwise, the scheduled items are relocated to the compiled GraphRuntime directly. Specifially, if the item was :ALLOCATE, :allocated-p is set to T. blueprint[list] is a lowered schedule-item polyhedral[list] is a Polyhedral IR obtained by lowering blueprint auto-schedule-p[list] is set to T if it is worth to run auto-scheduler. (If there is a symbolic incremental, loop is not an affine and cannot run isl scheduler) items[list] are the scheduled items items-to-cache[list] are the copies of items but having the unique read/write. It is used to determine the equivalence of the two schedule-items. rank[fixnum] is the highest rank of the iteration space. storage-id-src[list] is the list of the storage-id of the source buffer (optimized by running memory-planner) storage-id-dst[list] is the list of the storage-id of the destination buffer (optimized by running memory-planner) namespace[list] a list of symbols appeared in the items. It is used to map a new blueprint from the cached blueprint. When optimizing schedule-item in-place, the 0th read is consumed. [function] graph-schedule (graph-schedule graph) Creates a schedule-graph(FastGraph) from the given graph . Example (Scheduler + Shape Inference + Rewriting Rules) This code snippet demonstrates how to create a schedule-graph from AASM Graph. AASM Graph is obtained by running caten with JIT=0. lisp CATEN-USER> ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( pprint-graph ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {0} \u2514 :MAX {N1} \u2514 :VIEW {N2} \u2502 \u251c load(0.0) \u2502 \u2514 Allocate[:float32] NIL \u251c :VIEW {N3} \u251c :MOVE {N4} \u251c Allocate[:float32] (3 3 1) \u2514 :VIEW {N5} \u251c :ADD {N6} \u251c :VIEW {N7} \u2502 \u251c load(0.0) \u2502 \u2514 Allocate[:float32] (3 3 1) \u2514 :MUL {N8} \u2514 :VIEW {N9} \u2502 \u251c Allocate[:float32] (3 3) \u251c :MOVE {N10} \u251c Allocate[:float32] (3 3 3) \u2514 :VIEW {N11} \u251c Allocate[:float32] (3 3) ============================================================================================================================================ Example ( let* (( graph ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) ( vm ( make-runtime graph :fw-outputs ( graph-outputs graph )))) ( run-type-infer vm ) ;; Running the shape inference ( apply-rewriting-rules vm ) ;; Running the rewriting rules ;; graph-schedule to finally run the scheduler ( pprint-graph ( graph-schedule ( runtime-graph vm )))) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {N1} \u2514 [KERNEL] FUSED_RELU_SUMNODE_MATMUL18889 \u251c Allocate[:float32] (3 3 3) \u251c Allocate[:float32] (3 3) \u251c Allocate[:float32] (3 3) \u251c Allocate[:float32] (3 3 1) \u251c Allocate[:float32] (3 3 1) \u2514 Allocate[:float32] NIL ============================================================================================================================================ Lowerer (Blueprint) The package caten/codegen/blueprint is responsible for lowering the schedule-item into a blueprint. A blueprint is an IR that represents a computation graph with explicit loop bounds. The lower-schedule-item method infers loop boundaries based on Schedule-item and performs lowering into a format that includes :FOR/:ENDFOR nodes.### [function] LOWER-SCHEDULE-ITEM (lower-schedule-item node base-graph scheduled-graph) Lowers the Schedule-Item into blueprint. node is a schedule-item to lower which belongs to scheduled-graph. base-graph is the graph you passed to the graph-schedule. scheduled-graph is the scheduled graph obtained by graph-schedule. Example (Scheduler + Lowerer) This code snippet demonstrates how to lower the schedule-graph into a blueprint created in the previous section. Example ( let* (( graph ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) ( vm ( make-runtime graph :fw-outputs ( graph-outputs graph )))) ( run-type-infer vm ) ;; Running the shape inference ( apply-rewriting-rules vm ) ;; Running the rewriting rules ;; graph-schedule to finally run the scheduler ( let (( schedule-graph ( graph-schedule ( runtime-graph vm )))) ( caten/codegen/expr-cache::with-expr-cache () ;; need this to forcibly keep the loop affine. ( let (( bp ( lower-schedule-item ( nth 5 ( graph-nodes schedule-graph )) graph schedule-graph ))) ( print-blueprint bp nil ))))) Result Result { for (int _gid0=0;(_gid0<3);_gid0+=1) { for (int _gid1=0;(_gid1<3);_gid1+=1) { val_2 = 0.0; for (int _gid2=0;(_gid2<3);_gid2+=1) { val_2 = (val_2+(val_4[((3*_gid0)+_gid2)]*val_6[(_gid1+(3*_gid2))])); // :reduction=t } // _gid2 val_12[((3*_gid0)+_gid1)] = max(val_2, 0.0); } // _gid1 } // _gid0 } Renderer TODO (Docs for Renderer/rendre-expr/extensible polyhedral compiler/etc) Here is a list of nodes used to render the kernel. Render IRs :DEFINE-GLOBAL The node :DEFINE-GLOBAL declares a global variable in the kernel. (it corresponds to the argument of the kernel.) When optimizing define-global in-place, the 0th read is consumed. :AREF :AREF corresponds to the following code: ID[*space] storage-id[symbol] An index to the reference pointer optimized by the memory-planner. buffer[AbstractBuffer] The buffer to be accessed. space[Iteration-Space] The iteration space :AREF belongs to. When optimizing aref in-place, the 0th read is consumed. :ENDIF } // endif When optimizing endif in-place, the 0th read is consumed. :IF if(condition) When optimizing if in-place, the 0th read is consumed. :ENDFOR } // idx When optimizing endfor in-place, the 0th read is consumed. :FOR for(int idx=upfrom, below, by) scope[keyword] If :global , the loop is parallelizable. One of :global, :local. When optimizing for in-place, the 0th read is consumed. EXPR TODO Memory Planner TODO: Docs (MIP Solver) Scop Analyzer TODO: Docs for scop","title":"caten/codegen"},{"location":"packages/caten.codegen/#catencodegen","text":"","title":"caten/codegen"},{"location":"packages/caten.codegen/#hackable-kernel-generator","text":"The caten/codegen is a system designed to JIT-compile AASM Graphs into other languages, such as C or Metal, to achieve faster computation. This package has the following two objectives: Device Independence : It imposes no constraints on the execution device, allowing users to extend it with only 200\u2013300 lines of code. Performance : It ensures computations run fairly fast on each target device. Caten's code generation is divided into two stages based on the intended purpose: caten/codegen Generates unoptimized code (e.g., Scheduler, Lowerer, op fusion, etc.). caten/polyhedral (Optional) Performs advanced kernel loop transformations and optimizations on the scheduled code, such as Auto Scheduler. This document focuses on the first stage, caten/codegen , and provides related information and some examples.","title":"Hackable Kernel Generator"},{"location":"packages/caten.codegen/#how-to-enable-jit","text":"To enable JIT, you need to feed the :BACKEND contextvar with the desired backend. (backends must be defined by caten/codegen/backend:define-backend and :JIT must be set to 1) The following code snippet demonstrates how to enable JIT on repl: ;; Option1: Set globally (setf (ctx:getenv :BACKEND) \"CLANG\") ;; Option2: Set locally (ctx:with-contextvar (:BACKEND \"CLANG\") ;; ... ) After setting the contextvar, you can run the function caten to JIT-compile the AASM Graphs, e.g.: (caten (!rand `(3 3)))","title":"How to enable JIT"},{"location":"packages/caten.codegen/#how-to-debug-jit","text":"You can set the JIT_DEBUG contextvar to debug the JIT compilation process. The debugging level ranges from 0 to 4, as shown below: JIT_DEBUG | Effect ----------|------- 0 | None 1 | Nothing for now 2 | Displays blueprint and scop 3 | Displays schedule-graph 4 | Displays memory-planner and the compiled code","title":"How to debug JIT"},{"location":"packages/caten.codegen/#how-to-learn-jit","text":"You can start by reading ./caten/docs/getting-started.lisp to understand the basic concepts of caten/codegen through a running example. Additionally, the following sections provide documentation and explanations for each JIT mechanism, along with simple executable examples.","title":"How to learn JIT"},{"location":"packages/caten.codegen/#shape-inference","text":"The JIT compilation starts by inferring the shape of each points in the graph. caten/codegen/shape-inference is responsible for this task.","title":"Shape Inference"},{"location":"packages/caten.codegen/#function-run-type-infer","text":"(run-type-infer runtime) Runs the shape inference to the given GraphRuntime, returning Type-Reporter .","title":"[function] run-type-infer"},{"location":"packages/caten.codegen/#struct-inferred-type","text":"A structure Inferred-Type contains the shape/stride/view information of the node at each points. Also, it containts the iteration space information that is used to render the kernel. If the shape inference is successfully done and properly deployed to the target graph by the rewriting-rule , the function (read-type-relay node) will return the Inferred-Type structure. relay-reads returns the list of the buffer corresponding to the node-reads. (If node-reads a number, nil is set) relay-writes returns the list of the buffer corresponding to the node-writes. relay-read-iters returns the list of the iteration space corresponding to the node-reads. relay-write-iters returns the list of the iteration space corresponding to the node-writes.","title":"[struct] Inferred-Type"},{"location":"packages/caten.codegen/#struct-iteration-space","text":"Iteration-Space is a structure that contains the shape/stride/view information of the each buffer. It is used to render the kernel. iteration-space-shape to get the shape, iteration-space-strides to get the stride, iteration-space-views to get the offsets and increments, and iteration-space-procedure to get how the iteraton-space is collapsed or permuted. Each elements for shape/stride are Expr. iteration-space-expr-aref to render the aref. (iteration-space-expr-aref iteration-space buffer gids) gids corresponds for the loop idx in the kernel.","title":"[struct] Iteration-Space"},{"location":"packages/caten.codegen/#rewriting-rules","text":"TODO: apply-rewriting-rules doc","title":"Rewriting Rules"},{"location":"packages/caten.codegen/#optimization-for-gemm","text":"TODO: WMMA Rewriting Docs","title":"Optimization for gemm"},{"location":"packages/caten.codegen/#scheduler","text":"The caten/codegen/scheduler is responsible for assigining the execution order of the computation graph in the aasm. Occasionally they merges or rewrites a view in order to schedule multiple nodes into the same schedule if possible. The function graph-schedule is an entry point, and takes a shape-inferred aasm graph as input, performs scheduling, and returns a schedule graph (called Schedule-Graph) whose each node is composed of Schedule-Item. One Schedule-Item corresponds to one kernel in GPU, graph-schedule must ensure that each item is merged only within the bounds that can be lowered into a single kernel.## Schedule-Item","title":"Scheduler"},{"location":"packages/caten.codegen/#schedule-item","text":"Schedule-Item is an intermidate object to represent a one kernel in GPU. f = cache_name or name write_ids = f(*[storage_id_dst], *[dynamic_shape], *[inputs]) ^ can be modified by the memory-planner It has a unique name , and cache-name . If cache-name was assigned, the compiler will fail to compile this schedule-item and reuse the kernel named cache-name instead. In order to lowering the computation graph as the foreign language, items must be consisted of JITAble operations (except for special irs and :allocate). If it qualifies, jitable is set to T. Otherwise, the scheduled items are relocated to the compiled GraphRuntime directly. Specifially, if the item was :ALLOCATE, :allocated-p is set to T. blueprint[list] is a lowered schedule-item polyhedral[list] is a Polyhedral IR obtained by lowering blueprint auto-schedule-p[list] is set to T if it is worth to run auto-scheduler. (If there is a symbolic incremental, loop is not an affine and cannot run isl scheduler) items[list] are the scheduled items items-to-cache[list] are the copies of items but having the unique read/write. It is used to determine the equivalence of the two schedule-items. rank[fixnum] is the highest rank of the iteration space. storage-id-src[list] is the list of the storage-id of the source buffer (optimized by running memory-planner) storage-id-dst[list] is the list of the storage-id of the destination buffer (optimized by running memory-planner) namespace[list] a list of symbols appeared in the items. It is used to map a new blueprint from the cached blueprint. When optimizing schedule-item in-place, the 0th read is consumed.","title":":SCHEDULE-ITEM"},{"location":"packages/caten.codegen/#function-graph-schedule","text":"(graph-schedule graph) Creates a schedule-graph(FastGraph) from the given graph .","title":"[function] graph-schedule"},{"location":"packages/caten.codegen/#example-scheduler-shape-inference-rewriting-rules","text":"This code snippet demonstrates how to create a schedule-graph from AASM Graph. AASM Graph is obtained by running caten with JIT=0. lisp CATEN-USER> ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( pprint-graph ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {0} \u2514 :MAX {N1} \u2514 :VIEW {N2} \u2502 \u251c load(0.0) \u2502 \u2514 Allocate[:float32] NIL \u251c :VIEW {N3} \u251c :MOVE {N4} \u251c Allocate[:float32] (3 3 1) \u2514 :VIEW {N5} \u251c :ADD {N6} \u251c :VIEW {N7} \u2502 \u251c load(0.0) \u2502 \u2514 Allocate[:float32] (3 3 1) \u2514 :MUL {N8} \u2514 :VIEW {N9} \u2502 \u251c Allocate[:float32] (3 3) \u251c :MOVE {N10} \u251c Allocate[:float32] (3 3 3) \u2514 :VIEW {N11} \u251c Allocate[:float32] (3 3) ============================================================================================================================================ Example ( let* (( graph ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) ( vm ( make-runtime graph :fw-outputs ( graph-outputs graph )))) ( run-type-infer vm ) ;; Running the shape inference ( apply-rewriting-rules vm ) ;; Running the rewriting rules ;; graph-schedule to finally run the scheduler ( pprint-graph ( graph-schedule ( runtime-graph vm )))) Result Result ============================================================================================================================================ === [P: 0] ================================================================================================================================= [P=0, ID=0]: :PAUSE/BACKWARD {N1} \u2514 [KERNEL] FUSED_RELU_SUMNODE_MATMUL18889 \u251c Allocate[:float32] (3 3 3) \u251c Allocate[:float32] (3 3) \u251c Allocate[:float32] (3 3) \u251c Allocate[:float32] (3 3 1) \u251c Allocate[:float32] (3 3 1) \u2514 Allocate[:float32] NIL ============================================================================================================================================","title":"Example (Scheduler + Shape Inference + Rewriting Rules)"},{"location":"packages/caten.codegen/#lowerer-blueprint","text":"The package caten/codegen/blueprint is responsible for lowering the schedule-item into a blueprint. A blueprint is an IR that represents a computation graph with explicit loop bounds. The lower-schedule-item method infers loop boundaries based on Schedule-item and performs lowering into a format that includes :FOR/:ENDFOR nodes.### [function] LOWER-SCHEDULE-ITEM (lower-schedule-item node base-graph scheduled-graph) Lowers the Schedule-Item into blueprint. node is a schedule-item to lower which belongs to scheduled-graph. base-graph is the graph you passed to the graph-schedule. scheduled-graph is the scheduled graph obtained by graph-schedule.","title":"Lowerer (Blueprint)"},{"location":"packages/caten.codegen/#example-scheduler-lowerer","text":"This code snippet demonstrates how to lower the schedule-graph into a blueprint created in the previous section. Example ( let* (( graph ( ctx:with-contextvar ( :BACKEND \"LISP\" ) ( runtime-graph ( caten ( !relu ( !matmul ( make-tensor ` ( 3 3 )) ( make-tensor ` ( 3 3 )))))))) ( vm ( make-runtime graph :fw-outputs ( graph-outputs graph )))) ( run-type-infer vm ) ;; Running the shape inference ( apply-rewriting-rules vm ) ;; Running the rewriting rules ;; graph-schedule to finally run the scheduler ( let (( schedule-graph ( graph-schedule ( runtime-graph vm )))) ( caten/codegen/expr-cache::with-expr-cache () ;; need this to forcibly keep the loop affine. ( let (( bp ( lower-schedule-item ( nth 5 ( graph-nodes schedule-graph )) graph schedule-graph ))) ( print-blueprint bp nil ))))) Result Result { for (int _gid0=0;(_gid0<3);_gid0+=1) { for (int _gid1=0;(_gid1<3);_gid1+=1) { val_2 = 0.0; for (int _gid2=0;(_gid2<3);_gid2+=1) { val_2 = (val_2+(val_4[((3*_gid0)+_gid2)]*val_6[(_gid1+(3*_gid2))])); // :reduction=t } // _gid2 val_12[((3*_gid0)+_gid1)] = max(val_2, 0.0); } // _gid1 } // _gid0 }","title":"Example (Scheduler + Lowerer)"},{"location":"packages/caten.codegen/#renderer","text":"TODO (Docs for Renderer/rendre-expr/extensible polyhedral compiler/etc) Here is a list of nodes used to render the kernel.","title":"Renderer"},{"location":"packages/caten.codegen/#render-irs","text":"","title":"Render IRs"},{"location":"packages/caten.codegen/#define-global","text":"The node :DEFINE-GLOBAL declares a global variable in the kernel. (it corresponds to the argument of the kernel.) When optimizing define-global in-place, the 0th read is consumed.","title":":DEFINE-GLOBAL"},{"location":"packages/caten.codegen/#aref","text":":AREF corresponds to the following code: ID[*space] storage-id[symbol] An index to the reference pointer optimized by the memory-planner. buffer[AbstractBuffer] The buffer to be accessed. space[Iteration-Space] The iteration space :AREF belongs to. When optimizing aref in-place, the 0th read is consumed.","title":":AREF"},{"location":"packages/caten.codegen/#endif","text":"} // endif When optimizing endif in-place, the 0th read is consumed.","title":":ENDIF"},{"location":"packages/caten.codegen/#if","text":"if(condition) When optimizing if in-place, the 0th read is consumed.","title":":IF"},{"location":"packages/caten.codegen/#endfor","text":"} // idx When optimizing endfor in-place, the 0th read is consumed.","title":":ENDFOR"},{"location":"packages/caten.codegen/#for","text":"for(int idx=upfrom, below, by) scope[keyword] If :global , the loop is parallelizable. One of :global, :local. When optimizing for in-place, the 0th read is consumed.","title":":FOR"},{"location":"packages/caten.codegen/#expr","text":"TODO","title":"EXPR"},{"location":"packages/caten.codegen/#memory-planner","text":"TODO: Docs (MIP Solver)","title":"Memory Planner"},{"location":"packages/caten.codegen/#scop-analyzer","text":"TODO: Docs for scop","title":"Scop Analyzer"},{"location":"packages/caten.external.gguf/","text":"GGUF Looking to work with quantized models? The caten/gguf package enables you to read quantized model weights directly from GGUF files, construct a StateDict, and create tokenizers for caten/llm . It's a convenient tool for integrating quantized models into your projects. Please note that the quantization functionality currently supports only a limited number of bit depths. We appreciate your understanding and are continuously working to expand this feature. (As of this writing, fp32/fp16 only, Int8 Quant will be added soon!) [class] GGUF A class that represents the GGUF file format. (gguf-version gguf) returns a fixnum indicating the version of the GGUF file. (gguf-tensor-count gguf) returns the number of tensors in the GGUF file. (gguf-metadata-kv-count gguf) returns the number of metadata key-value pairs in the GGUF file. (gguf-metadata gguf) returns a list of metadata key-value pairs. (gguf-tensor-info gguf) returns a list of tensor information. (gguf-metadata-get gguf key) to get the corresponding value of the key where key is a string. [function] make-gguf Creates GGUF from the given stream. The definition is described in the following link: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#file-structure In short, this function accepts the following format: [Magic Number (4 Byte)] | [GGUF Version (4 Byte)] | [Tensor_Count (8 Byte)] | [Metadata_KV_Count (8 Byte)] | [Rest_data] [function] load-gguf (load-gguf pathname) Creates a gguf file from the given pathname. [function] load-gguf-url (load-gguf-url url filename &optional output-directory) Creates a gguf file from the given URL. The downloaded file will be saved in the output-directory named as filename. If the file already exists, it will not download the file again. [function] gguf->state-dict (gguf->state-dict gguf) Creates a caten/state-dict from the given gguf file's tensor-info. [function] gguf->bpe-tokenizer (gguf->bpe-tokenizer gguf &key (metadata-tokens \"tokenizer.ggml.tokens\") (metadata-merges \"tokenizer.ggml.merges\")) Creates a BPE tokenizer (which is caten/llm:Tokenizer) from the given gguf file's metadata. [struct] Metadata A structure to represent metadata in GGUF file. (metadata-key metadata) to access the key typed as string. (metadata-value-type metadata) to access the type of value typed as a keyword. (metadata-value metadata) to access the value typed as a number, boolean, or simple-array. [struct] Tensor-Info Tensor-Info stores the information of a tensor in the gguf file. (tensor-info-name tensor-info) returns the name of the tensor which is a string, (tensor-info-n-dimension tensor-info) returns the rank of the tensor. (tensor-info-dimensions tensor-info) returns the shape of the tensor. (tensor-info-ggml-type tensor-info) returns the data type of the tensor which is a keyword. (tensor-info-relative-offset tensor-info) returns the offset of the tensor in the gguf file. (tensor-info-absolute-offset tensor-info) returns the absolute offset of the tensor in the stream, (inconviniently buffers are stored with this offset but caten will precompute them). (tensor-info-buffer tensor-info) returns the parsed buffer of the tensor.","title":"caten/gguf"},{"location":"packages/caten.external.gguf/#gguf","text":"Looking to work with quantized models? The caten/gguf package enables you to read quantized model weights directly from GGUF files, construct a StateDict, and create tokenizers for caten/llm . It's a convenient tool for integrating quantized models into your projects. Please note that the quantization functionality currently supports only a limited number of bit depths. We appreciate your understanding and are continuously working to expand this feature. (As of this writing, fp32/fp16 only, Int8 Quant will be added soon!)","title":"GGUF"},{"location":"packages/caten.external.gguf/#class-gguf","text":"A class that represents the GGUF file format. (gguf-version gguf) returns a fixnum indicating the version of the GGUF file. (gguf-tensor-count gguf) returns the number of tensors in the GGUF file. (gguf-metadata-kv-count gguf) returns the number of metadata key-value pairs in the GGUF file. (gguf-metadata gguf) returns a list of metadata key-value pairs. (gguf-tensor-info gguf) returns a list of tensor information. (gguf-metadata-get gguf key) to get the corresponding value of the key where key is a string.","title":"[class] GGUF"},{"location":"packages/caten.external.gguf/#function-make-gguf","text":"Creates GGUF from the given stream. The definition is described in the following link: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md#file-structure In short, this function accepts the following format: [Magic Number (4 Byte)] | [GGUF Version (4 Byte)] | [Tensor_Count (8 Byte)] | [Metadata_KV_Count (8 Byte)] | [Rest_data]","title":"[function] make-gguf"},{"location":"packages/caten.external.gguf/#function-load-gguf","text":"(load-gguf pathname) Creates a gguf file from the given pathname.","title":"[function] load-gguf"},{"location":"packages/caten.external.gguf/#function-load-gguf-url","text":"(load-gguf-url url filename &optional output-directory) Creates a gguf file from the given URL. The downloaded file will be saved in the output-directory named as filename. If the file already exists, it will not download the file again.","title":"[function] load-gguf-url"},{"location":"packages/caten.external.gguf/#function-gguf-state-dict","text":"(gguf->state-dict gguf) Creates a caten/state-dict from the given gguf file's tensor-info.","title":"[function] gguf-&gt;state-dict"},{"location":"packages/caten.external.gguf/#function-gguf-bpe-tokenizer","text":"(gguf->bpe-tokenizer gguf &key (metadata-tokens \"tokenizer.ggml.tokens\") (metadata-merges \"tokenizer.ggml.merges\")) Creates a BPE tokenizer (which is caten/llm:Tokenizer) from the given gguf file's metadata.","title":"[function] gguf-&gt;bpe-tokenizer"},{"location":"packages/caten.external.gguf/#struct-metadata","text":"A structure to represent metadata in GGUF file. (metadata-key metadata) to access the key typed as string. (metadata-value-type metadata) to access the type of value typed as a keyword. (metadata-value metadata) to access the value typed as a number, boolean, or simple-array.","title":"[struct] Metadata"},{"location":"packages/caten.external.gguf/#struct-tensor-info","text":"Tensor-Info stores the information of a tensor in the gguf file. (tensor-info-name tensor-info) returns the name of the tensor which is a string, (tensor-info-n-dimension tensor-info) returns the rank of the tensor. (tensor-info-dimensions tensor-info) returns the shape of the tensor. (tensor-info-ggml-type tensor-info) returns the data type of the tensor which is a keyword. (tensor-info-relative-offset tensor-info) returns the offset of the tensor in the gguf file. (tensor-info-absolute-offset tensor-info) returns the absolute offset of the tensor in the stream, (inconviniently buffers are stored with this offset but caten will precompute them). (tensor-info-buffer tensor-info) returns the parsed buffer of the tensor.","title":"[struct] Tensor-Info"},{"location":"packages/caten.external.llm/","text":"Overview TODO: Docs Tokenizers TODO: Docs [class] Tokenizer A tokenizer is a class that can encode and decode strings into and from. - (encode tokenizer string) -> (list fixnum ...) encodes a string into a tokenized form. - (decode tokenizer (list fixnum ...)) -> string decodes a tokenized form into a string. [class] BPETokenizer An implementation of Byte Pair Encoding tokenizer. Initialized by the make-bpe-tokenizer function. [generic] encode NIL [generic] decode NIL Models (TODO: Complete docs) Definitions are here: https://github.com/hikettei/Caten/blob/main/external/llm/layers.lisp Currently, the following models are implemented and tested: Attention Scaled-Dot-Product-Attention FeedForward TransformerBlock Transformer","title":"caten/llm"},{"location":"packages/caten.external.llm/#overview","text":"TODO: Docs","title":"Overview"},{"location":"packages/caten.external.llm/#tokenizers","text":"TODO: Docs","title":"Tokenizers"},{"location":"packages/caten.external.llm/#class-tokenizer","text":"A tokenizer is a class that can encode and decode strings into and from. - (encode tokenizer string) -> (list fixnum ...) encodes a string into a tokenized form. - (decode tokenizer (list fixnum ...)) -> string decodes a tokenized form into a string.","title":"[class] Tokenizer"},{"location":"packages/caten.external.llm/#class-bpetokenizer","text":"An implementation of Byte Pair Encoding tokenizer. Initialized by the make-bpe-tokenizer function.","title":"[class] BPETokenizer"},{"location":"packages/caten.external.llm/#generic-encode","text":"NIL","title":"[generic] encode"},{"location":"packages/caten.external.llm/#generic-decode","text":"NIL","title":"[generic] decode"},{"location":"packages/caten.external.llm/#models","text":"(TODO: Complete docs) Definitions are here: https://github.com/hikettei/Caten/blob/main/external/llm/layers.lisp Currently, the following models are implemented and tested: Attention Scaled-Dot-Product-Attention FeedForward TransformerBlock Transformer","title":"Models"},{"location":"packages/caten.external.onnx/","text":"ONNX Looking to compile Caten models directly from ONNX files? The caten/onnx package has you covered. It supports dynamic shapes and weight loading, allowing for flexible and efficient model integration. Currently, caten/onnx supports a limited set of operations. If you encounter any unsupported operations, we warmly welcome your contributions to expand its functionality. Please consider submitting a PR to add them! We'd like to support all opsets which is impossible! Requirement: cl-onnx requires cl-protobufs , and cl-protobufs may require protocol-compiler to be installed on your system. (Follow the instruction in cl-protobufs first) # Example $ sudo apt-get install protobuf-compiler [function] from-onnx (from-onnx path &key opset) Constructs a caten ir from an onnx model file at path . opset specifies the opset version to use. If not specified, uses the opset version in the file. Inputs or dynamic shapes for caten is created as a keyword (so pass the input tensor like (:input . , (rand (list 3 3))) ...","title":"caten/oonx"},{"location":"packages/caten.external.onnx/#onnx","text":"Looking to compile Caten models directly from ONNX files? The caten/onnx package has you covered. It supports dynamic shapes and weight loading, allowing for flexible and efficient model integration. Currently, caten/onnx supports a limited set of operations. If you encounter any unsupported operations, we warmly welcome your contributions to expand its functionality. Please consider submitting a PR to add them! We'd like to support all opsets which is impossible! Requirement: cl-onnx requires cl-protobufs , and cl-protobufs may require protocol-compiler to be installed on your system. (Follow the instruction in cl-protobufs first) # Example $ sudo apt-get install protobuf-compiler","title":"ONNX"},{"location":"packages/caten.external.onnx/#function-from-onnx","text":"(from-onnx path &key opset) Constructs a caten ir from an onnx model file at path . opset specifies the opset version to use. If not specified, uses the opset version in the file. Inputs or dynamic shapes for caten is created as a keyword (so pass the input tensor like (:input . , (rand (list 3 3))) ...","title":"[function] from-onnx"},{"location":"packages/caten.nn.activations/","text":"Activations [function] !sigmoid (!sigmoid x) Applies the Sigmoid function element-wise to the input tensor. [function] !hard-sigmoid (!hard-sigmoid x &key (alpha 0.2) (beta 0.5)) Applies the HardSigmoid function element-wise to the input tensor. [function] !relu (!relu x) Applies the ReLU function element-wise to the input tensor. [function] !leaky-relu (!leaky-relu x &key (neg-slope 1e-3)) Applies the LeakyReLU function element-wise to the input tensor. [function] !log-softmax (!log-softmax x &key (axis -1)) Applies the LogSoftmax function element-wise to the input tensor. [function] !elu (!elu x &key (alpha 1.0)) Applies the ELU function element-wise to the input tensor. [function] !relu6 (!relu6 x) Applies the ReLU6 function element-wise to the input tensor. [function] !softmax (!softmax x &key (axis -1)) Applies the Softmax function to the input tensor. [function] !softplus (!softplus x &key (beta 1.0)) Applies the Softplus function element-wise to the input tensor. [function] !softsign (!softsign x) Applies the Softsign function element-wise to the input tensor. [function] !softshrink (!softshrink x &key (lmd 0.5)) Applies the SoftShrink function element-wise to the input tensor. [function] !celu (!celu x &key (alpha 1.0)) Applies the CeLU function element-wise to the input tensor. [function] !silu (!silu x) Applies the SiLU function element-wise to the input tensor. [function] !logsigmoid (!logsigmoid x) Applies the LogSigmoid function element-wise to the input tensor. [function] !gelu (!gelu x &key (approx :tanh)) Applies the GeLU activation to the input tensor. There are two ways to approximate the GeLU function. :tanh to use tanh. :sigmoid to use sigmoid. [function] !selu (!selu x) Applies the SeLU function element-wise to the input tensor. [function] !mish (!mish x) Applies the Mish function element-wise to the input tensor. [function] !hardswish (!hardswish x) Applies the HardSwish function element-wise to the input tensor. [function] !hardtanh (!hardtanh x &key (min_val -1.0) (max_val 1.0)) Applies the HardTanh function element-wise to the input tensor. [function] !softmin (!softmin x) Applies the Softmin function element-wise to the input tensor.","title":"Activation"},{"location":"packages/caten.nn.activations/#activations","text":"","title":"Activations"},{"location":"packages/caten.nn.activations/#function-sigmoid","text":"(!sigmoid x) Applies the Sigmoid function element-wise to the input tensor.","title":"[function] !sigmoid"},{"location":"packages/caten.nn.activations/#function-hard-sigmoid","text":"(!hard-sigmoid x &key (alpha 0.2) (beta 0.5)) Applies the HardSigmoid function element-wise to the input tensor.","title":"[function] !hard-sigmoid"},{"location":"packages/caten.nn.activations/#function-relu","text":"(!relu x) Applies the ReLU function element-wise to the input tensor.","title":"[function] !relu"},{"location":"packages/caten.nn.activations/#function-leaky-relu","text":"(!leaky-relu x &key (neg-slope 1e-3)) Applies the LeakyReLU function element-wise to the input tensor.","title":"[function] !leaky-relu"},{"location":"packages/caten.nn.activations/#function-log-softmax","text":"(!log-softmax x &key (axis -1)) Applies the LogSoftmax function element-wise to the input tensor.","title":"[function] !log-softmax"},{"location":"packages/caten.nn.activations/#function-elu","text":"(!elu x &key (alpha 1.0)) Applies the ELU function element-wise to the input tensor.","title":"[function] !elu"},{"location":"packages/caten.nn.activations/#function-relu6","text":"(!relu6 x) Applies the ReLU6 function element-wise to the input tensor.","title":"[function] !relu6"},{"location":"packages/caten.nn.activations/#function-softmax","text":"(!softmax x &key (axis -1)) Applies the Softmax function to the input tensor.","title":"[function] !softmax"},{"location":"packages/caten.nn.activations/#function-softplus","text":"(!softplus x &key (beta 1.0)) Applies the Softplus function element-wise to the input tensor.","title":"[function] !softplus"},{"location":"packages/caten.nn.activations/#function-softsign","text":"(!softsign x) Applies the Softsign function element-wise to the input tensor.","title":"[function] !softsign"},{"location":"packages/caten.nn.activations/#function-softshrink","text":"(!softshrink x &key (lmd 0.5)) Applies the SoftShrink function element-wise to the input tensor.","title":"[function] !softshrink"},{"location":"packages/caten.nn.activations/#function-celu","text":"(!celu x &key (alpha 1.0)) Applies the CeLU function element-wise to the input tensor.","title":"[function] !celu"},{"location":"packages/caten.nn.activations/#function-silu","text":"(!silu x) Applies the SiLU function element-wise to the input tensor.","title":"[function] !silu"},{"location":"packages/caten.nn.activations/#function-logsigmoid","text":"(!logsigmoid x) Applies the LogSigmoid function element-wise to the input tensor.","title":"[function] !logsigmoid"},{"location":"packages/caten.nn.activations/#function-gelu","text":"(!gelu x &key (approx :tanh)) Applies the GeLU activation to the input tensor. There are two ways to approximate the GeLU function. :tanh to use tanh. :sigmoid to use sigmoid.","title":"[function] !gelu"},{"location":"packages/caten.nn.activations/#function-selu","text":"(!selu x) Applies the SeLU function element-wise to the input tensor.","title":"[function] !selu"},{"location":"packages/caten.nn.activations/#function-mish","text":"(!mish x) Applies the Mish function element-wise to the input tensor.","title":"[function] !mish"},{"location":"packages/caten.nn.activations/#function-hardswish","text":"(!hardswish x) Applies the HardSwish function element-wise to the input tensor.","title":"[function] !hardswish"},{"location":"packages/caten.nn.activations/#function-hardtanh","text":"(!hardtanh x &key (min_val -1.0) (max_val 1.0)) Applies the HardTanh function element-wise to the input tensor.","title":"[function] !hardtanh"},{"location":"packages/caten.nn.activations/#function-softmin","text":"(!softmin x) Applies the Softmin function element-wise to the input tensor.","title":"[function] !softmin"},{"location":"packages/caten.nn.convolutions/","text":"Convolutions [class] ConvND (ConvND in-channels out-channels kernel-size &key (groups 1) (stride 1) (dilation 1) (padding 0) (bias t)) Applies a convolution over a tensor with a given weight and optional bias . NOTE: unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions. [function] !convnd (!convnd x weight &key (bias nil) (groups 1) (stride 1) (dilation 1) (padding 0)) Applies a convolutional layer over a tensor x with a given weight and optional bias .","title":"Convolution"},{"location":"packages/caten.nn.convolutions/#convolutions","text":"","title":"Convolutions"},{"location":"packages/caten.nn.convolutions/#class-convnd","text":"(ConvND in-channels out-channels kernel-size &key (groups 1) (stride 1) (dilation 1) (padding 0) (bias t)) Applies a convolution over a tensor with a given weight and optional bias . NOTE: unlike PyTorch, this implementation is not limited to only 2d convolutions and instead works for any number of dimensions.","title":"[class] ConvND"},{"location":"packages/caten.nn.convolutions/#function-convnd","text":"(!convnd x weight &key (bias nil) (groups 1) (stride 1) (dilation 1) (padding 0)) Applies a convolutional layer over a tensor x with a given weight and optional bias .","title":"[function] !convnd"},{"location":"packages/caten.nn.criterion/","text":"Criterion [class] L1NormLoss (L1NormLoss &key (reduction :mean)) Implements L1Norm Loss: l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = abs(x_n - y_n) reduction is one of :mean or :sum. [function] !l1norm (!l1norm a b &key (reduction :mean)) Computes L1Norm [class] MSELoss (MSELoss &key (reduction :mean)) Implements MSE Loss: l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = (x_n - y_n)^2 reduction is one of :mean or :sum. [function] !mse (!mse a b &key (reduction :mean)) Computes MSE Loss between a and b. [class] CrossEntropyLoss (CrossEntropyLoss &key (reduction :mean) (delta 1e-5)) Returns a tensor that measures the Cross-Entropy-Error between each element in the x and labels. labels are one-hot encoded. L_i = -p_ilog(x_i + delta) \\begin{equation} out_i= \\begin{cases} sum(L) & \\text{reduction = sum} \\\\ mean(L) & \\text{reduction = mean} \\\\ L & \\text{otherwise} \\end{cases} \\end{equation} [function] !cross-entropy (!cross-entropy x labels &key (reduction :mean) (delta 1e-5)) Computes Cross Entropy Loss between x and labels. labels are one-hot encoded.","title":"Criterion"},{"location":"packages/caten.nn.criterion/#criterion","text":"","title":"Criterion"},{"location":"packages/caten.nn.criterion/#class-l1normloss","text":"(L1NormLoss &key (reduction :mean)) Implements L1Norm Loss: l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = abs(x_n - y_n) reduction is one of :mean or :sum.","title":"[class] L1NormLoss"},{"location":"packages/caten.nn.criterion/#function-l1norm","text":"(!l1norm a b &key (reduction :mean)) Computes L1Norm","title":"[function] !l1norm"},{"location":"packages/caten.nn.criterion/#class-mseloss","text":"(MSELoss &key (reduction :mean)) Implements MSE Loss: l(x, y) = L = {l_1, ..., l_n}^\\intercal, l_n = (x_n - y_n)^2 reduction is one of :mean or :sum.","title":"[class] MSELoss"},{"location":"packages/caten.nn.criterion/#function-mse","text":"(!mse a b &key (reduction :mean)) Computes MSE Loss between a and b.","title":"[function] !mse"},{"location":"packages/caten.nn.criterion/#class-crossentropyloss","text":"(CrossEntropyLoss &key (reduction :mean) (delta 1e-5)) Returns a tensor that measures the Cross-Entropy-Error between each element in the x and labels. labels are one-hot encoded. L_i = -p_ilog(x_i + delta) \\begin{equation} out_i= \\begin{cases} sum(L) & \\text{reduction = sum} \\\\ mean(L) & \\text{reduction = mean} \\\\ L & \\text{otherwise} \\end{cases} \\end{equation}","title":"[class] CrossEntropyLoss"},{"location":"packages/caten.nn.criterion/#function-cross-entropy","text":"(!cross-entropy x labels &key (reduction :mean) (delta 1e-5)) Computes Cross Entropy Loss between x and labels. labels are one-hot encoded.","title":"[function] !cross-entropy"},{"location":"packages/caten.nn.embeddings/","text":"Embeddings [class] Embedding (Embedding vocab-size embedding-dim) Implements an embedding layer.","title":"Embedding"},{"location":"packages/caten.nn.embeddings/#embeddings","text":"","title":"Embeddings"},{"location":"packages/caten.nn.embeddings/#class-embedding","text":"(Embedding vocab-size embedding-dim) Implements an embedding layer.","title":"[class] Embedding"},{"location":"packages/caten.nn.encoding/","text":"Encoding [class] RoPE (RoPE dim &key (base 10000)) Implements Rotation Positional Encoding (RoPE). [function] !rope (!rope x dim &key (base 10000)) Applies Rotation Positional Encoding (RoPE) to the input tensor x with the specified dim .","title":"Encoding"},{"location":"packages/caten.nn.encoding/#encoding","text":"","title":"Encoding"},{"location":"packages/caten.nn.encoding/#class-rope","text":"(RoPE dim &key (base 10000)) Implements Rotation Positional Encoding (RoPE).","title":"[class] RoPE"},{"location":"packages/caten.nn.encoding/#function-rope","text":"(!rope x dim &key (base 10000)) Applies Rotation Positional Encoding (RoPE) to the input tensor x with the specified dim .","title":"[function] !rope"},{"location":"packages/caten.nn.linears/","text":"Linear [class] Linear (Linear in-featrues out-features &key (bias t)) Implements a linear transformation.","title":"Linear"},{"location":"packages/caten.nn.linears/#linear","text":"","title":"Linear"},{"location":"packages/caten.nn.linears/#class-linear","text":"(Linear in-featrues out-features &key (bias t)) Implements a linear transformation.","title":"[class] Linear"},{"location":"packages/caten.nn.normalizations/","text":"Normalizations [class] BatchNorm (BatchNorm dims &key (eps 1e-5) (affine t) (bias t)) Batch Normalization Layer [function] !batch-norm (!batch-norm x &optional weight bias mean invstd (axis 1) (eps 1e-5)) Computes the batch normalization of a tensor x with optional weight and bias tensors. [class] LayerNorm (LayerNorm dims &key (eps 1e-5) (affine t) (bias t)) Layer Normalization Layer [function] !layer-norm (!layer-norm x normalized-shape &key (weight) (bias) (eps 1e-5)) Computes the layer normalization of a tensor x with optional weight and bias tensors. [class] RMSNorm (RMSNorm dims &key (eps 1e-5) (weight t)) Root Mean Square Normalization Layer [function] !rms-norm (!rms-norm x normalized-shape &key (weight) (eps 1e-5)) Computes the root mean square normalization of a tensor x with optional weight tensor.","title":"Normalization"},{"location":"packages/caten.nn.normalizations/#normalizations","text":"","title":"Normalizations"},{"location":"packages/caten.nn.normalizations/#class-batchnorm","text":"(BatchNorm dims &key (eps 1e-5) (affine t) (bias t)) Batch Normalization Layer","title":"[class] BatchNorm"},{"location":"packages/caten.nn.normalizations/#function-batch-norm","text":"(!batch-norm x &optional weight bias mean invstd (axis 1) (eps 1e-5)) Computes the batch normalization of a tensor x with optional weight and bias tensors.","title":"[function] !batch-norm"},{"location":"packages/caten.nn.normalizations/#class-layernorm","text":"(LayerNorm dims &key (eps 1e-5) (affine t) (bias t)) Layer Normalization Layer","title":"[class] LayerNorm"},{"location":"packages/caten.nn.normalizations/#function-layer-norm","text":"(!layer-norm x normalized-shape &key (weight) (bias) (eps 1e-5)) Computes the layer normalization of a tensor x with optional weight and bias tensors.","title":"[function] !layer-norm"},{"location":"packages/caten.nn.normalizations/#class-rmsnorm","text":"(RMSNorm dims &key (eps 1e-5) (weight t)) Root Mean Square Normalization Layer","title":"[class] RMSNorm"},{"location":"packages/caten.nn.normalizations/#function-rms-norm","text":"(!rms-norm x normalized-shape &key (weight) (eps 1e-5)) Computes the root mean square normalization of a tensor x with optional weight tensor.","title":"[function] !rms-norm"},{"location":"packages/caten.nn.optimizers/","text":"Optimizers [class] AbstractOptimizer AbstractOptimizer is the base class for all optimizers. One AbstractOptimizer corresponds to one Tensor class with :requires-grad=T . (optimizer-param optimizer) to get the corresponding tensor. [generic] step-optimizer NIL [function] hook-optimizers (hook-optimizers runtime hooker) This function is used to hook the optimizers in the recognised parameters in runtime-params. hooker is an function that takes one argument, which is the tensor that requires-grad=T, returns the AbstractOptimizer. A list of created optimizers are returned. [function] zero-grad (zero-grad optimizer) Fills the gradient of the optimizer with zeros. [class] SGD Implements SGD Optimizer: Param_{new}\\gets{Param - Param_{grad}\\times{lr}} where the initarg :lr is the learning rate. [function] SGD (SGD :lr 1e-3) Returns a lambda function that takes one argument, which is the parameter tensor, and returns an instance of SGD optimizer.","title":"Optimizers"},{"location":"packages/caten.nn.optimizers/#optimizers","text":"","title":"Optimizers"},{"location":"packages/caten.nn.optimizers/#class-abstractoptimizer","text":"AbstractOptimizer is the base class for all optimizers. One AbstractOptimizer corresponds to one Tensor class with :requires-grad=T . (optimizer-param optimizer) to get the corresponding tensor.","title":"[class] AbstractOptimizer"},{"location":"packages/caten.nn.optimizers/#generic-step-optimizer","text":"NIL","title":"[generic] step-optimizer"},{"location":"packages/caten.nn.optimizers/#function-hook-optimizers","text":"(hook-optimizers runtime hooker) This function is used to hook the optimizers in the recognised parameters in runtime-params. hooker is an function that takes one argument, which is the tensor that requires-grad=T, returns the AbstractOptimizer. A list of created optimizers are returned.","title":"[function] hook-optimizers"},{"location":"packages/caten.nn.optimizers/#function-zero-grad","text":"(zero-grad optimizer) Fills the gradient of the optimizer with zeros.","title":"[function] zero-grad"},{"location":"packages/caten.nn.optimizers/#class-sgd","text":"Implements SGD Optimizer: Param_{new}\\gets{Param - Param_{grad}\\times{lr}} where the initarg :lr is the learning rate.","title":"[class] SGD"},{"location":"packages/caten.nn.optimizers/#function-sgd","text":"(SGD :lr 1e-3) Returns a lambda function that takes one argument, which is the parameter tensor, and returns an instance of SGD optimizer.","title":"[function] SGD"},{"location":"packages/caten.nn.padding/","text":"Padding [class] Padding The Func Padding pads the input_tensor with the specified padding. The argument is given in the following format. (output_tensor, input_tensor, *index_components, *padding_starts, *padding_ends) where each (length index_components), (length padding_starts), and (length padding_ends) == ndim(input_tensor) == ndim(output_tensor). With JIT=1, everything is expected to be fused in a single EXPR like: out = where(start_0 < index_components[0] < end_0 and start_1 < index_components[1] < end_1 and ..., input_tensor[i=i'-start_0, j=j'_start_1, ...], value) [function] !padding (!padding x padding &key (value 0.0)) Pads the tensor x with the specified padding and value. Padding is specified as: ((pad_0_1 pad_0_2) (pad_1_0 pad_1_0) ...) . If T is provided instead of (pad_0_0 pad_0_1) , that means no padding is applied to the axis. (that is, T = (0 0) ) Each start_0 and pad_0 is expected as a positive integer (caten will not check this!), or a scalar int32/int64 tensor. [function] !padding2d (!padding2d x padding &key (value 0.0)) Pads the last two dimension of the tensor x with the specified padding and value.. padding is specified as: (padding_left padding_right padding_top padding_bottom).","title":"Padding"},{"location":"packages/caten.nn.padding/#padding","text":"","title":"Padding"},{"location":"packages/caten.nn.padding/#class-padding","text":"The Func Padding pads the input_tensor with the specified padding. The argument is given in the following format. (output_tensor, input_tensor, *index_components, *padding_starts, *padding_ends) where each (length index_components), (length padding_starts), and (length padding_ends) == ndim(input_tensor) == ndim(output_tensor). With JIT=1, everything is expected to be fused in a single EXPR like: out = where(start_0 < index_components[0] < end_0 and start_1 < index_components[1] < end_1 and ..., input_tensor[i=i'-start_0, j=j'_start_1, ...], value)","title":"[class] Padding"},{"location":"packages/caten.nn.padding/#function-padding","text":"(!padding x padding &key (value 0.0)) Pads the tensor x with the specified padding and value. Padding is specified as: ((pad_0_1 pad_0_2) (pad_1_0 pad_1_0) ...) . If T is provided instead of (pad_0_0 pad_0_1) , that means no padding is applied to the axis. (that is, T = (0 0) ) Each start_0 and pad_0 is expected as a positive integer (caten will not check this!), or a scalar int32/int64 tensor.","title":"[function] !padding"},{"location":"packages/caten.nn.padding/#function-padding2d","text":"(!padding2d x padding &key (value 0.0)) Pads the last two dimension of the tensor x with the specified padding and value.. padding is specified as: (padding_left padding_right padding_top padding_bottom).","title":"[function] !padding2d"},{"location":"packages/caten.nn.pooling/","text":"Pooling [class] AvgPool (AvgPool kernel-size &key (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Implements Average Pooling layer. [function] !avgpool (!avgpool x &key (kernel-size `(2 2)) (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Applies average pooling over the tensor. [class] MaxPool (MaxPool kernel-size &key (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Implements Max Pooling layer. [function] !maxpool (!maxpool x &key (kernel-size `(2 2)) (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Applies max pooling over the tensor.","title":"Pooling"},{"location":"packages/caten.nn.pooling/#pooling","text":"","title":"Pooling"},{"location":"packages/caten.nn.pooling/#class-avgpool","text":"(AvgPool kernel-size &key (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Implements Average Pooling layer.","title":"[class] AvgPool"},{"location":"packages/caten.nn.pooling/#function-avgpool","text":"(!avgpool x &key (kernel-size `(2 2)) (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Applies average pooling over the tensor.","title":"[function] !avgpool"},{"location":"packages/caten.nn.pooling/#class-maxpool","text":"(MaxPool kernel-size &key (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Implements Max Pooling layer.","title":"[class] MaxPool"},{"location":"packages/caten.nn.pooling/#function-maxpool","text":"(!maxpool x &key (kernel-size `(2 2)) (stride nil) (dilation 1) (padding 0) (ceiling #'ceiling)) Applies max pooling over the tensor.","title":"[function] !maxpool"}]}